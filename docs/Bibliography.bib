@article{Waterton2012,
abstract = {Although many imaging biomarkers have been described for cancer research, few are sufficiently robust, reliable and well-characterised to be used as routine tools in clinical cancer research. In particular, biomarkers which show that investigational therapies have reduced tumour cell proliferation, or induced necrotic or apoptotic cell death are not commonly used to support decision-making in drug development, even though such pharmacodynamic effects are common goals of many classes of investigational drugs. Moreover we lack well-qualified biomarkers of propensity to metastasise. The qualification and technical validation of imaging biomarkers poses unique challenges not always encountered when validating biospecimen biomarkers. These include standardisation of acquisition and analysis, imaging-pathology correlation, cross-sectional clinical-biomarker correlations and correlation with outcome. Such work is ideally suited to precompetitive research and public-private partnerships, and this has been recognised within the Innovative Medicines Initiative (IMI), a Joint Undertaking between the European Union and the European Federation of Pharmaceutical Industries and Associations, which has initiated projects in the areas of drug safety, drug efficacy, knowledge management and training.},
author = {Waterton, John C and Pylkkanen, Liisa},
doi = {10.1016/j.ejca.2011.11.037},
issn = {1879-0852},
journal = {European journal of cancer},
mendeley-groups = {Radiomics},
month = {mar},
number = {4},
pages = {409--15},
pmid = {22226478},
title = {{Qualification of imaging biomarkers for oncology drug development.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22226478},
volume = {48},
year = {2012}
}
@article{EuropeanSocietyofRadiologyESR2013,
abstract = {Development of imaging biomarkers is a structured process in which new biomarkers are discovered, verified, validated and qualified against biological processes and clinical end-points. The validation process not only concerns the determination of the sensitivity and specificity but also the measurement of reproducibility. Reproducibility assessments and standardisation of the acquisition and data analysis methods are crucial when imaging biomarkers are used in multicentre trials for assessing response to treatment. Quality control in multicentre trials can be performed with the use of imaging phantoms. The cost-effectiveness of imaging biomarkers also needs to be determined. A lot of imaging biomarkers are being developed, but there are still unmet needs-for example, in the detection of tumour invasiveness. Main Messages • Using imaging biomarkers to streamline drug discovery and disease progression is a huge advancement in healthcare. • The qualification and technical validation of imaging biomarkers pose unique challenges in that the accuracy, methods, standardisations and reproducibility are strictly monitored. • The clinical value of new biomarkers is of the highest priority in terms of patient management, assessing risk factors and disease prognosis.},
author = {{European Society of Radiology (ESR)}},
doi = {10.1007/s13244-013-0220-5},
issn = {1869-4101},
journal = {Insights into imaging},
mendeley-groups = {Radiomics},
month = {apr},
number = {2},
pages = {147--52},
pmid = {23397519},
title = {{ESR statement on the stepwise development of imaging biomarkers.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23397519 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3609959},
volume = {4},
year = {2013}
}
@article{Sullivan2015,
abstract = {Although investigators in the imaging community have been active in developing and evaluating quantitative imaging biomarkers (QIBs), the development and implementation of QIBs have been hampered by the inconsistent or incorrect use of terminology or methods for technical performance and statistical concepts. Technical performance is an assessment of how a test performs in reference objects or subjects under controlled conditions. In this article, some of the relevant statistical concepts are reviewed, methods that can be used for evaluating and comparing QIBs are described, and some of the technical performance issues related to imaging biomarkers are discussed. More consistent and correct use of terminology and study design principles will improve clinical research, advance regulatory science, and foster better care for patients who undergo imaging studies.},
author = {Sullivan, Daniel C and Obuchowski, Nancy A and Kessler, Larry G and Raunig, David L and Gatsonis, Constantine and Huang, Erich P and Kondratovich, Marina and McShane, Lisa M and Reeves, Anthony P and Barboriak, Daniel P and Guimaraes, Alexander R and Wahl, Richard L and {RSNA-QIBA Metrology Working Group}},
doi = {10.1148/radiol.2015142202},
issn = {1527-1315},
journal = {Radiology},
mendeley-groups = {Radiomics},
month = {dec},
number = {3},
pages = {813--25},
pmid = {26267831},
title = {{Metrology Standards for Quantitative Imaging Biomarkers.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26267831 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4666097},
volume = {277},
year = {2015}
}
@article{Mulshine2015,
abstract = {The Quantitative Imaging Biomarker Alliance (QIBA) is a multidisciplinary consortium sponsored by the RSNA to define processes that enable the implementation and advancement of quantitative imaging methods described in a QIBA profile document that outlines the process to reliably and accurately measure imaging features. A QIBA profile includes factors such as technical (product-specific) standards, user activities, and relationship to a clinically meaningful metric, such as with nodule measurement in the course of CT screening for lung cancer. In this report, the authors describe how the QIBA approach is being applied to the measurement of small pulmonary nodules such as those found during low-dose CT-based lung cancer screening. All sources of variance with imaging measurement were defined for this process. Through a process of experimentation, literature review, and assembly of expert opinion, the strongest evidence was used to define how to best implement each step in the imaging acquisition and evaluation process. This systematic approach to implementing a quantitative imaging biomarker with standardized specifications for image acquisition and postprocessing for a specific quantitative measurement of a pulmonary nodule results in consistent performance characteristics of the measurement (eg, bias and variance). Implementation of the QIBA small nodule profile may allow more efficient and effective clinical management of the diagnostic workup of individuals found to have suspicious pulmonary nodules in the course of lung cancer screening evaluation.},
author = {Mulshine, James L and Gierada, David S and Armato, Samuel G and Avila, Rick S and Yankelevitz, David F and Kazerooni, Ella A and McNitt-Gray, Michael F and Buckler, Andrew J and Sullivan, Daniel C},
doi = {10.1016/j.jacr.2014.12.003},
issn = {1558-349X},
journal = {Journal of the American College of Radiology},
keywords = {Lung cancer screening,low-dose CT scans,metrology,pulmonary nodules,quantitative imaging biomarker},
mendeley-groups = {Radiomics},
month = {apr},
number = {4},
pages = {390--5},
pmid = {25842017},
title = {{Role of the Quantitative Imaging Biomarker Alliance in optimizing CT for the evaluation of lung cancer screen-detected nodules.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25842017},
volume = {12},
year = {2015}
}
@article{Clarke2014,
abstract = {The purpose of this editorial is to provide a brief history of National Institutes of Health National Cancer Institute (NCI) workshops as related to quantitative imaging within the oncology setting. The editorial will then focus on the recently supported NCI initiatives, including the Quantitative Imaging Network (QIN) initiative and its organizational structure, including planned research goals and deliverables. The publications in this issue of Translational Oncology come from many of the current members of this QIN research network.},
author = {Clarke, Laurence P and Nordstrom, Robert J and Zhang, Huiming and Tandon, Pushpa and Zhang, Yantian and Redmond, George and Farahani, Keyvan and Kelloff, Gary and Henderson, Lori and Shankar, Lalitha and Deye, James and Capala, Jacek and Jacobs, Paula},
issn = {1936-5233},
journal = {Translational oncology},
mendeley-groups = {Radiomics},
month = {feb},
number = {1},
pages = {1--4},
pmid = {24772201},
title = {{The Quantitative Imaging Network: NCI's Historical Perspective and Planned Goals.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24772201 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3998696},
volume = {7},
year = {2014}
}
@article{Lambin2017,
abstract = {Radiomics, the high-throughput mining of quantitative image features from standard-of-care medical imaging that enables data to be extracted and applied within clinical-decision support systems to improve diagnostic, prognostic, and predictive accuracy, is gaining importance in cancer research. Radiomic analysis exploits sophisticated image analysis tools and the rapid development and validation of medical imaging data that uses image-based signatures for precision diagnosis and treatment, providing a powerful tool in modern medicine. Herein, we describe the process of radiomics, its pitfalls, challenges, opportunities, and its capacity to improve clinical decision making, emphasizing the utility for patients with cancer. Currently, the field of radiomics lacks standardized evaluation of both the scientific integrity and the clinical relevance of the numerous published radiomics investigations resulting from the rapid growth of this area. Rigorous evaluation criteria and reporting guidelines need to be established in order for radiomics to mature as a discipline. Herein, we provide guidance for investigations to meet this urgent need in the field of radiomics.},
author = {Lambin, Philippe and Leijenaar, Ralph T. H. and Deist, Timo M. and Peerlings, Jurgen and de Jong, Evelyn E. C. and van Timmeren, Janita and Sanduleanu, Sebastian and Larue, Ruben T. H. M. and Even, Aniek J. G. and Jochems, Arthur and van Wijk, Yvonka and Woodruff, Henry and van Soest, Johan and Lustberg, Tim and Roelofs, Erik and van Elmpt, Wouter J. C. and Dekker, Andr{\'{e}} L. A. J. and Mottaghy, Felix M. and Wildberger, Joachim E. and Walsh, Sean},
doi = {10.1038/nrclinonc.2017.141},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Lambin2017 - Radiomics the bridge between medical imaging and personalized medicine.pdf:pdf},
issn = {1759-4782},
journal = {Nature reviews. Clinical oncology},
mendeley-groups = {Radiomics},
month = {dec},
number = {12},
pages = {749--762},
pmid = {28975929},
title = {{Radiomics: the bridge between medical imaging and personalized medicine.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28975929},
volume = {14},
year = {2017}
}
@article{nordstrom2016quantitative,
  title={The Quantitative Imaging Network in Precision Medicine},
  author={Nordstrom, Robert J},
  journal={Tomography},
  volume={2},
  number={4},
  pages={239},
  year={2016},
  publisher={NIH Public Access}
}
@article{Desseroit2017,
abstract = {The main purpose of this study was to assess the reliability of shape and heterogeneity features in both the PET and the low-dose CT components of PET/CT. A secondary objective was to investigate the impact of image quantization. Methods: A Health Insurance Portability and Accountability Act-compliant secondary analysis of deidentified prospectively acquired PET/CT test-retest datasets of 74 patients from multicenter Merck and American College of Radiology Imaging Network trials was performed. Metabolically active volumes were automatically delineated on PET with a fuzzy locally adaptive bayesian algorithm. Software was used to semiautomatically delineate the anatomic volumes on the low-dose CT component. Two quantization methods were considered: a quantization into a set number of bins (quantization B) and an alternative quantization with bins of fixed width (quantization W). Four shape descriptors, 10 first-order metrics, and 26 textural features were evaluated. Bland-Altman analysis was used to quantify repeatability. Features were subsequently categorized as very reliable, reliable, moderately reliable, or poorly reliable with respect to the corresponding volume variability. Results: Repeatability was highly variable among features. Numerous metrics were identified as poorly or moderately reliable. Others were reliable or very reliable in both modalities and in all categories (shape and first-, second-, and third-order metrics). Image quantization played a major role in feature repeatability. Features were more reliable in PET with quantization B, whereas quantization W showed better results in CT. Conclusion: The test-retest repeatability of shape and heterogeneity features in PET and low-dose CT varied greatly among metrics. The level of repeatability also depended strongly on the quantization step, with different optimal choices for each modality. The repeatability of PET and low-dose CT features should be carefully considered when selecting metrics to build multiparametric models.},
archivePrefix = {arXiv},
arxivId = {1610.01390},
author = {Desseroit, Marie-Charlotte and Tixier, Florent and Weber, Wolfgang A. and Siegel, Barry A. and {Cheze Le Rest}, Catherine and Visvikis, Dimitris and Hatt, Mathieu},
doi = {10.2967/jnumed.116.180919},
eprint = {1610.01390},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Desseroit2016 - Reliability of PETCT shape and heterogeneity features in functional and morphological components.pdf:pdf},
issn = {1535-5667},
journal = {Journal of nuclear medicine},
keywords = {PET/CT,radiomics,repeatability,texture analysis},
mendeley-groups = {Radiomics},
month = {mar},
number = {3},
pages = {406--411},
pmid = {27765856},
title = {{Reliability of PET/CT Shape and Heterogeneity Features in Functional and Morphologic Components of Non-Small Cell Lung Cancer Tumors: A Repeatability Analysis in a Prospective Multicenter Cohort.}},
url = {http://jnm.snmjournals.org/lookup/doi/10.2967/jnumed.116.180919 http://www.ncbi.nlm.nih.gov/pubmed/27765856 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5331937},
volume = {58},
year = {2017}
}

@article{Vovk2007,
abstract = {Medical image acquisition devices provide a vast amount of anatomical and functional information, which facilitate and improve diagnosis and patient treatment, especially when supported by modern quantitative image analysis methods. However, modality specific image artifacts, such as the phenomena of intensity inhomogeneity in magnetic resonance images (MRI), are still prominent and can adversely affect quantitative image analysis. In this paper, numerous methods that have been developed to reduce or eliminate intensity inhomogeneities in MRI are reviewed. First, the methods are classified according to the inhomogeneity correction strategy. Next, different qualitative and quantitative evaluation approaches are reviewed. Third, 60 relevant publications are categorized according to several features and analyzed so as to reveal major trends, popularity, evaluation strategies and applications. Finally, key evaluation issues and future development of the inhomogeneity correction field, supported by the results of the analysis, are discussed.},
archivePrefix = {arXiv},
arxivId = {2},
author = {Vovk, Uros and Pernus, Franjo and Likar, Bostjan},
doi = {10.1109/TMI.2006.891486},
eprint = {2},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Vovk2007 - A review of methods for correction of intensity inhomogeneity in MRI.pdf:pdf},
isbn = {0278-0062},
issn = {0278-0062},
journal = {IEEE transactions on medical imaging},
keywords = {Bias field,Intensity inhomogeneity,Intensity nonuniformity,Magnetic resonance images (MRI),Segmentation,Shading},
mendeley-groups = {Radiomics/IBSI},
month = {mar},
number = {3},
pages = {405--21},
pmid = {17354645},
title = {{A review of methods for correction of intensity inhomogeneity in MRI.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17354645},
volume = {26},
year = {2007}
}
@article{Balafar2010,
abstract = {Brain image segmentation is one of the most important parts of clinical diagnostic tools. Brain images mostly contain noise, inhomogeneity and sometimes deviation. Therefore, accurate segmentation of brain images is a very difficult task. However, the process of accurate segmentation of these images is very important and crucial for a correct diagnosis by clinical tools. We presented a review of the methods used in brain segmentation. The review covers imaging modalities, magnetic resonance imaging and methods for noise reduction, inhomogeneity correction and segmentation. We conclude with a discussion on the trend of future research in brain segmentation.},
author = {Balafar, M. A. and Ramli, A. R. and Saripan, M. I. and Mashohor, S.},
doi = {10.1007/s10462-010-9155-0},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Balafar2010 - review of brain MRI image segmentation methods.pdf:pdf},
isbn = {0269-2821},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {Brain,MRI,Segmentation},
mendeley-groups = {Radiomics/IBSI},
month = {mar},
number = {3},
pages = {261--274},
title = {{Review of brain MRI image segmentation methods}},
url = {http://link.springer.com/10.1007/s10462-010-9155-0},
volume = {33},
year = {2010}
}
@inbook{ElNaqa2017,
abstract = {PET imaging is a main diagnostic modality of different diseases including cancer. In the particular case of cancer, PET is widely used for staging of disease progression, identification of the treatment gross tumor volume, monitoring of disease, as well as prediction of outcomes and personalization of treatment regimens. Among the arsenal of different functional imaging modalities, PET has benefited from early adoption of quantitative image analysis starting from simple standard uptake value (SUV) normalization to more advanced extraction of complex imaging uptake patterns, thanks chiefly to the application of sophisticated image processing algorithms. In this chapter, we discuss the application of image processing techniques to PET imaging with special focus on the oncological radiotherapy domain starting from basic feature extraction to application in target definition using image segmentation/registration and more recent image-based outcome modeling in the radiomics field. We further extend the discussion into hybrid anatomical functional combinations of PET/CT and PET/MR multimodalities.},
address = {Cham},
author = {{El Naqa}, Issam},
booktitle = {Basic Science of PET Imaging},
doi = {10.1007/978-3-319-40070-9_12},
isbn = {978-3-319-40070-9},
pages = {285--301},
publisher = {Springer International Publishing},
title = {{Image Processing and Analysis of PET and Hybrid PET Imaging}},
url = {https://doi.org/10.1007/978-3-319-40070-9{\_}12},
year = {2017}
}
@article{LePogam2013,
abstract = {Denoising of Positron Emission Tomography (PET) images is a challenging task due to the inherent low signal-to-noise ratio (SNR) of the acquired data. A pre-processing denoising step may facilitate and improve the results of further steps such as segmentation, quantification or textural features characterization. Different recent denoising techniques have been introduced and most state-of-the-art methods are based on filtering in the wavelet domain. However, the wavelet transform suffers from some limitations due to its non-optimal processing of edge discontinuities. More recently, a new multi scale geometric approach has been proposed, namely the curvelet transform. It extends the wavelet transform to account for directional properties in the image. In order to address the issue of resolution loss associated with standard denoising, we considered a strategy combining the complementary wavelet and curvelet transforms. We compared different figures of merit (e.g. SNR increase, noise decrease in homogeneous regions, resolution loss, and intensity bias) on simulated and clinical datasets with the proposed combined approach and the wavelet-only and curvelet-only filtering techniques. The three methods led to an increase of the SNR. Regarding the quantitative accuracy however, the wavelet and curvelet only denoising approaches led to larger biases in the intensity and the contrast than the proposed combined algorithm. This approach could become an alternative solution to filters currently used after image reconstruction in clinical systems such as the Gaussian filter.},
author = {{Le Pogam}, Adrien and Hanzouli, H. and Hatt, Mathieu and {Cheze Le Rest}, Catherine and Visvikis, Dimitris},
doi = {10.1016/j.media.2013.05.005},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/LePogam2013 - Denoising of PET images by combining wavelets and curvelets.pdf:pdf},
issn = {1361-8423},
journal = {Medical image analysis},
keywords = {Curvelet transform,Denoising,Filtering,Positron emission tomography,Wavelet transform},
mendeley-groups = {Radiomics/IBSI},
month = {dec},
number = {8},
pages = {877--91},
pmid = {23837964},
publisher = {Elsevier B.V.},
title = {{Denoising of PET images by combining wavelets and curvelets for improved preservation of resolution and quantitation.}},
url = {http://dx.doi.org/10.1016/j.media.2013.05.005 http://www.ncbi.nlm.nih.gov/pubmed/23837964},
volume = {17},
year = {2013}
}
@article{Vallieres2017,
abstract = {Quantitative extraction of high-dimensional mineable data from medical images is a process known as radiomics. Radiomics is foreseen as an essential prognostic tool for cancer risk assessment and the quantification of intratumoural heterogeneity. In this work, 1615 radiomic features (quantifying tumour image intensity, shape, texture) extracted from pre-treatment FDG-PET and CT images of 300 patients from four different cohorts were analyzed for the risk assessment of locoregional recurrences (LR) and distant metastases (DM) in head-and-neck cancer. Prediction models combining radiomic and clinical variables were constructed via random forests and imbalance-adjustment strategies using two of the four cohorts. Independent validation of the prediction and prognostic performance of the models was carried out on the other two cohorts (LR: AUC = 0.69 and CI = 0.67; DM: AUC = 0.86 and CI = 0.88). Furthermore, the results obtained via Kaplan-Meier analysis demonstrated the potential of radiomics for assessing the risk of specific tumour outcomes using multiple stratification groups. This could have important clinical impact, notably by allowing for a better personalization of chemo-radiation treatments for head-and-neck cancer patients from different risk groups.},
archivePrefix = {arXiv},
arxivId = {1703.08516},
author = {Valli{\`{e}}res, Martin and Kay-Rivest, Emily and Perrin, L{\'{e}}o Jean and Liem, Xavier and Furstoss, Christophe and Aerts, Hugo J. W. L. and Khaouam, Nader and Nguyen-Tan, Phuc Felix and Wang, Chang-Shu and Sultanem, Khalil and Seuntjens, Jan and {El Naqa}, Issam},
doi = {10.1038/s41598-017-10371-5},
eprint = {1703.08516},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Valli{\`{e}}res2017 - Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer.pdf:pdf},
isbn = {2045-2322 (Electronic) 2045-2322 (Linking)},
issn = {2045-2322},
journal = {Scientific reports},
month = {aug},
number = {},
pages = {10117},
pmid = {28860628},
title = {{Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer.}},
url = {http://arxiv.org/abs/1703.08516 http://www.ncbi.nlm.nih.gov/pubmed/28860628 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5579274},
volume = {7},
year = {2017}
}
@article{Vallieres2017a,
abstract = {Texture-based radiomic models constructed from medical images have the potential to support cancer treatment management via personalized assessment of tumour aggressiveness. While the identification of stable texture features under varying imaging settings is crucial for the translation of radiomics analysis into routine clinical practice, we hypothesize in this work that a complementary optimization of image acquisition parameters prior to texture feature extraction could enhance the predictive performance of texture-based radiomic models. As a proof of concept, we evaluated the possibility of enhancing a model constructed for the early prediction of lung metastases in soft-tissue sarcomas by optimizing PET and MR image acquisition protocols via computerized simulations of image acquisitions with varying parameters. Simulated PET images from 30 STS patients were acquired by varying the extent of axial data combined per slice ('span'). Simulated T 1-weighted and T 2-weighted MR images were acquired by varying the repetition time and echo time in a spin-echo pulse sequence, respectively. We analyzed the impact of the variations of PET and MR image acquisition parameters on individual textures, and we investigated how these variations could enhance the global response and the predictive properties of a texture-based model. Our results suggest that it is feasible to identify an optimal set of image acquisition parameters to improve prediction performance. The model constructed with textures extracted from simulated images acquired with a standard clinical set of acquisition parameters reached an average AUC of [Formula: see text] in bootstrap testing experiments. In comparison, the model performance significantly increased using an optimal set of image acquisition parameters ([Formula: see text]), with an average AUC of [Formula: see text]. Ultimately, specific acquisition protocols optimized to generate superior radiomics measurements for a given clinical problem could be developed and standardized via dedicated computer simulations and thereafter validated using clinical scanners.},
author = {Valli{\`{e}}res, Martin and Laberge, S{\'{e}}bastien and Diamant, Andr{\'{e}} and {El Naqa}, Issam},
doi = {10.1088/1361-6560/aa8a49},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Valli{\`{e}}res2017 - Enhancement of multimodality texture-based prediction models via optimization of PET and MR image acquisition protocols.pdf:pdf},
issn = {1361-6560},
journal = {Physics in medicine and biology},
month = {oct},
number = {22},
pages = {8536--8565},
pmid = {28872054},
title = {{Enhancement of multimodality texture-based prediction models via optimization of PET and MR image acquisition protocols: a proof of concept.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28872054},
volume = {62},
year = {2017}
}
@article{Nyflot2015,
abstract = {Image heterogeneity metrics such as textural features are an active area of research for evaluating clinical outcomes with positron emission tomography (PET) imaging and other modalities. However, the effects of stochastic image acquisition noise on these metrics are poorly understood. We performed a simulation study by generating 50 statistically independent PET images of the NEMA IQ phantom with realistic noise and resolution properties. Heterogeneity metrics based on gray-level intensity histograms, co-occurrence matrices, neighborhood difference matrices, and zone size matrices were evaluated within regions of interest surrounding the lesions. The impact of stochastic variability was evaluated with percent difference from the mean of the 50 realizations, coefficient of variation and estimated sample size for clinical trials. Additionally, sensitivity studies were performed to simulate the effects of patient size and image reconstruction method on the quantitative performance of these metrics. Complex trends in variability were revealed as a function of textural feature, lesion size, patient size, and reconstruction parameters. In conclusion, the sensitivity of PET textural features to normal stochastic image variation and imaging parameters can be large and is feature-dependent. Standards are needed to ensure that prospective studies that incorporate textural features are properly designed to measure true effects that may impact clinical outcomes.},
author = {Nyflot, Matthew J. and Yang, Fei and Byrd, Darrin and Bowen, Stephen R. and Sandison, George A. and Kinahan, Paul E.},
doi = {10.1117/1.JMI.2.4.041002},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Nyflot2015 - Quantitative radiomics - impact of stochastic effects on textural feature analysis imples the need for standards.pdf:pdf},
isbn = {2329-4302 (Print) 2329-4302 (Linking)},
issn = {2329-4302},
journal = {Journal of medical imaging},
keywords = {heterogeneity,positron emission tomography,quantitative,simulation,standardization,textural features},
month = {oct},
number = {4},
pages = {041002},
pmid = {26251842},
title = {{Quantitative radiomics: impact of stochastic effects on textural feature analysis implies the need for standards.}},
url = {http://medicalimaging.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JMI.2.4.041002 http://www.ncbi.nlm.nih.gov/pubmed/26251842 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4524811},
volume = {2},
year = {2015}
}
@article{Kuhnert2016,
abstract = {BACKGROUND In oncological imaging using PET/CT, the standardized uptake value has become the most common parameter used to measure tracer accumulation. The aim of this analysis was to evaluate ultra high definition (UHD) and ordered subset expectation maximization (OSEM) PET/CT reconstructions for their potential impact on quantification. PATIENTS AND METHODS We analyzed 40 PET/CT scans of lung cancer patients who had undergone PET/CT. Standardized uptake values corrected for body weight (SUV) and lean body mass (SUL) were determined in the single hottest lesion in the lung and normalized to the liver for UHD and OSEM reconstruction. Quantitative uptake values and their normalized ratios for the two reconstruction settings were compared using the Wilcoxon test. The distribution of quantitative uptake values and their ratios in relation to the reconstruction method used were demonstrated in the form of frequency distribution curves, box-plots and scatter plots. The agreement between OSEM and UHD reconstructions was assessed through Bland-Altman analysis. RESULTS A significant difference was observed after OSEM and UHD reconstruction for SUV and SUL data tested (p {\textless} 0.0005 in all cases). The mean values of the ratios after OSEM and UHD reconstruction showed equally significant differences (p {\textless} 0.0005 in all cases). Bland-Altman analysis showed that the SUV and SUL and their normalized values were, on average, up to 60 {\%} higher after UHD reconstruction as compared to OSEM reconstruction. CONCLUSION OSEM and HD reconstruction brought a significant difference for SUV and SUL, which remained constantly high after normalization to the liver, indicating that standardization of reconstruction and the use of comparable SUV measurements are crucial when using PET/CT.},
author = {Kuhnert, Georg and Boellaard, Ronald and Sterzer, Sergej and Kahraman, Deniz and Scheffler, Matthias and Wolf, J{\"{u}}rgen and Dietlein, Markus and Drzezga, Alexander and Kobe, Carsten},
doi = {10.1007/s00259-015-3165-8},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Kuhnert2016 - Impact of PETCT image reconstruction methods and liver uptake normalization strategies on quantitative image analysis.pdf:pdf},
isbn = {1619-7089 (Electronic)$\backslash$r1619-7070 (Linking)},
issn = {1619-7089},
journal = {European journal of nuclear medicine and molecular imaging},
keywords = {Lung cancer,PET,Quantification,SUV},
month = {feb},
number = {2},
pages = {249--58},
pmid = {26280981},
title = {{Impact of PET/CT image reconstruction methods and liver uptake normalization strategies on quantitative image analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26280981},
volume = {43},
year = {2016}
}
@incollection{Hutter2011,
abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.},
author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-642-25566-3_40},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Hutter2011 - Sequential model-based optimization for general algorithm configuration.pdf:pdf},
isbn = {9783642255656},
issn = {03029743},
pages = {507--523},
title = {{Sequential Model-Based Optimization for General Algorithm Configuration}},
url = {http://link.springer.com/10.1007/978-3-642-25566-3{\_}40},
volume = {6683 LNCS},
year = {2011}
}
@article{Antolini2005,
abstract = {To derive models suitable for outcome prediction, a crucial aspect is the availability of appropriate measures of predictive accuracy, which have to be usable for a general class of models. The Harrell's C discrimination index is an extension of the area under the ROC curve to the case of censored survival data, which owns a straightforward interpretability. For a model including covariates with time-dependent effects and/or time-dependent covariates, the original definition of C would require the prediction of individual failure times, which is not generally addressed in most clinical applications. Here we propose a time-dependent discrimination index Ctd where the whole predicted survival function is utilized as outcome prediction, and the ability to discriminate among subjects having different outcome is summarized over time. Ctd is based on a novel definition of concordance: a subject who developed the event should have a less predicted probability of surviving beyond his/her survival time than any subject who survived longer. The predicted survival function of a subject who developed the event is compared to: (1) that of subjects who developed the event before his/her survival time, and (2) that of subjects who developed the event, or were censored, after his/her survival time. Subjects who were censored are involved in comparisons with subjects who developed the event before their observed times. The index reduces to the previous C in the presence of separation between survival curves on the whole follow-up. A confidence interval for Ctd is derived using the jackknife method on correlated one-sample U-statistics.The proposed index is used to evaluate the discrimination ability of a model, including covariates having time-dependent effects, concerning time to relapse in breast cancer patients treated with adjuvant tamoxifen. The model was obtained from 596 patients entered prospectively at Istituto Nazionale per lo Studio e la Cura dei Tumori di Milano (INT). The model discrimination ability was validated on an independent testing data set of 175 patients provided by Centro Regionale Indicatori Biochimici di Tumore (CRIBT) in Venice.},
author = {Antolini, Laura and Boracchi, Patrizia and Biganzoli, Elia},
doi = {10.1002/sim.2427},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Antolini2005 - A time-dependent discrimination index for survival data.pdf:pdf},
isbn = {0277-6715},
issn = {0277-6715},
journal = {Statistics in medicine},
keywords = {Discrimination,Non-parametric method,ROC analysis,Survival model,Time dependence},
month = {dec},
number = {24},
pages = {3927--44},
pmid = {16320281},
title = {{A time-dependent discrimination index for survival data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16320281},
volume = {24},
year = {2005}
}
@article{Shinohara2014,
abstract = {While computed tomography and other imaging techniques are measured in absolute units with physical meaning, magnetic resonance images are expressed in arbitrary units that are difficult to interpret and differ between study visits and subjects. Much work in the image processing literature on intensity normalization has focused on histogram matching and other histogram mapping techniques, with little emphasis on normalizing images to have biologically interpretable units. Furthermore, there are no formalized principles or goals for the crucial comparability of image intensities within and across subjects. To address this, we propose a set of criteria necessary for the normalization of images. We further propose simple and robust biologically motivated normalization techniques for multisequence brain imaging that have the same interpretation across acquisitions and satisfy the proposed criteria. We compare the performance of different normalization methods in thousands of images of patients with Alzheimer's disease, hundreds of patients with multiple sclerosis, and hundreds of healthy subjects obtained in several different studies at dozens of imaging centers.},
annote = {Paper on MR image processing},
author = {Shinohara, Russell T. and Sweeney, Elizabeth M. and Goldsmith, Jeff and Shiee, Navid and Mateen, Farrah J. and Calabresi, Peter A. and Jarso, Samson and Pham, Dzung L. and Reich, Daniel S. and Crainiceanu, Ciprian M. and {Australian Imaging Biomarkers Lifestyle Flagship Study of Ageing} and {Alzheimer's Disease Neuroimaging Initiative}},
doi = {10.1016/j.nicl.2014.08.008},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Shinohara2014 - Statistical normalization techniques for magnetic resonance imaging.pdf:pdf},
issn = {2213-1582},
journal = {NeuroImage. Clinical},
keywords = {Image analysis,Magnetic resonance imaging,Normalization,Statistics},
pages = {9--19},
pmid = {25379412},
publisher = {Elsevier B.V.},
title = {{Statistical normalization techniques for magnetic resonance imaging.}},
url = {http://dx.doi.org/10.1016/j.nicl.2014.08.008 http://www.ncbi.nlm.nih.gov/pubmed/25379412 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4215426},
volume = {6},
year = {2014}
}
@article{Lao2017,
abstract = {Traditional radiomics models mainly rely on explicitly-designed handcrafted features from medical images. This paper aimed to investigate if deep features extracted via transfer learning can generate radiomics signatures for prediction of overall survival (OS) in patients with Glioblastoma Multiforme (GBM). This study comprised a discovery data set of 75 patients and an independent validation data set of 37 patients. A total of 1403 handcrafted features and 98304 deep features were extracted from preoperative multi-modality MR images. After feature selection, a six-deep-feature signature was constructed by using the least absolute shrinkage and selection operator (LASSO) Cox regression model. A radiomics nomogram was further presented by combining the signature and clinical risk factors such as age and Karnofsky Performance Score. Compared with traditional risk factors, the proposed signature achieved better performance for prediction of OS (C-index = 0.710, 95{\%} CI: 0.588, 0.932) and significant stratification of patients into prognostically distinct groups (P {\textless} 0.001, HR = 5.128, 95{\%} CI: 2.029, 12.960). The combined model achieved improved predictive performance (C-index = 0.739). Our study demonstrates that transfer learning-based deep features are able to generate prognostic imaging signature for OS prediction and patient stratification for GBM, indicating the potential of deep imaging feature-based biomarker in preoperative care of GBM patients.},
author = {Lao, Jiangwei and Chen, Yinsheng and Li, Zhi-Cheng and Li, Qihua and Zhang, Ji and Liu, Jing and Zhai, Guangtao},
doi = {10.1038/s41598-017-10649-8},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Lao2017 - A deep learning-based radiomics model for prediction of survival in glioblastoma multiforme.pdf:pdf},
isbn = {2045-2322 (Electronic) 2045-2322 (Linking)},
issn = {2045-2322},
journal = {Scientific reports},
month = {sep},
number = {1},
pages = {10353},
pmid = {28871110},
publisher = {Springer US},
title = {{A Deep Learning-Based Radiomics Model for Prediction of Survival in Glioblastoma Multiforme.}},
url = {http://www.nature.com/articles/s41598-017-10649-8 http://www.ncbi.nlm.nih.gov/pubmed/28871110 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5583361},
volume = {7},
year = {2017}
}
@article{Becker2017,
abstract = {BACKGROUND Texture analysis in oncological magnetic resonance imaging (MRI) may yield surrogate markers for tumor differentiation and staging, both of which are important factors in the treatment planning for cervical cancer. PURPOSE To identify texture features which may predict tumor differentiation and nodal status in diffusion-weighted imaging (DWI) of cervical carcinoma. MATERIAL AND METHODS Twenty-three patients were enrolled in this prospective, institutional review board (IRB)-approved study. Pelvic MRI was performed at 3-T including a DWI echo-planar sequence with b-values 40, 300, and 800 s/mm(2). Apparent diffusion coefficient (ADC) maps were used for region of interest (ROI)-based texture analysis (32 texture features) of tumor, muscle, and fat based on histogram and gray-level matrices (GLM). All features confounded by the ROI size (linear model) were excluded. The remaining features were examined for correlations with histological differentiation (Spearman) and nodal status (Kruskal-Wallis). Hierarchical cluster analysis was used to identify correlations between features. A P value {\textless} 0.05 was considered statistically significant. RESULTS Mean age was 55 years (range = 37-78 years). Biopsy revealed two well-differentiated, eight moderately differentiated, two moderately to poorly differentiated tumors, and five poorly differentiated tumors. Six tumors could not be graded. Lymph nodes were involved in 11 patients. Three GLM features correlated with the differentiation: LRHGE (ϱ = 0.53, P = 0.03), ZP (ϱ = -0.49, P {\textless} 0.05), and SZE (ϱ = -0.51, P = 0.04). Two histogram features, skewness (0.65 vs. 1.08, P = 0.04) and kurtosis (0.53 vs. 1.67, P = 0.02), were higher in patients with positive nodal status. Cluster analysis revealed several co-correlations. CONCLUSION We identified potentially predictive GLM features for histological tumor differentiation and histogram features for nodal cancer stage.},
author = {Becker, Anton S and Ghafoor, Soleen and Marcon, Magda and Perucho, Jose A and Wurnig, Moritz C and Wagner, Matthias W and Khong, Pek-Lan and Lee, Elaine Yp and Boss, Andreas},
doi = {10.1177/2058460117729574},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Becker2017 - MRI texture features may predict differentiation and nodal stage of cervical cancer - a pilot study.pdf:pdf},
isbn = {2058460117729},
issn = {2058-4601},
journal = {Acta radiologica open},
keywords = {Cervical cancer,apparent diffusion coefficient (ADC),diffusion-weighted imaging (DWI),texture analysis,texture features},
month = {oct},
number = {10},
pages = {1--10},
pmid = {29085671},
title = {{MRI texture features may predict differentiation and nodal stage of cervical cancer: a pilot study.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29085671 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5648100},
volume = {6},
year = {2017}
}
@article{Li2017,
abstract = {Organ homeostasis, cellular differentiation, signal relay, and in situ function all depend on the spatial organization of cells in complex tissues. For this reason, comprehensive, high-resolution mapping of cell positioning, phenotypic identity, and functional state in the context of macroscale tissue structure is critical to a deeper understanding of diverse biological processes. Here we report an easy to use method, clearing-enhanced 3D (Ce3D), which generates excellent tissue transparency for most organs, preserves cellular morphology and protein fluorescence, and is robustly compatible with antibody-based immunolabeling. This enhanced signal quality and capacity for extensive probe multiplexing permits quantitative analysis of distinct, highly intermixed cell populations in intact Ce3D-treated tissues via 3D histo-cytometry. We use this technology to demonstrate large-volume, high-resolution microscopy of diverse cell types in lymphoid and nonlymphoid organs, as well as to perform quantitative analysis of the composition and tissue distribution of multiple cell populations in lymphoid tissues. Combined with histo-cytometry, Ce3D provides a comprehensive strategy for volumetric quantitative imaging and analysis that bridges the gap between conventional section imaging and disassociation-based techniques.},
author = {Li, Weizhe and Germain, Ronald N. and Gerner, Michael Y.},
doi = {10.1073/pnas.1708981114},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/LiWeizhe2017 - Multiplex quantitative cellular analysis in large tissue volumes with clearing-enhanced 3D microscopy.pdf:pdf},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {histo-cytometry,immune system,quantitative microscopy,tissue clearing},
month = {aug},
number = {35},
pages = {E7321--E7330},
pmid = {28808033},
title = {{Multiplex, quantitative cellular analysis in large tissue volumes with clearing-enhanced 3D microscopy (Ce3D).}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1708981114 http://www.ncbi.nlm.nih.gov/pubmed/28808033 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5584454},
volume = {114},
year = {2017}
}
@article{Li2014,
abstract = {This paper proposes a new energy minimization method called multiplicative intrinsic component optimization (MICO) for joint bias field estimation and segmentation of magnetic resonance (MR) images. The proposed method takes full advantage of the decomposition of MR images into two multiplicative components, namely, the true image that characterizes a physical property of the tissues and the bias field that accounts for the intensity inhomogeneity, and their respective spatial properties. Bias field estimation and tissue segmentation are simultaneously achieved by an energy minimization process aimed to optimize the estimates of the two multiplicative components of an MR image. The bias field is iteratively optimized by using efficient matrix computations, which are verified to be numerically stable by matrix analysis. More importantly, the energy in our formulation is convex in each of its variables, which leads to the robustness of the proposed energy minimization algorithm. The MICO formulation can be naturally extended to 3D/4D tissue segmentation with spatial/sptatiotemporal regularization. Quantitative evaluations and comparisons with some popular softwares have demonstrated superior performance of MICO in terms of robustness and accuracy.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Li, Chunming and Gore, John C. and Davatzikos, Christos},
doi = {10.1016/j.mri.2014.03.010},
eprint = {15334406},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Li2017 - Multiplicative intrinsic component optimization for MRI bias field estimation and tissue segmentation.pdf:pdf},
isbn = {1873-5894 (Electronic)$\backslash$r0730-725X (Linking)},
issn = {1873-5894},
journal = {Magnetic resonance imaging},
keywords = {4D segmentation,Bias field correction,Bias field estimation,Brain segmentation,Intensity inhomogeneity,MRI},
month = {sep},
number = {7},
pages = {913--23},
pmid = {24928302},
publisher = {Elsevier B.V.},
title = {{Multiplicative intrinsic component optimization (MICO) for MRI bias field estimation and tissue segmentation.}},
url = {http://dx.doi.org/10.1016/j.mri.2014.03.010 http://www.ncbi.nlm.nih.gov/pubmed/24928302 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4401088},
volume = {32},
year = {2014}
}
@article{Mayerhoefer2009,
abstract = {MRI texture features are generally considered to be sensitive to variations in signal-to-noise ratio and spatial resolution, which represents an obstacle for the widespread clinical application of texture-based pattern discrimination with MRI. This study investigates the sensitivity of texture features of different categories (co-occurrence matrix, run-length matrix, absolute gradient, autoregressive model, and wavelet transform) to variations in the number of acquisitions (NAs), repetition time (TR), echo time (TE), and sampling bandwidth (SBW) at different spatial resolutions. Special emphasis was placed on the influence of MRI protocol heterogeneity and implications for the results of pattern discrimination. Experiments were performed using two polystyrene spheres and agar gel phantoms with different nodular patterns. T2-weighted multislice multiecho images were obtained using a 3.0 T scanner equipped with a microimaging gradient insert coil. Linear discriminant analysis and k nearest neighbor classification were used for texture-based pattern discrimination. Results show that texture features of all categories are increasingly sensitive to acquisition parameter variations with increasing spatial resolution. Nevertheless, as long as the spatial resolution is sufficiently high, variations in NA, TR, TE, and SBW have little effect on the results of pattern discrimination. Texture features derived from the co-occurrence matrix are superior to features of other categories because they enable discrimination of different patterns close to the resolution limits for the smallest structures of physical texture even for datasets that are heterogeneous with regard to different acquisition parameters, including spatial resolution.},
author = {Mayerhoefer, Marius E. and Szomolanyi, Pavol and Jirak, Daniel and Materka, Andrzej and Trattnig, Siegfried},
doi = {10.1118/1.3081408},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Mayerhoefer2009 - Effect of MRI acquisition parameter variations and protocol heterogeneity.pdf:pdf},
isbn = {0094-2405 (Print)$\backslash$r0094-2405 (Linking)},
issn = {00942405},
journal = {Medical Physics},
keywords = {magnetic resonance imaging,pattern recognition,texture analysis},
number = {4},
pages = {1236--1243},
pmid = {19472631},
title = {{Effects of MRI acquisition parameter variations and protocol heterogeneity on the results of texture analysis and pattern discrimination: An application-oriented study}},
url = {http://doi.wiley.com/10.1118/1.3081408},
volume = {36},
year = {2009}
}
@article{Baker2016,
abstract = {A Nature survey lifts the lid on how researchers view the ‘crisis' rocking science and what they think will help.},
author = {Baker, Monya},
doi = {10.1038/533452a},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Baker2016 - Is there a reproducibility crisis.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {1476-4687},
journal = {Nature},
number = {7604},
pages = {452--4},
pmid = {27225100},
title = {1,500 scientists lift the lid on reproducibility.},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27225100},
volume = {533},
year = {2016}
}
@article{Rahmim2017,
abstract = {No disease modifying therapies for Parkinson's disease (PD) have been found effective to date. To properly power clinical trials for discovery of such therapies, the ability to predict outcome in PD is critical, and there is a significant need for discovery of prognostic biomarkers of PD. Dopamine transporter (DAT) SPECT imaging is widely used for diagnostic purposes in PD. In the present work, we aimed to evaluate whether longitudinal DAT SPECT imaging can significantly improve prediction of outcome in PD patients. In particular, we investigated whether radiomics analysis of DAT SPECT images, in addition to use of conventional non-imaging and imaging measures, could be used to predict motor severity at year 4 in PD subjects. We selected 64 PD subjects (38 male, 26 female; age at baseline (year 0): 61.9 ± 7.3, range [46,78]) from the Parkinson's Progressive Marker Initiative (PPMI) database. Inclusion criteria included (i) having had at least 2 SPECT scans at years 0 and 1 acquired on a similar scanner, (ii) having undergone a high-resolution 3 T MRI scan, and (iii) having motor assessment (MDS-UPDRS-III) available in year 4 used as outcome measure. Image analysis included automatic region-of-interest (ROI) extraction on MRI images, registration of SPECT images onto the corresponding MRI images, and extraction of radiomic features. Non-imaging predictors included demographics, disease duration as well as motor and non-motor clinical measures in years 0 and 1. The image predictors included 92 radiomic features extracted from the caudate, putamen, and ventral striatum of DAT SPECT images at years 0 and 1 to quantify heterogeneity and texture in uptake. Random forest (RF) analysis with 5000 trees was used to combine both non-imaging and imaging variables to predict motor outcome (UPDRS-III: 27.3 ± 14.7, range [3,77]). The RF prediction was evaluated using leave-one-out cross-validation. Our results demonstrated that addition of radiomic features to conventional measures significantly improved (p {\textless} 0.001) prediction of outcome, reducing the absolute error of predicting MDS-UPDRS-III from 9.00 ± 0.88 to 4.12 ± 0.43. This shows that radiomics analysis of DAT SPECT images has a significant potential towards development of effective prognostic biomarkers in PD.},
author = {Rahmim, Arman and Huang, Peng and Shenkov, Nikolay and Fotouhi, Sima and Davoodi-Bojd, Esmaeil and Lu, Lijun and Mari, Zoltan and Soltanian-Zadeh, Hamid and Sossi, Vesna},
doi = {10.1016/j.nicl.2017.08.021},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Rahmim2017 - Improved prediction of outcome in Parkinsons disease using radiomics analysis of longitudinal DAT SPECT images.pdf:pdf},
isbn = {4109550696},
issn = {22131582},
journal = {NeuroImage: Clinical},
keywords = {DAT SPECT,Longitudinal,Outcome prediction,Parkinson's disease,Radiomics,Textural features},
pages = {539--544},
publisher = {Elsevier Inc},
title = {{Improved prediction of outcome in Parkinson's disease using radiomics analysis of longitudinal DAT SPECT images}},
url = {http://dx.doi.org/10.1016/j.nicl.2017.08.021 http://linkinghub.elsevier.com/retrieve/pii/S2213158217302115},
volume = {16},
year = {2017}
}
@article{Parmar2014,
abstract = {Due to advances in the acquisition and analysis of medical imaging, it is currently possible to quantify the tumor phenotype. The emerging field of Radiomics addresses this issue by converting medical images into minable data by extracting a large number of quantitative imaging features. One of the main challenges of Radiomics is tumor segmentation. Where manual delineation is time consuming and prone to inter-observer variability, it has been shown that semi-automated approaches are fast and reduce inter-observer variability. In this study, a semiautomatic region growing volumetric segmentation algorithm, implemented in the free and publicly available 3D-Slicer platform, was investigated in terms of its robustness for quantitative imaging feature extraction. Fifty-six 3D-radiomic features, quantifying phenotypic differences based on tumor intensity, shape and texture, were extracted from the computed tomography images of twenty lung cancer patients. These radiomic features were derived from the 3D-tumor volumes defined by three independent observers twice using 3D-Slicer, and compared to manual slice-by-slice delineations of five independent physicians in terms of intra-class correlation coefficient (ICC) and feature range. Radiomic features extracted from 3D-Slicer segmentations had significantly higher reproducibility (ICC = 0.85±0.15, p = 0.0009) compared to the features extracted from the manual segmentations (ICC = 0.77±0.17). Furthermore, we found that features extracted from 3D-Slicer segmentations were more robust, as the range was significantly smaller across observers (p = 3.819e-07), and overlapping with the feature ranges extracted from manual contouring (boundary lower: p = 0.007, higher: p = 5.863e-06). Our results show that 3D-Slicer segmented tumor volumes provide a better alternative to the manual delineation for feature quantification, as they yield more reproducible imaging descriptors. Therefore, 3D-Slicer can be employed for quantitative image feature extraction and image data mining research in large patient cohorts.},
author = {Parmar, Chintan and Rios-Velazquez, Emmanuel and Leijenaar, Ralph T. H. and Jermoumi, Mohammed and Carvalho, Sara and Mak, Raymond H. and Mitra, Sushmita and Shankar, B. Uma and Kikinis, Ron and Haibe-Kains, Benjamin and Lambin, Philippe and Aerts, Hugo J. W. L.},
doi = {10.1371/journal.pone.0102107},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Parmar2014 - Robust radiomics feature quantification using semiautomatic volumetric segmentations.pdf:pdf},
isbn = {1932-6203 (Electronic) 1932-6203 (Linking)},
issn = {1932-6203},
journal = {PloS one},
number = {7},
pages = {e102107},
pmid = {25025374},
title = {{Robust Radiomics feature quantification using semiautomatic volumetric segmentation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25025374 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4098900},
volume = {9},
year = {2014}
}
@article{Narang2016,
abstract = {Gliomas are tumors which develop in the brain, the most aggressive of which is glioblastoma multiforme (GBM). Despite extensive research to better understand the underlying biology of GBM and advancements in the treatment of this disease, it has an extremely poor prognosis. Poor outcomes in GBM are due to its molecular and clinical heterogeneity, as in other solid tumors. As imaging approaches have been taken to comprehensively characterize tumors, radiomics has emerged as the concept of extracting quantitative radiologic features and drawing associations with clinical outcomes. Radiomics has the potential to improve the predictive ability of radiological datasets. Tumor radiographs are segmented through manual, semi-automated or fully-automated procedures. Segmentation is followed by feature extraction from the tumor volume. Data analysis and predictive modeling are used to relate image-derived features with clinical outcomes. Substantial progress has already been made in solving many of the technical hurdles inherent in the radiomics process. Advances in sequencing, gene expression profiling and machine learning have increased the resolution of datasets and improved the sensitivity and specificity of computational methods used to analyze them. Numerous logistical, computational and clinical challenges remain to unlocking the full potential of the radiomics approach. To make the approach useful in clinical practice, improved statistical models are needed which relate GBM imaging features with patient outcomes with high specificity/sensitivity. More studies correlating radiomic features with disease outcomes and molecular attributes are also needed to illuminate the tumor biology which gives rise to imaging features and underlie response to therapy.},
author = {Narang, Shivali and Lehrer, Michael and Yang, Dalu and Lee, Joonsang and Rao, Arvind},
doi = {10.21037/tcr.2016.06.31},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Narang2016 - Radiomics in glioblastomas - current status challenges and potential opportunities.pdf:pdf},
issn = {2218676X},
journal = {Translational Cancer Research},
keywords = {computed tomography,ct,doi,emission tomography,glioblastoma,magnetic resonance imaging,mri,pet,positron,radiomics,tcr},
month = {aug},
number = {4},
pages = {383--397},
title = {{Radiomics in glioblastoma: current status, challenges and potential opportunities}},
url = {http://tcr.amegroups.com/article/view/8806/7741},
volume = {5},
year = {2016}
}
@article{Molina2017,
abstract = {{\textcopyright} 2017 Molina et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.Purpose: Textural measures have been widely explored as imaging biomarkers in cancer. However, their robustness under dynamic range and spatial resolution changes in brain 3D magnetic resonance images (MRI) has not been assessed. The aim of this work was to study potential variations of textural measures due to changes in MRI protocols. Materials and methods: Twenty patients harboring glioblastoma with pretreatment 3D T1-weighted MRIs were included in the study. Four different spatial resolution combinations and three dynamic ranges were studied for each patient. Sixteen three-dimensional textural heterogeneity measures were computed for each patient and configuration including co-occurrence matrices (CM) features and run-length matrices (RLM) features. The coefficient of variation was used to assess the robustness of the measures in two series of experiments corresponding to (i) changing the dynamic range and (ii) changing the matrix size. Results: No textural measures were robust under dynamic range changes. Entropy was the only textural feature robust under spatial resolution changes (coefficient of variation under 10{\%} in all cases). Conclusion: Textural measures of three-dimensional brain tumor images are not robust neither under dynamic range nor under matrix size changes. Standards should be harmonized to use textural features as imaging biomarkers in radiomic-based studies. The implications of this work go beyond the specific tumor type studied here and pose the need for standardization in textural feature calculation of oncological images.},
author = {Molina, David and P{\'{e}}rez-Beteta, Juli{\'{a}}n and Mart{\'{i}}nez-Gonz{\'{a}}lez, Alicia and Martino, Juan and Velasquez, Carlos and Arana, Estanislao and P{\'{e}}rez-Garc{\'{i}}a, V{\'{i}}ctor M.},
doi = {10.1371/journal.pone.0178843},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Molina2017 - Lack of robustness of textural measures MRI.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {6},
pages = {1--14},
title = {{Lack of robustness of textural measures obtained from 3D brain tumor MRIs impose a need for standardization}},
volume = {12},
year = {2017}
}
@article{Grossmann2017,
abstract = {Medical imaging can visualize characteristics of human cancer noninvasively. Radiomics is an emerging field that translates these medical images into quantitative data to enable phenotypic profiling of tumors. While radiomics has been associated with several clinical endpoints, the complex relationships of radiomics, clinical factors, and tumor biology are largely unknown. To this end, we analyzed two independent cohorts of respectively 262 North American and 89 European patients with lung cancer, and consistently identified previously undescribed associations between radiomic imaging features, molecular pathways, and clinical factors. In particular, we found a relationship between imaging features, immune response, inflammation, and survival, which was further validated by immunohistochemical staining. Moreover, a number of imaging features showed predictive value for specific pathways; for example, intra-tumor heterogeneity features predicted activity of RNA polymerase transcription (AUC = 0.62, p=0.03) and intensity dispersion was predictive of the autodegration pathway of a ubiquitin ligase (AUC = 0.69, p{\textless}10(-4)). Finally, we observed that prognostic biomarkers performed highest when combining radiomic, genetic, and clinical information (CI = 0.73, p{\textless}10(-9)) indicating complementary value of these data. In conclusion, we demonstrate that radiomic approaches permit noninvasive assessment of both molecular and clinical characteristics of tumors, and therefore have the potential to advance clinical decision-making by systematically analyzing standard-of-care medical images.},
author = {Grossmann, Patrick and Stringfield, Olya and El-Hachem, Nehme and Bui, Marilyn M. and {Rios Velazquez}, Emmanuel and Parmar, Chintan and Leijenaar, Ralph T. H. and Haibe-Kains, Benjamin and Lambin, Philippe and Gillies, Robert J. and Aerts, Hugo J. W. L.},
doi = {10.7554/eLife.23421},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Grossmann2017 - Defining the biological basis of radiomics phenotypes in lung cancer.pdf:pdf},
isbn = {1617525715},
issn = {2050-084X},
journal = {eLife},
keywords = {cancer biology,computational biology,genomics,human,imaging,oncology,radiomics,systems biology},
month = {jul},
pmid = {28731408},
title = {{Defining the biological basis of radiomic phenotypes in lung cancer.}},
url = {https://elifesciences.org/articles/23421 http://www.ncbi.nlm.nih.gov/pubmed/28731408 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5590809},
volume = {6},
year = {2017}
}
@article{Graf1999,
abstract = {Prognostic classification schemes have often been used in medical applications, but rarely subjected to a rigorous examination of their adequacy. For survival data, the statistical methodology to assess such schemes consists mainly of a range of ad hoc approaches, and there is an alarming lack of commonly accepted standards in this field. We review these methods and develop measures of inaccuracy which may be calculated in a validation study in order to assess the usefulness of estimated patient-specific survival probabilities associated with a prognostic classification scheme. These measures are meaningful even when the estimated probabilities are misspecified, and asymptotically they are not affected by random censorship. In addition, they can be used to derive R(2)-type measures of explained residual variation. A breast cancer study will serve for illustration throughout the paper.},
author = {Graf, Erika and Schmoor, Claudia and Sauerbrei, Willi and Schumacher, Martin},
doi = {10.1002/(SICI)1097-0258(19990915/30)18:17/18<2529::AID-SIM274>3.0.CO;2-5},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Graf1999 - Assessment and comparison of prognostic classsification schemes for survival data.pdf:pdf},
isbn = {0277-6715 (Print)},
issn = {0277-6715},
journal = {Statistics in medicine},
number = {17-18},
pages = {2529--45},
pmid = {10474158},
title = {{Assessment and comparison of prognostic classification schemes for survival data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10474158},
volume = {18},
year = {1999}
}
@article{Forghani2017,
abstract = {In the last article of this issue, advanced analysis capabilities of DECT is reviewed, including spectral Hounsfield unit attenuation curves, virtual monochromatic images, material decomposition maps, tissue effective Z determination, and other advanced post-processing DECT tools, followed by different methods of analysis of the attenuation curves generated using DECT. The article concludes with exciting future horizons and potential applications, such as the use of the rich quantitative data in dual energy CT scans for texture or radiomic analysis and the use of machine learning methods for generation of prediction models using spectral data.},
author = {Forghani, Reza and Srinivasan, Ashok and Forghani, Behzad},
doi = {10.1016/j.nic.2017.04.007},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Forghani2017 - Advanced Tissue Characterization and Texture Analysis Using Dual-Energy Computed Tomography.pdf:pdf},
issn = {1557-9867},
journal = {Neuroimaging clinics of North America},
keywords = {Dual-energy CT,Head and neck squamous cell carcinoma,Machine learning,Material decomposition iodine maps,Radiomic analysis,Spectral Hounsfield unit attenuation curves,Texture analysis,Virtual monochromatic images},
month = {aug},
number = {3},
pages = {533--546},
pmid = {28711211},
publisher = {Elsevier Inc},
title = {{Advanced Tissue Characterization and Texture Analysis Using Dual-Energy Computed Tomography: Horizons and Emerging Applications.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S105251491730028X http://www.ncbi.nlm.nih.gov/pubmed/28711211},
volume = {27},
year = {2017}
}
@article{Hu1962,
abstract = {In this paper a theory of two-dimensional moment invariants for planar geometric figures is presented. A fundamental theorem is established to relate such moment invariants to the well-known algebraic invariants. Complete systems of moment invariants under translation, similitude and orthogonal transformations are derived. Some moment invariants under general two-dimensional linear transformations are also included. Both theoretical formulation and practical models of visual pattern recognition based upon these moment invariants are discussed. A simple simulation program together with its performance are also presented. It is shown that recognition of geometrical patterns and alphabetical characters independently of position, size and orientation can be accomplished. It is also indicated that generalization is possible to include invariance with parallel projection.},
author = {Hu, Ming-Kuei},
doi = {10.1109/TIT.1962.1057692},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Hu1962 - Visual pattern recognition by moment invariants.pdf:pdf},
isbn = {0096-1000},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
keywords = {geometric,invariants,moments,pattern,recognition},
month = {feb},
number = {2},
pages = {179--187},
title = {{Visual pattern recognition by moment invariants}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1057692 http://ieeexplore.ieee.org/document/1057692/},
volume = {8},
year = {1962}
}
@article{Zhao2015,
author = {Zhao, Binsheng and Schwartz, Lawrence H. and Kris, Mark G.},
doi = {http://doi.org/10.7937/K9/TCIA.2015.U1X8A5NR},
journal = {The Cancer Imaging Archive},
title = {{Data From RIDER{\_}Lung CT}},
year = {2015}
}
@article{Zhao2009,
abstract = {PURPOSE To evaluate the variability of tumor unidimensional, bidimensional, and volumetric measurements on same-day repeat computed tomographic (CT) scans in patients with non-small cell lung cancer. MATERIALS AND METHODS This HIPAA-compliant study was approved by the institutional review board, with informed patient consent. Thirty-two patients with non-small cell lung cancer, each of whom underwent two CT scans of the chest within 15 minutes by using the same imaging protocol, were included in this study. Three radiologists independently measured the two greatest diameters of each lesion on both scans and, during another session, measured the same tumors on the first scan. In a separate analysis, computer software was applied to assist in the calculation of the two greatest diameters and the volume of each lesion on both scans. Concordance correlation coefficients (CCCs) and Bland-Altman plots were used to assess the agreements between the measurements of the two repeat scans (reproducibility) and between the two repeat readings of the same scan (repeatability). RESULTS The reproducibility and repeatability of the three radiologists' measurements were high (all CCCs, {\textgreater}or=0.96). The reproducibility of the computer-aided measurements was even higher (all CCCs, 1.00). The 95{\%} limits of agreements for the computer-aided unidimensional, bidimensional, and volumetric measurements on two repeat scans were (-7.3{\%}, 6.2{\%}), (-17.6{\%}, 19.8{\%}), and (-12.1{\%}, 13.4{\%}), respectively. CONCLUSION Chest CT scans are well reproducible. Changes in unidimensional lesion size of 8{\%} or greater exceed the measurement variability of the computer method and can be considered significant when estimating the outcome of therapy in a patient.},
author = {Zhao, Binsheng and James, Leonard P. and Moskowitz, Chaya S. and Guo, Pingzhen and Ginsberg, Michelle S. and Lefkowitz, Robert A. and Qin, Yilin and Riely, Gregory J. and Kris, Mark G. and Schwartz, Lawrence H.},
doi = {10.1148/radiol.2522081593},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Zhao2009 - Evaluating variability in tumour measurements from same-day repeat ct scans of patients with non-small cell lung cancer.pdf:pdf},
isbn = {1527-1315 (Electronic)$\backslash$n0033-8419 (Linking)},
issn = {1527-1315},
journal = {Radiology},
month = {jul},
number = {1},
pages = {263--72},
pmid = {19561260},
title = {{Evaluating variability in tumor measurements from same-day repeat CT scans of patients with non-small cell lung cancer.}},
url = {http://pubs.rsna.org/doi/10.1148/radiol.2522081593 http://www.ncbi.nlm.nih.gov/pubmed/19561260 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2797680},
volume = {252},
year = {2009}
}
@article{Sollini2017,
abstract = {Imaging with positron emission tomography (PET)/computed tomography (CT) is crucial in the management of cancer because of its value in tumor staging, response assessment, restaging, prognosis and treatment responsiveness prediction. In the last years, interest has grown in texture analysis which provides an "in-vivo" lesion characterization, and predictive information in several malignances including NSCLC; however several drawbacks and limitations affect these studies, especially because of lack of standardization in features calculation, definitions and methodology reporting. The present paper provides a comprehensive review of literature describing the state-of-the-art of FDG-PET/CT texture analysis in NSCLC, suggesting a proposal for harmonization of methodology.},
author = {Sollini, M. and Cozzi, L. and Antunovic, L. and Chiti, A. and Kirienko, M.},
doi = {10.1038/s41598-017-00426-y},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Sollini2017 - PET radiomics in NSCLC - state of the art and a proposal for harmonization of methodology.pdf:pdf},
isbn = {2045-2322 (Electronic)$\backslash$r2045-2322 (Linking)},
issn = {2045-2322},
journal = {Scientific reports},
month = {mar},
number = {1},
pages = {358},
pmid = {28336974},
publisher = {Springer US},
title = {{PET Radiomics in NSCLC: state of the art and a proposal for harmonization of methodology.}},
url = {http://www.nature.com/articles/s41598-017-00426-y http://www.ncbi.nlm.nih.gov/pubmed/28336974 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5428425},
volume = {7},
year = {2017}
}
@article{Battiti1994,
abstract = {This paper investigates the application of the mutual information criterion to evaluate a set of candidate features and to select an informative subset to be used as input data for a neural network classifier. Because the mutual information measures arbitrary dependencies between random variables, it is suitable for assessing the "information content" of features in complex classification tasks, where methods bases on linear relations (like the correlation) are prone to mistakes. The fact that the mutual information is independent of the coordinates chosen permits a robust estimation. Nonetheless, the use of the mutual information for tasks characterized by high input dimensionality requires suitable approximations because of the prohibitive demands on computation and samples. An algorithm is proposed that is based on a "greedy" selection of the features and that takes both the mutual information with respect to the output class and with respect to the already-selected features into account. Finally the results of a series of experiments are discussed.},
author = {Battiti, Roberto},
doi = {10.1109/72.298224},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Battiti1994 - Using mutual information for selecting features in supervised neural net learning.pdf:pdf},
isbn = {1045-9227},
issn = {1045-9227},
journal = {IEEE transactions on neural networks},
number = {4},
pages = {537--50},
pmid = {18267827},
title = {{Using mutual information for selecting features in supervised neural net learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18267827},
volume = {5},
year = {1994}
}
@article{Frings2014,
abstract = {PURPOSE To evaluate the feasibility and repeatability of various metabolically active tumor volume ( MATV metabolically active tumor volume ) quantification methods in fluorine 18 fluorodeoxyglucose ( FDG fluorine 18 fluorodeoxyglucose ) positron emission tomography (PET)/computed tomography (CT) in a multicenter setting and propose the optimal MATV metabolically active tumor volume method together with the minimal threshold for future response evaluation studies. MATERIALS AND METHODS The study was approved by the institutional review board of all four participating centers, and patients provided written informed consent. Thirty-four patients with advanced gastrointestinal malignancies underwent two FDG fluorine 18 fluorodeoxyglucose PET/CT examinations within 1 week. MATV metabolically active tumor volume s were defined semiautomatically with 27 variations of tumor delineation methods with different reference values. Feasibility was determined as the percentage of successful tumor segmentations per MATV metabolically active tumor volume method. Repeatability was determined with intraclass correlation coefficients, Bland-Altman plots, and limits of agreement ( LOA limit of agreement s) of the percentage difference between the test and repeat test measurements. In addition, LOA limit of agreement variability per center was investigated. RESULTS In total, 136 lesions were identified. Feasibility of tumor segmentation ranged from 54{\%} to 100{\%} (74-136 of 136 lesions); repeatability was evaluated for 19 MATV metabolically active tumor volume methods with feasibility of greater than 95{\%}. The median MATV metabolically active tumor volume derived with 50{\%} threshold of mean standardized uptake value ( SUV standardized uptake value ) of a sphere of 12-mm diameter with highest local intensity ( SUVhp mean SUV of a sphere of 12-mm diameter with highest local intensity ), which may not include the voxel with highest SUV standardized uptake value corrected for local background, was 5.7 and 6.1 mL for test and retest scans, respectively, with a relative LOA limit of agreement of 36.1{\%}. Comparable repeatability was found between centers. A difference in uptake time between scan 1 and 2 of 15 minutes or longer had a minor negative influence on repeatability. CONCLUSION MATV metabolically active tumor volume measured with 50{\%} of SUVhp mean SUV of a sphere of 12-mm diameter with highest local intensity corrected for local background is recommended in multicenter FDG fluorine 18 fluorodeoxyglucose PET/CT studies on the basis of a high feasibility (96{\%}) and repeatability ( LOA limit of agreement of 36.1{\%}).},
author = {Frings, Virginie and van Velden, Floris H. P. and Velasquez, Linda M. and Hayes, Wendy and van de Ven, Peter M. and Hoekstra, Otto S. and Boellaard, Ronald},
doi = {10.1148/radiol.14132807},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Frings2014 - Repeatability of metabolically active tumor volume measurements with FDG PETCT in advanced gastrointestinal malignancies.pdf:pdf},
isbn = {1527-1315 (Electronic)$\backslash$r0033-8419 (Linking)},
issn = {1527-1315},
journal = {Radiology},
month = {nov},
number = {2},
pages = {539--48},
pmid = {24865311},
title = {{Repeatability of metabolically active tumor volume measurements with FDG PET/CT in advanced gastrointestinal malignancies: a multicenter study.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24865311},
volume = {273},
year = {2014}
}
@article{Bogowicz2017,
abstract = {{\textcopyright} 2017 Acta Oncologica Foundation Purpose: An association between radiomic features extracted from CT and local tumor control in the head and neck squamous cell carcinoma (HNSCC) has been shown. This study investigated the value of pretreatment functional imaging (18F-FDG PET) radiomics for modeling of local tumor control. Material and Methods: Data from HNSCC patients (n = 121) treated with definitive radiochemotherapy were used for model training. In total, 569 radiomic features were extracted from both contrast-enhanced CT and 18F-FDG PET images in the primary tumor region. CT, PET and combined PET/CT radiomic models to assess local tumor control were trained separately. Five feature selection and three classification methods were implemented. The performance of the models was quantified using concordance index (CI) in 5-fold cross validation in the training cohort. The best models, per image modality, were compared and verified in the independent validation cohort (n = 51). The difference in CI was investigated using bootstrapping. Additionally, the observed and radiomics-based estimated probabilities of local tumor control were compared between two risk groups. Results: The feature selection using principal component analysis and the classification based on the multivariabale Cox regression with backward selection of the variables resulted in the best models for all image modalities (CI CT = 0.72, CI PET = 0.74, CI PET/CT = 0.77). Tumors more homogenous in CT density (decreased GLSZM size{\_}zone{\_}entropy ) and with a focused region of high FDG uptake (higher GLSZM SZLGE ) indicated better prognosis. No significant difference in the performance of the models in the validation cohort was observed (CI CT = 0.73, CI PET = 0.71, CI PET/CT = 0.73). However, the CT radiomics-based model overestimated the probability of tumor control in the poor prognostic group (predicted = 68{\%}, observed = 56{\%}). Conclusions: Both CT and PET radiomics showed equally good discriminative power for local tumor control modeling in HNSCC. However, CT-based predictions overestimated the local control rate in the poor prognostic validation cohort, and thus, we recommend to base the local control modeling on the 18F-FDG PET.},
author = {Bogowicz, M. and Riesterer, O. and Stark, L.S. and Studer, G. and Unkelbach, J. and Guckenberger, M. and Tanadini-Lang, S.},
doi = {10.1080/0284186X.2017.1346382},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Bogowicz2017 - Comparison of PET and CT radiomics for prediction of local tumor control in head and neck squamous cell carcinoma - Supplementar.pdf:pdf},
isbn = {1651-226X (Electronic) 0284-186X (Linking)},
issn = {1651226X},
journal = {Acta Oncologica},
pmid = {28820287},
title = {{Comparison of PET and CT radiomics for prediction of local tumor control in head and neck squamous cell carcinoma - SUPPLEMENT}},
year = {2017}
}
@article{Bogowicz2016a,
abstract = {This study aimed to identify a set of stable radiomic parameters in CT perfusion (CTP) maps with respect to CTP calculation factors and image discretization, as an input for future prognostic models for local tumor response to chemo-radiotherapy. Pre-treatment CTP images of eleven patients with oropharyngeal carcinoma and eleven patients with non-small cell lung cancer (NSCLC) were analyzed. 315 radiomic parameters were studied per perfusion map (blood volume, blood flow and mean transit time). Radiomics robustness was investigated regarding the potentially standardizable (image discretization method, Hounsfield unit (HU) threshold, voxel size and temporal resolution) and non-standardizable (artery contouring and noise threshold) perfusion calculation factors using the intraclass correlation (ICC). To gain added value for our model radiomic parameters correlated with tumor volume, a well-known predictive factor for local tumor response to chemo-radiotherapy, were excluded from the analysis. The remaining stable radiomic parameters were grouped according to inter-parameter Spearman correlations and for each group the parameter with the highest ICC was included in the final set. The acceptance level was 0.9 and 0.7 for the ICC and correlation, respectively. The image discretization method using fixed number of bins or fixed intervals gave a similar number of stable radiomic parameters (around 40{\%}). The potentially standardizable factors introduced more variability into radiomic parameters than the non-standardizable ones with 56-98{\%} and 43-58{\%} instability rates, respectively. The highest variability was observed for voxel size (instability rate {\textgreater}97{\%} for both patient cohorts). Without standardization of CTP calculation factors none of the studied radiomic parameters were stable. After standardization with respect to non-standardizable factors ten radiomic parameters were stable for both patient cohorts after correction for inter-parameter correlations. Voxel size, image discretization, HU threshold and temporal resolution have to be standardized to build a reliable predictive model based on CTP radiomics analysis.},
author = {Bogowicz, Marta and Riesterer, Oliver and Bundschuh, R. A. and Veit-Haibach, Patrick and H{\"{u}}llner, M. and Studer, Gabriela and Stieb, S. and Glatz, S. and Pruschy, M. and Guckenberger, Matthias and Tanadini-Lang, Stephanie},
doi = {10.1088/1361-6560/61/24/8736},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Bogowicz2016 - Stability of radiomic features in CT perfusion maps - supplement.pdf:pdf},
isbn = {1361-6560 (Electronic) 0031-9155 (Linking)},
issn = {1361-6560},
journal = {Physics in medicine and biology},
month = {dec},
number = {24},
pages = {8736--8749},
pmid = {27893446},
title = {{Stability of radiomic features in CT perfusion maps.}},
url = {http://stacks.iop.org/0031-9155/61/i=24/a=8736?key=crossref.209310b028dbf6bd86061a07ef6efc33 http://www.ncbi.nlm.nih.gov/pubmed/27893446},
volume = {61},
year = {2016}
}
@inproceedings{Bergstra2011,
abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel ap-proaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it pos-sible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neu-ral networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the ex-pected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreli-able for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Bergstra, James and Bardenet, R{\'{e}}mi and Bengio, Yoshua and K{\'{e}}gl, Bal{\'{a}}zs},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {2012arXiv1206.2944S},
editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
eprint = {1206.2944},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Bergstra2011 - Algorithms for hyper-parameter optimization.pdf:pdf},
isbn = {9781618395993},
issn = {10495258},
pages = {2546--2554},
publisher = {Curran Associates, Inc.},
title = {{Algorithms for Hyper-Parameter Optimization}},
year = {2011}
}
@article{Albu2008,
abstract = {This paper proposes a new morphology-based approach for the interslice interpolation of current transformer (CT) and MRI datasets composed of parallel slices. Our approach is object based and accepts as input data binary slices belonging to the same anatomical structure. Such slices may contain one or more regions, since topological changes between two adjacent slices may occur. Our approach handles explicitly interslice topology changes by decomposing a many-to-many correspondence into three fundamental cases: one-to-one, one-to-many, and zero-to-one correspondences. The proposed interpolation process is iterative. One iteration of this process computes a transition sequence between a pair of corresponding input slices, and selects the element located at equal distance from the input slices. This algorithmic design yields a gradual, smooth change of shape between the input slices. Therefore, the main contribution of our approach is its ability to interpolate between two anatomic shapes by creating a smooth, gradual change of shape, and without generating over-smoothed interpolated shapes.},
author = {Albu, Alexandra Branzan and Beugeling, Trevor and Laurendeau, Denis},
doi = {10.1109/TBME.2008.921158},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Albu2008 - A Morphology-Based Approach for Interslice Interpolation of Anatomical Slices From Volumetric Images.pdf:pdf},
isbn = {0018-9294 VO - 55},
issn = {1558-2531},
journal = {IEEE transactions on bio-medical engineering},
keywords = {Mathematical morphology,Shape-based interpolation,Volumetric imaging},
month = {aug},
number = {8},
pages = {2022--38},
pmid = {18632365},
title = {{A morphology-based approach for interslice interpolation of anatomical slices from volumetric images.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18632365},
volume = {55},
year = {2008}
}
@article{Apostolova2014,
abstract = {OBJECTIVE To propose a novel measure, namely the 'asphericity' (ASP), of spatial irregularity of FDG uptake in the primary tumour as a prognostic marker in head-and-neck cancer. METHODS PET/CT was performed in 52 patients (first presentation, n = 36; recurrence, n = 16). The primary tumour was segmented based on thresholding at the volume-reproducible intensity threshold after subtraction of the local background. ASP was used to characterise the deviation of the tumour's shape from sphere symmetry. Tumour stage, tumour localisation, lymph node metastases, distant metastases, SUVmax, SUVmean, metabolic tumour volume (MTV) and total lesion glycolysis (TLG) were also considered. The association of overall (OAS) and progression-free survival (PFS) with these parameters was analysed. RESULTS Cox regression revealed high SUVmax [hazard ratio (HR) = 4.4/7.4], MTV (HR = 4.6/5.7), TLG (HR = 4.8/8.9) and ASP (HR = 7.8/7.4) as significant predictors with respect to PFS/OAS in case of first tumour manifestation. The combination of high MTV and ASP showed very high HRs of 22.7 for PFS and 13.2 for OAS. In case of recurrence, MTV (HR = 3.7) and the combination of MTV/ASP (HR = 4.2) were significant predictors of PFS. CONCLUSIONS ASP of pretherapeutic FDG uptake in the primary tumour improves the prediction of tumour progression in head-and-neck cancer at first tumour presentation. KEY POINTS Asphericity (ASP) characterises the spatial heterogeneity of FDG uptake in tumours. ASP is a promising prognostic parameter in head-and-neck cancer. ASP is useful for identification of high-risk patients with head-and-neck cancer.},
author = {Apostolova, Ivayla and Steffen, Ingo G. and Wedel, Florian and Lougovski, Alexandr and Marnitz, Simone and Derlin, Thorsten and Amthauer, Holger and Buchert, Ralph and Hofheinz, Frank and Brenner, Winfried},
doi = {10.1007/s00330-014-3269-8},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Apostolova2014 - Asphericity of pretherapeutic tumour FDG uptake provides independent prognostic value in head-and-neck cancer.pdf:pdf},
isbn = {1432-1084 (Electronic) 0938-7994 (Linking)},
issn = {1432-1084},
journal = {European radiology},
keywords = {Asphericity,FDG PET,Head-and-neck cancer,Heterogeneity,Prognosis},
month = {sep},
number = {9},
pages = {2077--87},
pmid = {24965509},
title = {{Asphericity of pretherapeutic tumour FDG uptake provides independent prognostic value in head-and-neck cancer.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24965509},
volume = {24},
year = {2014}
}
@incollection{Solomon2011,
address = {Chichester, UK},
author = {Solomon, Chris and Breckon, Toby},
booktitle = {Fundamentals of Digital Image Processing},
chapter = {9},
doi = {10.1002/9780470689776.ch9},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Solomon2011 - Fundamentals of Digital Image Processing - Features.pdf:pdf},
isbn = {9780470844724},
month = {jan},
pages = {235--262},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Features}},
url = {http://doi.wiley.com/10.1002/9780470689776.ch9},
year = {2011}
}
@incollection{Solomon2011a,
address = {Chichester, UK},
author = {Solomon, Chris and Breckon, Toby},
booktitle = {Fundamentals of Digital Image Processing},
chapter = {4},
doi = {10.1002/9780470689776.ch4},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Solomon2011 - Fundamentals of Digital Image Processing - Enhancement.pdf:pdf},
isbn = {9780470844724},
month = {jan},
pages = {85--111},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Enhancement}},
url = {http://doi.wiley.com/10.1002/9780470689776.ch4},
year = {2011}
}
@incollection{Depeursinge2017,
abstract = {This chapter clarifies the important aspects of biomedical texture analysis under the general framework introduced in Chapter 1. It was proposed that any approach can be character- ized as the combination of local texture operators and regional aggregation functions. The type of scale and directional information that can or cannot be modeled by categories of texture processing methods is revealed through theoretic analyses and experimental valida- tions. Several key aspects are found to be commonly overlooked in the literature and are highlighted. First, we demonstrate the risk of using regions of interest for aggregation that are regrouping tissue types of different natures. Second, a detailed study of the type of directional information important for biomedical texture characterization suggests that fun- damental properties lie in the local organization of image directions. In addition, it was found that most approaches cannot efficiently characterize the latter, and even fewer can do it with invariance to local rotations. We conclude by deriving novel comparison axes to evaluate the relevance of biomedical texture analysis methods in a specific medical or biological applicative context.},
address = {London, UK},
author = {Depeursinge, Adrien},
booktitle = {Biomedical texture analysis},
chapter = {2},
edition = {1st},
editor = {Depeursinge, Adrien and Fageot, Julien and Al-Kadi, Omar},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/DePeursinge2017 - Multi-scale and multi-directional biomedical texture analysis.pdf:pdf},
isbn = {9780128121337},
keywords = {Texton,moving frames,texture analysis,uncertainty principle},
pages = {37--62},
publisher = {Academic Press},
title = {{Multi-Scale and Multi-Directional Biomedical Texture Analysis}},
year = {2017}
}
@incollection{Depeursinge2017b,
abstract = {This chapter aims to provide an overview of the foundations of texture processing for biomed- ical image analysis. Its purpose is to define precisely what biomedical texture is, how is it different from general texture information considered in computer vision, and what is the general problem formulation to translate 2D and 3D textured patterns from biomedical images to visually and biologically relevant measurements. First, a formal definition of biomedical tex- ture information is proposed from both perceptual and mathematical point of views. Second, a general problem formulation for biomedical texture analysis is introduced, considering that any approach can be characterized as a set of local texture operators and regional aggregation functions. The operators allow locally isolating desired texture information in terms of spatial scales and directions of a texture image. The type of desirable operator invariances are discussed, and are found to be different from photographic image analysis. Scalar-valued texture measurements are obtained by aggregating operator's response maps over regions of interest.},
address = {London, UK},
author = {Depeursinge, Adrien and Fageot, Julien and Al-Kadi, Omar},
booktitle = {Biomedical texture analysis},
chapter = {1},
edition = {1st},
editor = {Depeursinge, Adrien and Fageot, Julien and Al-Kadi, Omar},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/DePeursinge2017 - Fundamentals of texture processing for biomedical image analysis.pdf:pdf},
isbn = {9780128121337},
keywords = {Quantitative image analysis,heterogeneity,spatial stochastic process,texton,texture analysis},
pages = {7--35},
publisher = {Academic Press},
title = {{Fundamentals of Texture Processing for Biomedical Image Analysis}},
year = {2017}
}
@incollection{Depeursinge2017a,
abstract = {This chapter reviews most popular texture analysis approaches under novel comparison axes that are specific to biomedical imaging. A concise checklist is proposed as a user guide to assess the relevance of each approach for a particular medical or biological task in hand. We revealed that few approaches are regrouping most of the desirable properties for achieving optimal performance. In particular, moving frames texture representations based on learned steerable Riesz operators showed to enable data-specific and rigid-transformation- invariant characterization of local directional patterns, the latter being a fundamental property of biomedical textures. Potential limitations of having recourse to data augmentation and transfer learning for deep convolutional neural networks and dictionary learning approaches to palliate the lack of large annotated training collections in biomedical imaging are mentioned. We conclude by summarizing the strengths and limitations of current approaches, providing insights on key aspects required to build the next generation of biomedical texture analysis approaches.},
address = {London, UK},
author = {Depeursinge, Adrien and Fageot, Julien},
booktitle = {Biomedical texture analysis},
chapter = {3},
edition = {1st},
editor = {Depeursinge, Adrien and Fageot, Julien and Al-Kadi, Omar},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/DePeursinge2017 - Biomedical texture operators and aggregation functions - a methodological review and user's guide.pdf:pdf},
isbn = {9780128121337},
keywords = {Biomedical texture analysis,Gabor filters,convolutional neural networks,deep learning,dictionary learning,gray-level co-occurrence matrices,gray-level run-length matrices,gray-level size-zone matrices,local binary patterns,steerable wavelets.},
pages = {63--101},
publisher = {Academic Press},
title = {{Biomedical Texture Operators and Aggregation Functions}},
year = {2017}
}
@article{Aide2017,
abstract = {Quantitative positron emission tomography/ computed tomography (PET/CT) can be used as diagnostic or prognostic tools (i.e. single measurement) or for therapy monitoring (i.e. longitudinal studies) in multicentre studies. Use of quantitative parameters, such as standardized uptake values (SUVs), metabolic active tumor volumes (MATVs) or total lesion glycolysis (TLG), in a multicenter setting requires that these parameters be comparable among patients and sites, regardless of the PET/CT system used. This review describes the motivations and the methodologies for quantitative PET/ CT performance harmonization with emphasis on the EANM Research Ltd. (EARL) Fluorodeoxyglucose (FDG) PET/CT accreditation program, one of the international harmonization programs aiming at using FDG PET as a quantitative imaging biomarker. In addition, future accreditation initiatives will be discussed. The validation of the EARL accreditation program to harmonize SUVs and MATVs is described in a wide range of tumor types, with focus on therapy assessment using either the European Organization for Research and Treatment of Cancer (EORTC) criteria or PET Evaluation Response Criteria in Solid Tumors (PERCIST), as well as liver-based scales such as the Deauville score. Finally, also presented in this paper are the results from a survey across 51 EARL-accredited centers reporting how the program was implement-ed and its impact on daily routine and in clinical trials, harmo-nization of new metrics such as MATV and heterogeneity features.},
author = {Aide, Nicolas and Lasnon, Charline and Veit-Haibach, Patrick and Sera, Terez and Sattler, Bernhard and Boellaard, Ronald},
doi = {10.1007/s00259-017-3740-2},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Aide2017 - EANM-EARL harmonization strategies in PET quantification - from daily practice to multicentre oncological studies.pdf:pdf},
isbn = {0025901737},
issn = {1619-7070},
journal = {European Journal of Nuclear Medicine and Molecular Imaging},
keywords = {Deauville score,EARL accreditation,EORTC,Harmonization,MATV,PERCIST,PET/CT,SUV},
month = {aug},
number = {S1},
pages = {17--31},
publisher = {European Journal of Nuclear Medicine and Molecular Imaging},
title = {{EANM/EARL harmonization strategies in PET quantification: from daily practice to multicentre oncological studies}},
url = {http://link.springer.com/10.1007/s00259-017-3740-2},
volume = {44},
year = {2017}
}
@article{Leger2017,
author = {Leger, Stefan and Zwanenburg, Alex and Pilz, Karoline and Lohaus, Fabian and Linge, Annett and Z{\"{o}}phel, Klaus and Kotzerke, J{\"{o}}rg and Schreiber, Andreas and Tinhofer, Inge and Budach, Volker and Sak, Ali and Stuschke, Martin and Balermpas, Panagiotis and R{\"{o}}del, Claus and Ganswindt, Ute and Belka, Claus and Pigorsch, Steffi and Combs, Stephanie E. and M{\"{o}}nnich, David and Zips, Daniel and Krause, Mechthild and Baumann, Michael and Troost, Esther G. C. and L{\"{o}}ck, Steffen and Richter, Christian},
doi = {10.1038/s41598-017-13448-3},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Leger2017 - A comparative study of machine learning methods for time-to-event survival data for radiomics risk modelling.pdf:pdf},
issn = {2045-2322},
journal = {Scientific Reports},
month = {dec},
number = {1},
pages = {13206},
title = {{A comparative study of machine learning methods for time-to-event survival data for radiomics risk modelling}},
url = {http://www.nature.com/articles/s41598-017-13448-3},
volume = {7},
year = {2017}
}
@article{Larue2017,
abstract = {BACKGROUND Radiomic analyses of CT images provide prognostic information that can potentially be used for personalized treatment. However, heterogeneity of acquisition- and reconstruction protocols influences robustness of radiomic analyses. The aim of this study was to investigate the influence of different CT-scanners, slice thicknesses, exposures and gray-level discretization on radiomic feature values and their stability. MATERIAL AND METHODS A texture phantom with ten different inserts was scanned on nine different CT-scanners with varying tube currents. Scans were reconstructed with 1.5 mm or 3 mm slice thickness. Image pre-processing comprised gray-level discretization in ten different bin widths ranging from 5 to 50 HU and different resampling methods (i.e., linear, cubic and nearest neighbor interpolation to 1 × 1 × 3 mm(3) voxels) were investigated. Subsequently, 114 textural radiomic features were extracted from a 2.1 cm(3) sphere in the center of each insert. The influence of slice thickness, exposure and bin width on feature values was investigated. Feature stability was assessed by calculating the concordance correlation coefficient (CCC) in a test-retest setting and for different combinations of scanners, tube currents and slice thicknesses. RESULTS Bin width influenced feature values, but this only had a marginal effect on the total number of stable features (CCC {\textgreater} 0.85) when comparing different scanners, slice thicknesses or exposures. Most radiomic features were affected by slice thickness, but this effect could be reduced by resampling the CT-images before feature extraction. Statistics feature 'energy' was the most dependent on slice thickness. No clear correlation between feature values and exposures was observed. CONCLUSIONS CT-scanner, slice thickness and bin width affected radiomic feature values, whereas no effect of exposure was observed. Optimization of gray-level discretization to potentially improve prognostic value can be performed without compromising feature stability. Resampling images prior to feature extraction decreases the variability of radiomic features.},
author = {Larue, Ruben T. H. M. and van Timmeren, Janna E. and de Jong, Evelyn E. C. and Feliciani, Giacomo and Leijenaar, Ralph T. H. and Schreurs, Wendy M. J. and Sosef, Meindert N. and Raat, Frank H. P. J. and van der Zande, Frans H. R. and Das, Marco and van Elmpt, Wouter J. C. and Lambin, Philippe},
doi = {10.1080/0284186X.2017.1351624},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Larue2017 - Influence of gray level discretization on radiomic feature stability for different CT scanners tube currents and slice thicknesses a comprehensive.pdf:pdf},
issn = {1651-226X},
journal = {Acta oncologica},
month = {sep},
pages = {1--10},
pmid = {28885084},
publisher = {Informa UK Limited, trading as Taylor {\&} Francis Group},
title = {{Influence of gray level discretization on radiomic feature stability for different CT scanners, tube currents and slice thicknesses: a comprehensive phantom study.}},
url = {https://www.tandfonline.com/doi/full/10.1080/0284186X.2017.1351624 http://www.ncbi.nlm.nih.gov/pubmed/28885084},
year = {2017}
}
@article{Chassang2017,
abstract = {The use of personal data is critical to ensure quality and reliability in scientific research. The new Regulation [European Union (EU)] 2016/679 of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data [general data protection regulation (GDPR)], repealing Directive 95/46/EC, strengthens and harmonises the rules for protecting individuals' privacy rights and freedoms within and, under certain conditions, outside the EU territory. This new and historic legal milestone both prolongs and updates the EU acquis of the previous Data Protection Directive 95/46/EC. The GDPR fixes both general rules applying to any kind of personal data processing and specific rules applying to the processing of special categories of personal data such as health data taking place in the context of scientific research, this including clinical and translational research areas. This article aims to provide an overview of the new rules to consider where scientific projects include the processing of personal health data, genetic data or biometric data and other kinds of sensitive information whose use is strictly regulated by the GDPR in order to give the main key facts to researchers to adapt their practices and ensure compliance to the EU law to be enforced in May 2018.},
author = {Chassang, Gauthier},
doi = {10.3332/ecancer.2017.709},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Chassang2017 - The impact of the EU general data protection regulation on scientific research.pdf:pdf},
issn = {1754-6605},
journal = {Ecancermedicalscience},
keywords = {European Union (EU),biomedical research,computer security,humans,privacy,translational medical research},
pages = {709},
pmid = {28144283},
title = {{The impact of the EU general data protection regulation on scientific research.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28144283 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5243137},
volume = {11},
year = {2017}
}
@article{Cha2017,
abstract = {Cross-sectional X-ray imaging has become the standard for staging most solid organ malignancies. However, for some malignancies such as urinary bladder cancer, the ability to accurately assess local extent of the disease and understand response to systemic chemotherapy is limited with current imaging approaches. In this study, we explored the feasibility that radiomics-based predictive models using pre- and post-treatment computed tomography (CT) images might be able to distinguish between bladder cancers with and without complete chemotherapy responses. We assessed three unique radiomics-based predictive models, each of which employed different fundamental design principles ranging from a pattern recognition method via deep-learning convolution neural network (DL-CNN), to a more deterministic radiomics feature-based approach and then a bridging method between the two, utilizing a system which extracts radiomics features from the image patterns. Our study indicates that the computerized assessment using radiomics information from the pre- and post-treatment CT of bladder cancer patients has the potential to assist in assessment of treatment response.},
author = {Cha, Kenny H. and Hadjiiski, Lubomir and Chan, Heang-Ping and Weizer, Alon Z. and Alva, Ajjai and Cohan, Richard H. and Caoili, Elaine M. and Paramagul, Chintana and Samala, Ravi K.},
doi = {10.1038/s41598-017-09315-w},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Cha2017-Bladder cancer treatment response assessment in CT using radiomics with deep learning.pdf:pdf},
isbn = {2045-2322 (Electronic) 2045-2322 (Linking)},
issn = {2045-2322},
journal = {Scientific reports},
month = {aug},
number = {1},
pages = {8738},
pmid = {28821822},
publisher = {Springer US},
title = {{Bladder Cancer Treatment Response Assessment in CT using Radiomics with Deep-Learning.}},
url = {http://www.nature.com/articles/s41598-017-09315-w http://www.ncbi.nlm.nih.gov/pubmed/28821822 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5562694},
volume = {7},
year = {2017}
}
@article{Diamond2014,
abstract = {OBJECTIVE To investigate how consensus is operationalized in Delphi studies and to explore the role of consensus in determining the results of these studies. STUDY DESIGN AND SETTINGS Systematic review of a random sample of 100 English language Delphi studies, from two large multidisciplinary databases [ISI Web of Science (Thompson Reuters, New York, NY) and Scopus (Elsevier, Amsterdam, NL)], published between 2000 and 2009. RESULTS About 98 of the Delphi studies purported to assess consensus, although a definition for consensus was only provided in 72 of the studies (64 a priori). The most common definition for consensus was percent agreement (25 studies), with 75{\%} being the median threshold to define consensus. Although the authors concluded in 86 of the studies that consensus was achieved, consensus was only specified a priori (with a threshold value) in 42 of these studies. Achievement of consensus was related to the decision to stop the Delphi study in only 23 studies, with 70 studies terminating after a specified number of rounds. CONCLUSION Although consensus generally is felt to be of primary importance to the Delphi process, definitions of consensus vary widely and are poorly reported. Improved criteria for reporting of methods of Delphi studies are required.},
author = {Diamond, Ivan R. and Grant, Robert C. and Feldman, Brian M. and Pencharz, Paul B. and Ling, Simon C. and Moore, Aideen M. and Wales, Paul W.},
doi = {10.1016/j.jclinepi.2013.12.002},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Diamond2014 - Defining consensus - a systematic review recommends methodologic criteria for reporting of Delphi studies.pdf:pdf},
isbn = {0895-4356},
issn = {1878-5921},
journal = {Journal of clinical epidemiology},
keywords = {Consensus,Delphi study,Multi‐disciplinary,Publication quality,Reporting guidelines,Systematic review},
month = {apr},
number = {4},
pages = {401--9},
pmid = {24581294},
publisher = {Elsevier Inc},
title = {{Defining consensus: a systematic review recommends methodologic criteria for reporting of Delphi studies.}},
url = {http://dx.doi.org/10.1016/j.jclinepi.2013.12.002 http://www.ncbi.nlm.nih.gov/pubmed/24581294},
volume = {67},
year = {2014}
}
@article{Cheng2006,
abstract = {Breast cancer continues to be a sigificant public healt problem in the world. Early detection is the key for improving breast cancer prognosis. Mammography has been one of the most reliable methods for early detection of breast carcinomas. However, it is difficult for radiologists to provide both accurate and uniform evaluation for the enormous mammograms generated in widespread screening. The estimated sensitivity of radiologists in breast cancer screening is only about 75{\%}, but the performance would be improved if they were prompted with the possible locations of abnormalities. Breast cancer CAD systems can provide such help and they are important and necessary for breast cancer control. Microcalcifications and masses are the two most important indicators of malignancy, and their automated detection is very valuable for early breast cancer diagnosis. Since masses are often indistinguishable from the surrounding parenchymal, automated mass detection and classification is even more challenging. This paper discusses the methods for mass detection and classification, and compares their advantages and drawbacks. {\textcopyright} 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.},
author = {Cheng, Heng-Da and Shi, Xiangjun and Min, Rui and Hu, Li-Ming and Cai, Xiaopeng and Du, Haining},
doi = {10.1016/j.patcog.2005.07.006},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Cheng2005 - Approaches for automated detection and classification of masses in mammograms.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {CAD,Contrast enhancement,Feature selection,Fuzzy logic,Mammogram,Mass,Wavelet},
month = {apr},
number = {4},
pages = {646--668},
title = {{Approaches for automated detection and classification of masses in mammograms}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320305002955},
volume = {39},
year = {2006}
}
@inproceedings{Conseil2007,
address = {Poznan, Poland},
author = {Conseil, Simon and Bourennane, Salah and Martin, Lionel},
booktitle = {European Signal Processing Conference (EUSIPCO 2007)},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Conseil2007 - Comparison of fourier descriptors and hu moments for hand posture recognition.pdf:pdf},
keywords = {fourier,gesture recognition,pattern recognition},
pages = {1960--1964},
publisher = {IEEE},
title = {{Comparison of Fourier Descriptors and Hu Moments for Hand Posture Recognition}},
year = {2007}
}
@inproceedings{Dehghannasiri2013,
author = {Dehghannasiri, Roozbeh and Shirani, Shahram},
booktitle = {2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)},
doi = {10.1109/ICMEW.2013.6618274},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Dehghannasiri2013 - A view interpolation method without explicit disparity estimation.pdf:pdf},
isbn = {978-1-4799-1604-7},
month = {jul},
number = {1},
pages = {1--4},
publisher = {IEEE},
title = {{A view interpolation method without explicit disparity estimation}},
url = {http://ieeexplore.ieee.org/document/6618274/},
year = {2013}
}
@article{Bogowicz2017b,
abstract = {PURPOSE An association between radiomic features extracted from CT and local tumor control in the head and neck squamous cell carcinoma (HNSCC) has been shown. This study investigated the value of pretreatment functional imaging (18F-FDG PET) radiomics for modeling of local tumor control. MATERIAL AND METHODS Data from HNSCC patients (n = 121) treated with definitive radiochemotherapy were used for model training. In total, 569 radiomic features were extracted from both contrast-enhanced CT and 18F-FDG PET images in the primary tumor region. CT, PET and combined PET/CT radiomic models to assess local tumor control were trained separately. Five feature selection and three classification methods were implemented. The performance of the models was quantified using concordance index (CI) in 5-fold cross validation in the training cohort. The best models, per image modality, were compared and verified in the independent validation cohort (n = 51). The difference in CI was investigated using bootstrapping. Additionally, the observed and radiomics-based estimated probabilities of local tumor control were compared between two risk groups. RESULTS The feature selection using principal component analysis and the classification based on the multivariabale Cox regression with backward selection of the variables resulted in the best models for all image modalities (CICT = 0.72, CIPET = 0.74, CIPET/CT = 0.77). Tumors more homogenous in CT density (decreased GLSZMsize{\_}zone{\_}entropy) and with a focused region of high FDG uptake (higher GLSZMSZLGE) indicated better prognosis. No significant difference in the performance of the models in the validation cohort was observed (CICT = 0.73, CIPET = 0.71, CIPET/CT = 0.73). However, the CT radiomics-based model overestimated the probability of tumor control in the poor prognostic group (predicted = 68{\%}, observed = 56{\%}). CONCLUSIONS Both CT and PET radiomics showed equally good discriminative power for local tumor control modeling in HNSCC. However, CT-based predictions overestimated the local control rate in the poor prognostic validation cohort, and thus, we recommend to base the local control modeling on the 18F-FDG PET.},
author = {Bogowicz, Marta and Riesterer, Oliver and Stark, Luisa Sabrina and Studer, Gabriela and Unkelbach, Jan and Guckenberger, Matthias and Tanadini-Lang, Stephanie},
doi = {10.1080/0284186X.2017.1346382},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Bogowicz2017 - Comparison of PET and CT radiomics for prediction of local tumor control in head and neck squamous cell carcinoma.pdf:pdf},
isbn = {1651-226X (Electronic) 0284-186X (Linking)},
issn = {1651-226X},
journal = {Acta oncologica},
month = {aug},
pages = {1--6},
pmid = {28820287},
publisher = {Informa UK Limited, trading as Taylor {\&} Francis Group},
title = {{Comparison of PET and CT radiomics for prediction of local tumor control in head and neck squamous cell carcinoma.}},
url = {https://doi.org/10.1080/0284186X.2017.1346382 http://www.ncbi.nlm.nih.gov/pubmed/28820287},
year = {2017}
}
@article{Cysouw2017,
abstract = {PURPOSE Positron-emission tomography can be useful in oncology for diagnosis, (re)staging, determining prognosis, and response assessment. However, partial-volume effects hamper accurate quantification of lesions {\textless}2-3× the PET system's spatial resolution, and the clinical impact of this is not evident. This systematic review provides an up-to-date overview of studies investigating the impact of partial-volume correction (PVC) in oncological PET studies. METHODS We searched in PubMed and Embase databases according to the PRISMA statement, including studies from inception till May 9, 2016. Two reviewers independently screened all abstracts and eligible full-text articles and performed quality assessment according to QUADAS-2 and QUIPS criteria. For a set of similar diagnostic studies, we statistically pooled the results using bivariate meta-regression. RESULTS Thirty-one studies were eligible for inclusion. Overall, study quality was good. For diagnosis and nodal staging, PVC yielded a strong trend of increased sensitivity at expense of specificity. Meta-analysis of six studies investigating diagnosis of pulmonary nodules (679 lesions) showed no significant change in diagnostic accuracy after PVC (p = 0.222). Prognostication was not improved for non-small cell lung cancer and esophageal cancer, whereas it did improve for head and neck cancer. Response assessment was not improved by PVC for (locally advanced) breast cancer or rectal cancer, and it worsened in metastatic colorectal cancer. CONCLUSIONS The accumulated evidence to date does not support routine application of PVC in standard clinical PET practice. Consensus on the preferred PVC methodology in oncological PET should be reached. Partial-volume-corrected data should be used as adjuncts to, but not yet replacement for, uncorrected data.},
author = {Cysouw, Matthijs C F and Kramer, Gerbrand M. and Schoonmade, Linda J. and Boellaard, Ronald and de Vet, Henrica C. W. and Hoekstra, Otto S.},
doi = {10.1007/s00259-017-3775-4},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Cysouw2017 - Impact of partial-volume correction in oncological PET studies - a systematic review and meta-analysis.pdf:pdf},
isbn = {0025901737},
issn = {1619-7089},
journal = {European journal of nuclear medicine and molecular imaging},
keywords = {Oncology,Partial-volume correction,Partial-volume effect,Pet,Quantification},
month = {nov},
number = {12},
pages = {2105--2116},
pmid = {28776088},
publisher = {European Journal of Nuclear Medicine and Molecular Imaging},
title = {{Impact of partial-volume correction in oncological PET studies: a systematic review and meta-analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28776088},
volume = {44},
year = {2017}
}
@article{Coupe2008,
abstract = {A critical issue in image restoration is the problem of noise removal while keeping the integrity of relevant image information. Denoising is a crucial step to increase image quality and to improve the performance of all the tasks needed for quantitative imaging analysis. The method proposed in this paper is based on a 3-D optimized blockwise version of the nonlocal (NL)-means filter (Buades, et al., 2005). The NL-means filter uses the redundancy of information in the image under study to remove the noise. The performance of the NL-means filter has been already demonstrated for 2-D images, but reducing the computational burden is a critical aspect to extend the method to 3-D images. To overcome this problem, we propose improvements to reduce the computational complexity. These different improvements allow to drastically divide the computational time while preserving the performances of the NL-means filter. A fully automated and optimized version of the NL-means filter is then presented. Our contributions to the NL-means filter are: 1) an automatic tuning of the smoothing parameter; 2) a selection of the most relevant voxels; 3) a blockwise implementation; and 4) a parallelized computation. Quantitative validation was carried out on synthetic datasets generated with BrainWeb (Collins, et al., 1998). The results show that our optimized NL-means filter outperforms the classical implementation of the NL-means filter, as well as two other classical denoising methods [anisotropic diffusion (Perona and Malik, 1990)] and total variation minimization process (Rudin, et al., 1992) in terms of accuracy (measured by the peak signal-to-noise ratio) with low computation time. Finally, qualitative results on real data are presented .},
author = {Coupe, Pierrick and Yger, Pierre and Prima, Sylvain and Hellier, Pierre and Kervrann, Charles and Barillot, Christian},
doi = {10.1109/TMI.2007.906087},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Coup{\'{e}}2008 - An optimized blockwise non-local mean denoising filter for 3D MRI.pdf:pdf},
isbn = {1558-0062 (Electronic)$\backslash$n0278-0062 (Linking)},
issn = {1558-254X},
journal = {IEEE transactions on medical imaging},
keywords = {Image denoising,Image enhancement,Nonlocal means filter},
month = {apr},
number = {4},
pages = {425--41},
pmid = {18390341},
title = {{An optimized blockwise nonlocal means denoising filter for 3-D magnetic resonance images.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18390341 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2881565},
volume = {27},
year = {2008}
}
@article{Bogowicz2017a,
abstract = {Purpose: This study aimed to predict local tumor control (LC) after radiochemotherapy of head and neck squamous cell carcinoma (HNSCC) and human papillomavirus (HPV) status using computed tomography (CT) radiomics. Methods and Materials: HNSCC patients treated with definitive radiochemotherapy were included in the retrospective study approved by the local ethical commission (93 and 56 patients in the training and validation cohorts, respectively). Three hundred seventeen CT radiomic features, including those based on shape, intensity, texture, and wavelet transform, were calculated in the primary tumor region. Cox and logistic regression models were built to predict LC and HPV status, respectively. The best-performing features in the univariable analysis were included in the multivariable analysis after the exclusion of redundant features. The quality of the models was assessed using the concordance index (CI) for modeling of LC and receiver operating characteristics area under the curve (AUC) for HPV status prediction. The radiomics LC model was compared to a model incorporating clinical parameters (tumor stage, volume, and HPV status) and a mixed model. Results: A radiomic signature comprising 3 features was significantly associated with LC (CItraining = 0.75 and CIvalidation = 0.78), showing that tumors with a more heterogeneous CT density distribution are at risk for decreased LC. The addition of clinical parameters to the radiomics model slightly improved the model in the training cohort but not in the validation cohort. Another radiomic signature showed good performance in HPV status prediction (AUCtraining = 0.85 and AUCvalidation = 0.78) and indicated that HPV-positive tumors have a more homogenous CT density distribution. Conclusions: Heterogeneity of HNSCC tumor density, quantified by CT radiomics, is associated with LC after radiochemotherapy and HPV status.},
author = {Bogowicz, Marta and Riesterer, Oliver and Ikenberg, Kristian and Stieb, Sonja and Moch, Holger and Studer, Gabriela and Guckenberger, Matthias and Tanadini-Lang, Stephanie},
doi = {10.1016/j.ijrobp.2017.06.002},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Bogowicz2017 - CT radiomics predicts HPV status and local tumor control in head and neck squamous cell carcinoma.pdf:pdf},
isbn = {1879-355X (Electronic)
0360-3016 (Linking)},
issn = {1879355X},
journal = {International Journal of Radiation Oncology Biology Physics},
pmid = {28807534},
publisher = {Elsevier Inc.},
title = {{Computed Tomography Radiomics Predicts HPV Status and Local Tumor Control After Definitive Radiochemotherapy in Head and Neck Squamous Cell Carcinoma}},
url = {http://dx.doi.org/10.1016/j.ijrobp.2017.06.002},
year = {2017}
}
@book{Davison1997,
abstract = {Bootstrap methods are computer-intensive methods of statistical analysis, which use simulation to calculate standard errors, confidence intervals, and significance tests. The methods apply for any level of modelling, and so can be used for fully parametric, semiparametric, and completely nonparametric analysis. This 1997 book gives a broad and up-to-date coverage of bootstrap methods, with numerous applied examples, developed in a coherent way with the necessary theoretical basis. Applications include stratified data; finite populations; censored and missing data; linear, nonlinear, and smooth regression models; classification; time series and spatial problems. Special features of the book include: extensive discussion of significance tests and confidence intervals; material on various diagnostic methods; and methods for efficient computation, including improved Monte Carlo simulation. Each chapter includes both practical and theoretical exercises. S-Plus programs for implementing the methods described in the text are available from the supporting website.},
address = {Cambridge, UK},
author = {Davison, A. C. and Hinkley, D. V.},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Davison, Hinkley -Bootstrap Methods and their Application.pdf:pdf},
isbn = {0521573912},
issn = {00401706},
keywords = {application,tstrap methods and their},
number = {2},
pages = {216},
publisher = {Cambridge University Press},
title = {{Bootstrap Methods and Their Application}},
url = {http://www.jstor.org/stable/1271471?origin=crossref},
volume = {42},
year = {1997}
}
@article{Buhlmann2007,
abstract = {We present a statistical perspective on boosting. Special empha-sis is given to estimating potentially complex parametric or nonparametric models, including generalized linear and additive models as well as regres-sion models for survival analysis. Concepts of degrees of freedom and cor-responding Akaike or Bayesian information criteria, particularly useful for regularization and variable selection in high-dimensional covariate spaces, are discussed as well. The practical aspects of boosting procedures for fitting statistical mod-els are illustrated by means of the dedicated open-source software package mboost. This package implements functions which can be used for model fit-ting, prediction and variable selection. It is flexible, allowing for the imple-mentation of new boosting algorithms optimizing user-specified loss func-tions.},
archivePrefix = {arXiv},
arxivId = {0804.2777},
author = {B{\"{u}}hlmann, Peter and Hothorn, Torsten},
doi = {10.1214/07-STS242},
eprint = {0804.2777},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/B{\"{u}}hlmann2007 - Boosting algorithms.pdf:pdf},
isbn = {08834237 (ISSN)},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,generalized additive,generalized linear models,gradient boosting,models,software,survival analysis,variable selection},
number = {4},
pages = {477--505},
title = {{Boosting Algorithms: Regularization, Prediction and Model Fitting}},
url = {http://projecteuclid.org/euclid.ss/1207580163},
volume = {22},
year = {2007}
}
@article{Achanta2012,
abstract = {Computer vision applications have come to rely increasingly on superpixels in recent years, but it is not always clear what constitutes a good superpixel algorithm. In an effort to understand the benefits and drawbacks of existing methods, we empirically compare five state-of-the-art superpixel algorithms for their ability to adhere to image boundaries, speed, memory efficiency, and their impact on segmentation performance. We then introduce a new superpixel algorithm, simple linear iterative clustering (SLIC), which adapts a k-means clustering approach to efficiently generate superpixels. Despite its simplicity, SLIC adheres to boundaries as well as or better than previous methods. At the same time, it is faster and more memory efficient, improves segmentation performance, and is straightforward to extend to supervoxel generation.},
author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and S{\"{u}}sstrunk, Sabine},
doi = {10.1109/TPAMI.2012.120},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Achanta2012 - SLIC Superpixels Compared to State-of-the-Art Superpixel Methods.pdf:pdf},
isbn = {0162-8828},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Superpixels,clustering,k-means,segmentation},
month = {nov},
number = {11},
pages = {2274--82},
pmid = {22641706},
title = {{SLIC superpixels compared to state-of-the-art superpixel methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22641706},
volume = {34},
year = {2012}
}
@article{Babyak2004,
abstract = {OBJECTIVE Statistical models, such as linear or logistic regression or survival analysis, are frequently used as a means to answer scientific questions in psychosomatic research. Many who use these techniques, however, apparently fail to appreciate fully the problem of overfitting, ie, capitalizing on the idiosyncrasies of the sample at hand. Overfitted models will fail to replicate in future samples, thus creating considerable uncertainty about the scientific merit of the finding. The present article is a nontechnical discussion of the concept of overfitting and is intended to be accessible to readers with varying levels of statistical expertise. The notion of overfitting is presented in terms of asking too much from the available data. Given a certain number of observations in a data set, there is an upper limit to the complexity of the model that can be derived with any acceptable degree of uncertainty. Complexity arises as a function of the number of degrees of freedom expended (the number of predictors including complex terms such as interactions and nonlinear terms) against the same data set during any stage of the data analysis. Theoretical and empirical evidence--with a special focus on the results of computer simulation studies--is presented to demonstrate the practical consequences of overfitting with respect to scientific inference. Three common practices--automated variable selection, pretesting of candidate predictors, and dichotomization of continuous variables--are shown to pose a considerable risk for spurious findings in models. The dilemma between overfitting and exploring candidate confounders is also discussed. Alternative means of guarding against overfitting are discussed, including variable aggregation and the fixing of coefficients a priori. Techniques that account and correct for complexity, including shrinkage and penalization, also are introduced.},
author = {Babyak, Michael A.},
doi = {10.1097/01.psy.0000127692.23278.a9},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Baybak2004 - What{\_}You{\_}See{\_}May{\_}Not{\_}Be{\_}What{\_}You{\_}Get{\_}{\_}A{\_}Brief,.21.pdf:pdf},
isbn = {0033-3174},
issn = {1534-7796},
journal = {Psychosomatic medicine},
number = {3},
pages = {411--21},
pmid = {15184705},
title = {{What you see may not be what you get: a brief, nontechnical introduction to overfitting in regression-type models.}},
url = {http://www.psychosomaticmedicine.org/cgi/doi/10.1097/01.psy.0000127692.23278.a9 http://www.ncbi.nlm.nih.gov/pubmed/15184705},
volume = {66},
year = {2004}
}
@article{Friedman2010,
abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ(1) (the lasso), ℓ(2) (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Friedman2010 - Regularization Paths for Generalized Linear Models via Coordinate Descent.pdf:pdf},
journal = {Journal of statistical software},
number = {1},
pages = {1--22},
pmid = {20808728},
title = {{Regularization Paths for Generalized Linear Models via Coordinate Descent.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20808728 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2929880},
volume = {33},
year = {2010}
}
@techreport{RCoreTeam2017,
address = {Vienna, Austria},
author = {{R Core Team}},
keywords = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org/},
year = {2017}
}
@article{Houwelingen2013,
abstract = {In deriving a regression model analysts often have to use variable$\backslash$r$\backslash$nselection, despite of problems introduced by data- dependent model$\backslash$r$\backslash$nbuilding. Resampling approaches are proposed to handle some of the critical$\backslash$r$\backslash$nissues. In order to assess and compare several strategies, we will conduct a$\backslash$r$\backslash$nsimulation study with 15 predictors and a complex correlation structure in$\backslash$r$\backslash$nthe linear regression model. Using sample sizes of 100 and 400 and estimates of$\backslash$r$\backslash$nthe residual variance corresponding to R2 of 0.50 and 0.71, we consider 4 scenarios with varying amount of information.$\backslash$r$\backslash$nWe also consider two examples with 24 and 13 predictors, respectively. We will$\backslash$r$\backslash$ndiscuss the value of cross-validation, shrinkage and backward$\backslash$r$\backslash$nelimination (BE) with varying significance level. We will assess whether 2-step$\backslash$r$\backslash$napproaches using global or parameterwise shrinkage (PWSF) can improve selected models and will compare results to$\backslash$r$\backslash$nmodels derived with the LASSO procedure. Beside of MSE we will use model$\backslash$r$\backslash$nsparsity and further criteria for model assessment. The amount of information$\backslash$r$\backslash$nin the data has an influence on the selected models and the comparison of the$\backslash$r$\backslash$nprocedures. None of the approaches was best in all scenarios. The$\backslash$r$\backslash$nperformance of backward elimination with a suitably chosen significance level$\backslash$r$\backslash$nwas not worse compared to the LASSO and BE models selected were much sparser,$\backslash$r$\backslash$nan important advantage for interpretation and transportability. Compared to$\backslash$r$\backslash$nglobal shrinkage, PWSF had better performance. Provided that the amount of$\backslash$r$\backslash$ninformation is not too small, we conclude that BE followed by PWSF is a suitable$\backslash$r$\backslash$napproach when variable selection is a key part of data analysis.},
author = {Houwelingen, Hans C. Van and Sauerbrei, Willi},
doi = {10.4236/ojs.2013.32011},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/vanHouwelingen2014 - Cross-validation shrinkage and variable selection in linear regression revisited.pdf:pdf},
issn = {2161-718X},
journal = {Open Journal of Statistics},
keywords = {cross-validation,lasso,shrinkage,simulation study,variable selection},
number = {2},
pages = {79--102},
title = {{Cross-Validation, Shrinkage and Variable Selection in Linear Regression Revisited}},
url = {http://www.scirp.org/journal/PaperInformation.aspx?PaperID=30157{\&}{\#}abstract},
volume = {3},
year = {2013}
}
@article{Tibshirani2011,
abstract = {We propose a new method for estimation in linear models. The "lasso" minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly zero and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described. Keywords: regression, subset selection, shrinkage, quadratic programming.},
archivePrefix = {arXiv},
arxivId = {1369–7412/11/73273},
author = {Tibshirani, Robert},
doi = {10.1111/j.1467-9868.2011.00771.x},
eprint = {11/73273},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Tibshirani2011 - Regression shrinkage and selection via the lasso - a retrospective.pdf:pdf},
isbn = {0035-9246},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Penalization,Regularization,l1-penalty},
number = {3},
pages = {273--282},
pmid = {16272381},
primaryClass = {1369–7412},
title = {{Regression shrinkage and selection via the lasso: A retrospective}},
volume = {73},
year = {2011}
}
@article{Mroz2015,
abstract = {BACKGROUND Although the involvement of intra-tumor genetic heterogeneity in tumor progression, treatment resistance, and metastasis is established, genetic heterogeneity is seldom examined in clinical trials or practice. Many studies of heterogeneity have had prespecified markers for tumor subpopulations, limiting their generalizability, or have involved massive efforts such as separate analysis of hundreds of individual cells, limiting their clinical use. We recently developed a general measure of intra-tumor genetic heterogeneity based on whole-exome sequencing (WES) of bulk tumor DNA, called mutant-allele tumor heterogeneity (MATH). Here, we examine data collected as part of a large, multi-institutional study to validate this measure and determine whether intra-tumor heterogeneity is itself related to mortality. METHODS AND FINDINGS Clinical and WES data were obtained from The Cancer Genome Atlas in October 2013 for 305 patients with head and neck squamous cell carcinoma (HNSCC), from 14 institutions. Initial pathologic diagnoses were between 1992 and 2011 (median, 2008). Median time to death for 131 deceased patients was 14 mo; median follow-up of living patients was 22 mo. Tumor MATH values were calculated from WES results. Despite the multiple head and neck tumor subsites and the variety of treatments, we found in this retrospective analysis a substantial relation of high MATH values to decreased overall survival (Cox proportional hazards analysis: hazard ratio for high/low heterogeneity, 2.2; 95{\%} CI 1.4 to 3.3). This relation of intra-tumor heterogeneity to survival was not due to intra-tumor heterogeneity's associations with other clinical or molecular characteristics, including age, human papillomavirus status, tumor grade and TP53 mutation, and N classification. MATH improved prognostication over that provided by traditional clinical and molecular characteristics, maintained a significant relation to survival in multivariate analyses, and distinguished outcomes among patients having oral-cavity or laryngeal cancers even when standard disease staging was taken into account. Prospective studies, however, will be required before MATH can be used prognostically in clinical trials or practice. Such studies will need to examine homogeneously treated HNSCC at specific head and neck subsites, and determine the influence of cancer therapy on MATH values. Analysis of MATH and outcome in human-papillomavirus-positive oropharyngeal squamous cell carcinoma is particularly needed. CONCLUSIONS To our knowledge this study is the first to combine data from hundreds of patients, treated at multiple institutions, to document a relation between intra-tumor heterogeneity and overall survival in any type of cancer. We suggest applying the simply calculated MATH metric of heterogeneity to prospective studies of HNSCC and other tumor types.},
author = {Mroz, Edmund A. and Tward, Aaron D. and Tward, Aaron M. and Hammon, Rebecca J. and Ren, Yin and Rocco, James W.},
doi = {10.1371/journal.pmed.1001786},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Mroz2015 - Intra-tumor Genetic Heterogeneity and Mortality in Head and Neck Cancer Analysis of Data from The Cancer Genome Atlas.PDF:PDF},
issn = {1549-1676},
journal = {PLoS medicine},
month = {feb},
number = {2},
pages = {e1001786},
pmid = {25668320},
title = {{Intra-tumor genetic heterogeneity and mortality in head and neck cancer: analysis of data from the Cancer Genome Atlas.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4323109{\&}tool=pmcentrez{\&}rendertype=abstract http://www.ncbi.nlm.nih.gov/pubmed/25668320 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4323109},
volume = {12},
year = {2015}
}
@article{Cerami2012,
abstract = {The cBio Cancer Genomics Portal (http://cbioportal.org) is an open-access resource for interactive exploration of multidimensional cancer genomics data sets, currently providing access to data from more than 5,000 tumor samples from 20 cancer studies. The cBio Cancer Genomics Portal significantly lowers the barriers between complex genomic data and cancer researchers who want rapid, intuitive, and high-quality access to molecular profiles and clinical attributes from large-scale cancer genomics projects and empowers researchers to translate these rich data sets into biologic insights and clinical applications.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Cerami, Ethan and Gao, Jianjiong and Dogrusoz, Ugur and Gross, Benjamin E. and Sumer, Selcuk Onur and Aksoy, B{\"{u}}lent Arman and Jacobsen, Anders and Byrne, Caitlin J. and Heuer, Michael L. and Larsson, Erik and Antipin, Yevgeniy and Reva, Boris and Goldberg, Arthur P. and Sander, Chris and Schultz, Nikolaus},
doi = {10.1158/2159-8290.CD-12-0095},
eprint = {NIHMS150003},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Cerami2012 - The cBio Cancer Genomics Portal.pdf:pdf},
isbn = {10.1158/2159-8290.CD-12-0095},
issn = {2159-8290},
journal = {Cancer discovery},
month = {may},
number = {5},
pages = {401--4},
pmid = {22588877},
title = {{The cBio cancer genomics portal: an open platform for exploring multidimensional cancer genomics data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22588877 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3956037},
volume = {2},
year = {2012}
}
@article{Gao2013,
abstract = {The cBioPortal for Cancer Genomics (http://cbioportal.org) provides a Web resource for exploring, visualizing, and analyzing multidimensional cancer genomics data. The portal reduces molecular profiling data from cancer tissues and cell lines into readily understandable genetic, epigenetic, gene expression, and proteomic events. The query interface combined with customized data storage enables researchers to interactively explore genetic alterations across samples, genes, and pathways and, when available in the underlying data, to link these to clinical outcomes. The portal provides graphical summaries of gene-level data from multiple platforms, network visualization and analysis, survival analysis, patient-centric queries, and software programmatic access. The intuitive Web interface of the portal makes complex cancer genomics profiles accessible to researchers and clinicians without requiring bioinformatics expertise, thus facilitating biological discoveries. Here, we provide a practical guide to the analysis and visualization features of the cBioPortal for Cancer Genomics.},
author = {Gao, Jianjiong and Aksoy, B{\"{u}}lent Arman and Dogrusoz, Ugur and Dresdner, Gideon and Gross, Benjamin and Sumer, S Onur and Sun, Yichao and Jacobsen, Anders and Sinha, Rileen and Larsson, Erik and Cerami, Ethan and Sander, Chris and Schultz, Nikolaus},
doi = {10.1126/scisignal.2004088},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Gao2013 - Integrative analysis of complex cancer genomic and clinical profiles using the cBioPortal.pdf:pdf},
issn = {1937-9145},
journal = {Science signaling},
month = {apr},
number = {269},
pages = {pl1},
pmid = {23550210},
title = {{Integrative analysis of complex cancer genomics and clinical profiles using the cBioPortal.}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4160307/ http://www.ncbi.nlm.nih.gov/pubmed/4160307 http://www.ncbi.nlm.nih.gov/pubmed/23550210 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4160307},
volume = {6},
year = {2013}
}
@article{CancerGenomeAtlasNetwork2015,
abstract = {The Cancer Genome Atlas profiled 279 head and neck squamous cell carcinomas (HNSCCs) to provide a comprehensive landscape of somatic genomic alterations. Here we show that human-papillomavirus-associated tumours are dominated by helical domain mutations of the oncogene PIK3CA, novel alterations involving loss of TRAF3, and amplification of the cell cycle gene E2F1. Smoking-related HNSCCs demonstrate near universal loss-of-function TP53 mutations and CDKN2A inactivation with frequent copy number alterations including amplification of 3q26/28 and 11q13/22. A subgroup of oral cavity tumours with favourable clinical outcomes displayed infrequent copy number alterations in conjunction with activating mutations of HRAS or PIK3CA, coupled with inactivating mutations of CASP8, NOTCH1 and TP53. Other distinct subgroups contained loss-of-function alterations of the chromatin modifier NSD1, WNT pathway genes AJUBA and FAT1, and activation of oxidative stress factor NFE2L2, mainly in laryngeal tumours. Therapeutic candidate alterations were identified in most HNSCCs.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {{Cancer Genome Atlas Network}},
doi = {10.1038/nature14129},
eprint = {NIHMS150003},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/TCGA2015 - Comprehensive genomic characterization of head and neck squamous cell carcinomas.pdf:pdf},
isbn = {6314442508},
issn = {1476-4687},
journal = {Nature},
month = {jan},
number = {7536},
pages = {576--82},
pmid = {25631445},
title = {{Comprehensive genomic characterization of head and neck squamous cell carcinomas.}},
url = {http://www.nature.com/doifinder/10.1038/nature14129 http://www.ncbi.nlm.nih.gov/pubmed/25631445 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4311405},
volume = {517},
year = {2015}
}
@article{Shah2014,
abstract = {Multivariate imputation by chained equations (MICE) is commonly used for imputing missing data in epidemiologic research. The "true" imputation model may contain nonlinearities which are not included in default imputation models. Random forest imputation is a machine learning technique which can accommodate nonlinearities and interactions and does not require a particular regression model to be specified. We compared parametric MICE with a random forest-based MICE algorithm in 2 simulation studies. The first study used 1,000 random samples of 2,000 persons drawn from the 10,128 stable angina patients in the CALIBER database (Cardiovascular Disease Research using Linked Bespoke Studies and Electronic Records; 2001-2010) with complete data on all covariates. Variables were artificially made "missing at random," and the bias and efficiency of parameter estimates obtained using different imputation methods were compared. Both MICE methods produced unbiased estimates of (log) hazard ratios, but random forest was more efficient and produced narrower confidence intervals. The second study used simulated data in which the partially observed variable depended on the fully observed variables in a nonlinear way. Parameter estimates were less biased using random forest MICE, and confidence interval coverage was better. This suggests that random forest imputation may be useful for imputing complex epidemiologic data sets in which some patients have missing data.},
author = {Shah, Anoop D. and Bartlett, Jonathan W. and Carpenter, James and Nicholas, Owen and Hemingway, Harry},
doi = {10.1093/aje/kwt312},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Shah2014 - Comparison of random forest and parametric imputation models for imputing missing data using mice.pdf:pdf},
isbn = {1476-6256 (Electronic)$\backslash$r0002-9262 (Linking)},
issn = {1476-6256},
journal = {American journal of epidemiology},
keywords = {angina,imputation,missing data,missingness at random,regression trees,simulation,stable,survival},
month = {mar},
number = {6},
pages = {764--74},
pmid = {24589914},
title = {{Comparison of random forest and parametric imputation models for imputing missing data using MICE: a CALIBER study.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24589914 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3939843},
volume = {179},
year = {2014}
}
@article{Tibshirani1996,
abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
archivePrefix = {arXiv},
arxivId = {1369–7412/11/73273},
author = {Tibshirani, Robert},
doi = {10.2307/2346178},
eprint = {11/73273},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Tibshirani1996 - Regression shrinkage and selection via the lasso.pdf:pdf},
isbn = {0849320240},
issn = {00359246},
journal = {Journal of the Royal Statistical Society B},
number = {1},
pages = {267--288},
primaryClass = {1369–7412},
title = {{Regression selection and shrinkage via the lasso}},
volume = {58},
year = {1996}
}
@article{Leeman2017,
abstract = {Use of proton beam therapy has expanded, with the number of proton centres rapidly increasing not only in the USA but also worldwide. The physical characteristics of the proton beam offer important advantages versus widely used photon techniques in terms of radiation precision. In head and neck cancer in particular, proton beam therapy is uniquely suited for the complex anatomy of tumours and sensitive surrounding organs. De-intensification and personalisation of treatment to limit toxicity are of renewed importance in the context of human papilloma virus-associated disease, in which young patients will be cured but bear the consequences of adverse effects for decades. Comparisons of radiation dose distributions between photon and proton techniques suggest considerable benefit in terms of toxicity sparing, but this has only recently been confirmed by substantial clinical data. In this Review, we attempt to define the role of this method in the contemporary multidisciplinary management of various types of head and neck cancer.},
author = {Leeman, Jonathan E. and Romesser, Paul B. and Zhou, Ying and McBride, Sean and Riaz, Nadeem and Sherman, Eric and Cohen, Marc A. and Cahlon, Oren and Lee, Nancy},
doi = {10.1016/S1470-2045(17)30179-1},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Leeman2017 - Proton therapy for head and neck cancer - expanding the therapeutic window.pdf:pdf},
issn = {1474-5488},
journal = {The Lancet. Oncology},
month = {may},
number = {5},
pages = {e254--e265},
pmid = {28456587},
publisher = {Elsevier Ltd},
title = {{Proton therapy for head and neck cancer: expanding the therapeutic window.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1470204517301791 http://www.ncbi.nlm.nih.gov/pubmed/28456587},
volume = {18},
year = {2017}
}
@article{Peto1972,
author = {Peto, Richard and Peto, Julian},
doi = {10.2307/2344317},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Peto1972 - Asymptotically efficient rank invariant test procedures.pdf:pdf},
issn = {00359238},
journal = {Journal of the Royal Statistical Society. Series A (General)},
keywords = {experimental,life table},
number = {2},
pages = {185},
title = {{Asymptotically Efficient Rank Invariant Test Procedures}},
url = {http://www.jstor.org/stable/10.2307/2344317?origin=crossref},
volume = {135},
year = {1972}
}
@article{Koo2016,
abstract = {OBJECTIVE Intraclass correlation coefficient (ICC) is a widely used reliability index in test-retest, intrarater, and interrater reliability analyses. This article introduces the basic concept of ICC in the content of reliability analysis. DISCUSSION FOR RESEARCHERS There are 10 forms of ICCs. Because each form involves distinct assumptions in their calculation and will lead to different interpretations, researchers should explicitly specify the ICC form they used in their calculation. A thorough review of the research design is needed in selecting the appropriate form of ICC to evaluate reliability. The best practice of reporting ICC should include software information, "model," "type," and "definition" selections. DISCUSSION FOR READERS When coming across an article that includes ICC, readers should first check whether information about the ICC form has been reported and if an appropriate ICC form was used. Based on the 95{\%} confident interval of the ICC estimate, values less than 0.5, between 0.5 and 0.75, between 0.75 and 0.9, and greater than 0.90 are indicative of poor, moderate, good, and excellent reliability, respectively. CONCLUSION This article provides a practical guideline for clinical researchers to choose the correct form of ICC and suggests the best practice of reporting ICC parameters in scientific publications. This article also gives readers an appreciation for what to look for when coming across ICC while reading an article.},
author = {Koo, Terry K. and Li, Mae Y.},
doi = {10.1016/j.jcm.2016.02.012},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Koo2016 - A guideline of selecting and reporting intraclass correlation coefficients for reliability research.pdf:pdf},
isbn = {1556-3707},
issn = {1556-3707},
journal = {Journal of chiropractic medicine},
keywords = {Reliability and validity,Research,Statistics},
month = {jun},
number = {2},
pages = {155--63},
pmid = {27330520},
publisher = {Elsevier B.V.},
title = {{A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research.}},
url = {http://dx.doi.org/10.1016/j.jcm.2016.02.012 http://linkinghub.elsevier.com/retrieve/pii/S1556370716000158 http://www.ncbi.nlm.nih.gov/pubmed/27330520 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4913118},
volume = {15},
year = {2016}
}
@article{Bogowicz2016,
abstract = {This study aimed to identify a set of stable radiomic parameters in CT perfusion (CTP) maps with respect to CTP calculation factors and image discretization, as an input for future prognostic models for local tumor response to chemo-radiotherapy. Pre-treatment CTP images of eleven patients with oropharyngeal carcinoma and eleven patients with non-small cell lung cancer (NSCLC) were analyzed. 315 radiomic parameters were studied per perfusion map (blood volume, blood flow and mean transit time). Radiomics robustness was investigated regarding the potentially standardizable (image discretization method, Hounsfield unit (HU) threshold, voxel size and temporal resolution) and non-standardizable (artery contouring and noise threshold) perfusion calculation factors using the intraclass correlation (ICC). To gain added value for our model radiomic parameters correlated with tumor volume, a well-known predictive factor for local tumor response to chemo-radiotherapy, were excluded from the analysis. The remaining stable radiomic parameters were grouped according to inter-parameter Spearman correlations and for each group the parameter with the highest ICC was included in the final set. The acceptance level was 0.9 and 0.7 for the ICC and correlation, respectively. The image discretization method using fixed number of bins or fixed intervals gave a similar number of stable radiomic parameters (around 40{\%}). The potentially standardizable factors introduced more variability into radiomic parameters than the non-standardizable ones with 56-98{\%} and 43-58{\%} instability rates, respectively. The highest variability was observed for voxel size (instability rate {\textgreater}97{\%} for both patient cohorts). Without standardization of CTP calculation factors none of the studied radiomic parameters were stable. After standardization with respect to non-standardizable factors ten radiomic parameters were stable for both patient cohorts after correction for inter-parameter correlations. Voxel size, image discretization, HU threshold and temporal resolution have to be standardized to build a reliable predictive model based on CTP radiomics analysis.},
author = {Bogowicz, Marta and Riesterer, O. and Bundschuh, R. A. and Veit-Haibach, P. and H{\"{u}}llner, M. and Studer, G. and Stieb, S. and Glatz, S. and Pruschy, M. and Guckenberger, Matthias and Tanadini-Lang, Stephanie},
doi = {10.1088/1361-6560/61/24/8736},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Bogowicz2016 - Stability of radiomic features in CT perfusion maps.pdf:pdf},
isbn = {1361-6560 (Electronic) 0031-9155 (Linking)},
issn = {1361-6560},
journal = {Physics in medicine and biology},
month = {dec},
number = {24},
pages = {8736--8749},
pmid = {27893446},
title = {{Stability of radiomic features in CT perfusion maps.}},
url = {http://stacks.iop.org/0031-9155/61/i=24/a=8736?key=crossref.209310b028dbf6bd86061a07ef6efc33 http://www.ncbi.nlm.nih.gov/pubmed/27893446},
volume = {61},
year = {2016}
}
@article{Bay2008,
abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1016/j.cviu.2007.09.014},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Bay2008 - Speeded-up robust features (SURF).pdf:pdf},
isbn = {9783540338321},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Camera calibration,Feature description,Interest points,Local features,Object recognition},
month = {jun},
number = {3},
pages = {346--359},
title = {{Speeded-Up Robust Features (SURF)}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314207001555},
volume = {110},
year = {2008}
}
@article{Peng2005,
abstract = {Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.},
author = {Peng, Hanchuan and Long, Fuhui and Ding, Chris},
doi = {10.1109/TPAMI.2005.159},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Peng2005 - Feature selection based on mutual information.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Classification,Feature selection,Maximal dependency,Maximal relevance,Minimal redundancy,Mutual information},
month = {aug},
number = {8},
pages = {1226--1238},
pmid = {16119262},
title = {{Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy}},
url = {http://ieeexplore.ieee.org/document/1453511/},
volume = {27},
year = {2005}
}
@article{Shrout1979,
abstract = {Reliability coefficients often take the form of intraclass correlation coefficients. In this article, guidelines are given for choosing among 6 different forms of the intraclass correlation for reliability studies in which n targets are rated by k judges. Relevant to the choice of the coefficient are the appropriate statistical model for the reliability study and the applications to be made of the reliability results. Confidence intervals for each of the forms are reviewed. (23 ref)},
author = {Shrout, Patrick E. and Fleiss, Joseph L.},
doi = {10.1037/0033-2909.86.2.420},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Shrout1979 - Intraclass correlations - uses in assessing rater reliability.pdf:pdf},
isbn = {0033-2909 (Print)$\backslash$r0033-2909 (Linking)},
issn = {1939-1455},
journal = {Psychological Bulletin},
number = {2},
pages = {420--428},
pmid = {18839484},
title = {{Intraclass correlations: Uses in assessing rater reliability.}},
url = {http://content.apa.org/journals/bul/86/2/420 http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.86.2.420},
volume = {86},
year = {1979}
}
@article{Chang2000,
abstract = {The first part of this paper proposes an adaptive, data-driven threshold for image denoising via wavelet soft-thresholding. The threshold is derived in a Bayesian framework, and the prior used on the wavelet coefficients is the generalized Gaussian distribution (GGD) widely used in image processing applications. The proposed threshold is simple and closed-form, and it is adaptive to each subband because it depends on data-driven estimates of the parameters. Experimental results show that the proposed method, called BayesShrink, is typically within 5{\%} of the MSE of the best soft-thresholding benchmark with the image assumed known. It also outperforms SureShrink (Donoho and Johnstone 1994, 1995; Donoho 1995) most of the time. The second part of the paper attempts to further validate claims that lossy compression can be used for denoising. The BayesShrink threshold can aid in the parameter selection of a coder designed with the intention of denoising, and thus achieving simultaneous denoising and compression. Specifically, the zero-zone in the quantization step of compression is analogous to the threshold value in the thresholding function. The remaining coder design parameters are chosen based on a criterion derived from Rissanen's minimum description length (MDL) principle. Experiments show that this compression method does indeed remove noise significantly, especially for large noise power. However, it introduces quantization noise and should be used only if bitrate were an additional concern to denoising.},
author = {Chang, S. Grace and Yu, Bin and Vetterli, Martin},
doi = {10.1109/83.862633},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Chang2000 - Adapative wavelet thresholding for image denoising and compression.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
number = {9},
pages = {1532--1546},
title = {{Adaptive wavelet thresholding for image denoising and compression}},
url = {http://ieeexplore.ieee.org/document/862633/},
volume = {9},
year = {2000}
}
@article{Shiri2017,
author = {Shiri, Isaac and Rahmim, Arman and Ghaffarian, Pardis and Geramifar, Parham and Abdollahi, Hamid and Bitarafan-Rajabi, Ahmad},
doi = {10.1007/s00330-017-4859-z},
issn = {0938-7994},
journal = {European Radiology},
keywords = {PET/CT,Quantification,Radiomics,Reconstruction settings,Robustness,ct,pet,quantification,radiomics,reconstruction settings,robustness},
month = {nov},
number = {11},
pages = {4498--4509},
publisher = {European Radiology},
title = {{The impact of image reconstruction settings on 18F-FDG PET radiomic features: multi-scanner phantom and patient studies}},
url = {http://link.springer.com/10.1007/s00330-017-4859-z},
volume = {27},
year = {2017}
}

@article{Caudell2017,
abstract = {Radiotherapy has long been the mainstay of treatment for patients with head and neck cancer and has traditionally involved a stage-dependent strategy whereby all patients with the same TNM stage receive the same therapy. We believe there is a substantial opportunity to improve radiotherapy delivery beyond just technological and anatomical precision. In this Series paper, we explore several new ideas that could improve understanding of the phenotypic and genotypic differences that exist between patients and their tumours. We discuss how exploiting these differences and taking advantage of precision medicine tools-such as genomics, radiomics, and mathematical modelling-could open new doors to personalised radiotherapy adaptation and treatment. We propose a new treatment shift that moves away from an era of empirical dosing and fractionation to an era focused on the development of evidence to guide personalisation and biological adaptation of radiotherapy. We believe these approaches offer the potential to improve outcomes and reduce toxicity.},
author = {Caudell, Jimmy J. and Torres-Roca, Javier F. and Gillies, Robert J. and Enderling, Heiko and Kim, Sungjune and Rishi, Anupam and Moros, Eduardo G. and Harrison, Louis B.},
doi = {10.1016/S1470-2045(17)30252-8},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Caudell2017 - The future of personalised radiotherapy for head and neck cancer.pdf:pdf},
issn = {1474-5488},
journal = {The Lancet. Oncology},
month = {may},
number = {5},
pages = {e266--e273},
pmid = {28456586},
publisher = {Elsevier Ltd},
title = {{The future of personalised radiotherapy for head and neck cancer.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1470204517302528 http://www.ncbi.nlm.nih.gov/pubmed/28456586},
volume = {18},
year = {2017}
}
@article{Moons2015,
abstract = {Context Prediction models are developed to aid health care providers in estimating the probability or risk that a specific disease or condition is present (diagnostic models) or that a specific event will occur in the future (prognostic models), to inform their decision making. However, the overwhelming evidence shows that the quality of reporting of prediction model studies is poor. Only with full and clear reporting of information on all aspects of a prediction model can risk of bias and potential usefulness of prediction models be adequately assessed. Objective The Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) Initiative developed a set of recommendations for the reporting of studies developing, validating, or updating a prediction model, whether for diagnostic or prognostic purposes. Evidence acquisition This article describes how the TRIPOD Statement was developed. An extensive list of items based on a review of the literature was created, which was reduced after a Web-based survey and revised during a 3-day meeting in June 2011 with methodologists, health care professionals, and journal editors. The list was refined during several meetings of the steering group and in e-mail discussions with the wider group of TRIPOD contributors. Evidence synthesis The resulting TRIPOD Statement is a checklist of 22 items, deemed essential for transparent reporting of a prediction model study. The TRIPOD Statement aims to improve the transparency of the reporting of a prediction model study regardless of the study methods used. The TRIPOD Statement is best used in conjunction with the TRIPOD explanation and elaboration document. Conclusions To aid the editorial process and readers of prediction model studies, it is recommended that authors include a completed checklist in their submission (also available at www.tripod-statement.org). Patient summary The Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) Initiative developed a set of recommendations for the reporting of studies developing, validating, or updating a prediction model, whether for diagnostic or prognostic purposes.},
author = {Moons, Karel G. M. and Altman, Douglas G. and Reitsma, Johannes B. and Ioannidis, John P. A. and Macaskill, Petra and Steyerberg, Ewout W. and Vickers, Andrew J. and Ransohoff, David F. and Collins, Gary S.},
doi = {10.7326/M14-0698},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Guidelines/Moons2015 - Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD) - explanation and elaboration.pdf:pdf},
isbn = {1539-3704 (Electronic)$\backslash$r0003-4819 (Linking)},
issn = {0003-4819},
journal = {Annals of Internal Medicine},
keywords = {Diagnostic,Model development,Model validation,Prediction models,Prognostic,Transparent reporting},
month = {jan},
number = {1},
pages = {W1},
pmid = {25561516},
title = {{Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD): Explanation and Elaboration}},
url = {http://annals.org/article.aspx?doi=10.7326/M14-0698},
volume = {162},
year = {2015}
}
@article{Cui2017,
abstract = {OBJECTIVE: To develop and validate a volume-based, quantitative imaging marker by integrating multi-parametric MR images for predicting glioblastoma survival, and to investigate its relationship and synergy with molecular characteristics. METHODS: We retrospectively analysed 108 patients with primary glioblastoma. The discovery cohort consisted of 62 patients from the cancer genome atlas (TCGA). Another 46 patients comprising 30 from TCGA and 16 internally were used for independent validation. Based on integrated analyses of T1-weighted contrast-enhanced (T1-c) and diffusion-weighted MR images, we identified an intratumoral subregion with both high T1-c and low ADC, and accordingly defined a high-risk volume (HRV). We evaluated its prognostic value and biological significance with genomic data. RESULTS: On both discovery and validation cohorts, HRV predicted overall survival (OS) (concordance index: 0.642 and 0.653, P {\textless} 0.001 and P = 0.038, respectively). HRV stratified patients within the proneural molecular subtype (log-rank P = 0.040, hazard ratio = 2.787). We observed different OS among patients depending on their MGMT methylation status and HRV (log-rank P = 0.011). Patients with unmethylated MGMT and high HRV had significantly shorter survival (median survival: 9.3 vs. 18.4 months, log-rank P = 0.002). CONCLUSION: Volume of the high-risk intratumoral subregion identified on multi-parametric MRI predicts glioblastoma survival, and may provide complementary value to genomic information.},
author = {Cui, Yi and Ren, Shangjie and Tha, Khin Khin and Wu, Jia and Shirato, Hiroki and Li, Ruijiang},
doi = {10.1007/s00330-017-4751-x},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Cui2017 - Volume of high-risk intratumoral subregions at multi-parametric MR imaging.pdf:pdf},
issn = {0938-7994},
journal = {European Radiology},
keywords = {Glioblastoma multiforme,High-risk tumour volume,Multi-parametric MRI,Overall survival,Radiogenomics},
month = {feb},
pages = {1--10},
pmid = {28168370},
publisher = {European Radiology},
title = {{Volume of high-risk intratumoral subregions at multi-parametric MR imaging predicts overall survival and complements molecular analysis of glioblastoma}},
url = {http://link.springer.com/10.1007/s00330-017-4751-x},
year = {2017}
}
@article{Collins2015,
abstract = {Context Prediction models are developed to aid health care providers in estimating the probability or risk that a specific disease or condition is present (diagnostic models) or that a specific event will occur in the future (prognostic models), to inform their decision making. However, the overwhelming evidence shows that the quality of reporting of prediction model studies is poor. Only with full and clear reporting of information on all aspects of a prediction model can risk of bias and potential usefulness of prediction models be adequately assessed. Objective The Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) Initiative developed a set of recommendations for the reporting of studies developing, validating, or updating a prediction model, whether for diagnostic or prognostic purposes. Evidence acquisition This article describes how the TRIPOD Statement was developed. An extensive list of items based on a review of the literature was created, which was reduced after a Web-based survey and revised during a 3-day meeting in June 2011 with methodologists, health care professionals, and journal editors. The list was refined during several meetings of the steering group and in e-mail discussions with the wider group of TRIPOD contributors. Evidence synthesis The resulting TRIPOD Statement is a checklist of 22 items, deemed essential for transparent reporting of a prediction model study. The TRIPOD Statement aims to improve the transparency of the reporting of a prediction model study regardless of the study methods used. The TRIPOD Statement is best used in conjunction with the TRIPOD explanation and elaboration document. Conclusions To aid the editorial process and readers of prediction model studies, it is recommended that authors include a completed checklist in their submission (also available at www.tripod-statement.org). Patient summary The Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) Initiative developed a set of recommendations for the reporting of studies developing, validating, or updating a prediction model, whether for diagnostic or prognostic purposes.},
author = {Collins, Gary S. and Reitsma, Johannes B. and Altman, Douglas G. and Moons, Karel G. M.},
doi = {10.7326/M14-0697},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Collins2015 - Transparant reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD) - the TRIPOD statement.pdf:pdf},
isbn = {1539-3704 (Electronic)$\backslash$r0003-4819 (Linking)},
issn = {0003-4819},
journal = {Annals of Internal Medicine},
keywords = {Diagnostic,Model development,Model validation,Prediction models,Prognostic,Transparent reporting},
month = {jan},
number = {1},
pages = {55},
pmid = {25561516},
title = {{Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD): The TRIPOD Statement}},
url = {http://annals.org/article.aspx?doi=10.7326/M14-0697},
volume = {162},
year = {2015}
}
@article{Ikeda2010,
abstract = {Rank et al. have proposed an algorithm for estimating image noise variance composed of the following three steps: the noisy image is first filtered by a difference operator; a histogram of local signal variances is then computed; and, finally the noise variance is estimated from a statistical evaluation of the histogram. We have verified the accuracy of this algorithm on a CT image by indirect methods, and have shown that this method is able to estimate CT image noise variance with reasonable accuracy, regardless of whether or not the noiseless image is uniform. Further, we have proposed a simple alternative method for the last two steps of the Rank et al. method. However, one must pay attention to the fact that the estimated noise variance will be biased when the nearest two pixels are correlated and that this algorithm does not work well if the assumption of stationarity of noise components is violated. ?? 2010 Elsevier Ltd.},
author = {Ikeda, Mitsuru and Makino, Reiko and Imai, Kuniharu and Matsumoto, Maiko and Hitomi, Rika},
doi = {10.1016/j.compmedimag.2010.07.005},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Ikeda2010 - A method for estimating noise variance of CT image.pdf:pdf},
issn = {08956111},
journal = {Computerized Medical Imaging and Graphics},
keywords = {CT images,Image noise variance,Image quality evaluation,Medical image analysis,Radiation dose},
month = {dec},
number = {8},
pages = {642--650},
pmid = {20797837},
publisher = {Elsevier Ltd},
title = {{A method for estimating noise variance of CT image}},
url = {http://dx.doi.org/10.1016/j.compmedimag.2010.07.005 http://linkinghub.elsevier.com/retrieve/pii/S0895611110000765},
volume = {34},
year = {2010}
}
@article{Gourtsoyianni2017,
abstract = {Purpose To assess the day-to-day repeatability of global and local-regional magnetic resonance (MR) imaging texture features derived from primary rectal cancer. Materials and Methods After ethical approval and patient informed consent were obtained, two pretreatment T2-weighted axial MR imaging studies performed prospectively with the same imaging unit on 2 consecutive days in 14 patients with rectal cancer (11 men [mean age, 61.7 years], three women [mean age, 70.0 years]) were analyzed to extract (a) global first-order statistical histogram and model-based fractal features reflecting the whole-tumor voxel intensity histogram distribution and repeating patterns, respectively, without spatial information and (b) local-regional second-order and high-order statistical texture features reflecting the intensity and spatial interrelationships between adjacent in-plane or multiplanar voxels or regions, respectively. Repeatability was assessed for 46 texture features, and mean difference, 95{\%} limits of agreement, within-subject coefficient of variation (wCV), and repeatability coefficient (r) were recorded. Results Repeatability was better for global parameters than for most local-regional parameters. In particular, histogram mean, median, and entropy, fractal dimension mean and standard deviation, and second-order entropy, homogeneity, difference entropy, and inverse difference moment demonstrated good repeatability, with narrow limits of agreement and wCVs of 10{\%} or lower. Repeatability was poorest for the following high-order gray-level run-length (GLRL) gray-level zone size matrix (GLZSM) and neighborhood gray-tone difference matrix (NGTDM) parameters: GLRL intensity variability, GLZSM short-zone emphasis, GLZSM intensity nonuniformity, GLZSM intensity variability, GLZSM size zone variability, and NGTDM complexity, demonstrating wider agreement limits and wCVs of 50{\%} or greater. Conclusion MR imaging repeatability is better for global texture parameters than for local-regional texture parameters, indicating that global texture parameters should be sufficiently robust for clinical practice.},
author = {Gourtsoyianni, Sofia and Doumou, Georgia and Prezzi, Davide and Taylor, Benjamin and Stirling, J James and Taylor, N Jane and Siddique, Musib and Cook, Gary J. R. and Glynne-Jones, Robert and Goh, Vicky},
doi = {10.1148/radiol.2017161375},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Gourtsoyianni2017 - Primary rectal cancer - repeatability of global and local-regional MR imaging texture features.pdf:pdf},
issn = {0033-8419},
journal = {Radiology},
month = {may},
title = {{Primary Rectal Cancer: Repeatability of Global and Local-Regional MR Imaging Texture Features}},
url = {http://pubs.rsna.org/doi/10.1148/radiol.2017161375},
year = {2017}
}
@article{Immerkær1996,
abstract = {The paper presents a fast and simple method for estimating the variance of additive zero mean Gaussian noise in an image. The method can also be used to give a local estimate of the noise variance in the situation in which the noise variance varies across the image. It requires only the use of a 3 × 3 mask followed by a summation over the image or a local neighborhood. A total of 14 integer operations per pixel is necessary. The method performs well for a large range of noise variance values. In highly textured images or regions, though, the noise estimator perceives thin lines as noise.},
author = {Immerk{\ae}r, John},
doi = {10.1006/cviu.1996.0060},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Immerkaer1996 - Fast noise variance estimation.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
month = {sep},
number = {2},
pages = {300--302},
title = {{Fast Noise Variance Estimation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314296900600},
volume = {64},
year = {1996}
}
@article{Johnson2012,
abstract = {OBJECTIVE: In dual-energy CT (DECT), two CT datasets are acquired with different x-ray spectra. These spectra are generated using different tube potentials, partially also with additional filtration at 140 kVp. Spectral information can also be resolved by layer detectors or quantum-counting detectors. Several technical approaches-that is, sequential acquisition, rapid voltage switching, dual-source CT (DSCT), layer detector, quantum-counting detector-offer different spectral contrast and dose efficiency. Various postprocessing algorithms readily provide clinically relevant spectral information.$\backslash$n$\backslash$nCONCLUSION: DECT offers the possibility to exploit spectral information for diagnostic purposes. There are different technical approaches, all of which have inherent advantages and disadvantages, especially regarding spectral contrast and dose efficiency. There are numerous clinical applications of DECT that are easily accessible with specific postprocessing algorithms.},
author = {Johnson, Thorsten R. C.},
doi = {10.2214/AJR.12.9116},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Johnson2012 - Dual-energy CT - general principles.pdf:pdf},
issn = {0361-803X},
journal = {American Journal of Roentgenology},
keywords = {10,12,2012,2214,9116,accepted without revision,ajr,ct detector technology,doi,dual-,dual-energy ct,rapid voltage switching,received april 23,source ct,spectral ct},
month = {nov},
number = {5{\_}supplement},
pages = {S3--S8},
pmid = {23097165},
title = {{Dual-Energy CT: General Principles}},
url = {http://www.ajronline.org/doi/10.2214/AJR.12.9116},
volume = {199},
year = {2012}
}
@incollection{Schirra2008,
address = {Berlin, Heidelberg},
author = {Schirra, Stefan},
booktitle = {Algorithms - ESA 2008},
doi = {10.1007/978-3-540-87744-8_62},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Schirra2008 - How reliable are practical point-in-polygon strategies.pdf:pdf},
pages = {744--755},
publisher = {Springer Berlin Heidelberg},
title = {{How Reliable Are Practical Point-in-Polygon Strategies?}},
url = {http://link.springer.com/10.1007/978-3-540-87744-8{\_}62},
year = {2008}
}
@article{VanGriethuysen2017,
  title={Computational radiomics system to decode the radiographic phenotype},
  author={van Griethuysen, Joost JM and Fedorov, Andriy and Parmar, Chintan and Hosny, Ahmed and Aucoin, Nicole and Narayan, Vivek and Beets-Tan, Regina GH and Fillion-Robin, Jean-Christophe and Pieper, Steve and Aerts, Hugo JWL},
  journal={Cancer research},
  volume={77},
  number={21},
  pages={e104--e107},
  year={2017},
  publisher={AACR}
}
@article{Tajbakhsh2016,
abstract = {Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch? To address this question, we considered four distinct medical imaging applications in three specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from three different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that 1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; 2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; 3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and 4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.},
author = {Tajbakhsh, Nima and Shin, Jae Y. and Gurudu, Suryakanth R. and Hurst, R. Todd and Kendall, Christopher B. and Gotway, Michael B. and Liang, Jianming},
doi = {10.1109/TMI.2016.2535302},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Tajbakhsh2016 - Convolutional neural networks for medical image analysis - full training or fine tuning.pdf:pdf},
isbn = {0278-0062 VO - 35},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Carotid intima-media thickness,computer-aided detection,convolutional neural networks,deep learning,fine-tuning,medical image analysis,polyp detection,pulmonary embolism detection,video quality assessment},
number = {5},
pages = {1299--1312},
pmid = {26978662},
title = {{Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?}},
volume = {35},
year = {2016}
}
@article{Greenspan2016,
abstract = {The papers in this special section focus on the technology and applications supported by deep learning. Deep learning is a growing trend in general data analysis and has been termed one of the 10 breakthrough technologies of 2013. Deep learning is an improvement of artificial neural networks, consisting of more layers that permit higher levels of abstraction and improved predictions from data. To date, it is emerging as the leading machine-learning tool in the general imaging and computer vision domains. In particular, convolutional neural networks (CNNs) have proven to be powerful tools for a broad range of computer vision tasks. Deep CNNs automatically learn mid-level and high-level abstractions obtained from raw data (e.g., images). Recent results indicate that the generic descriptors extracted from CNNs are extremely effective in object recognition and localization in natural images. Medical image analysis groups across the world are quickly entering the field and applying CNNs and other deep learning methodologies to a wide variety of applications.},
archivePrefix = {arXiv},
arxivId = {1603.05959},
author = {Greenspan, H. and van Ginneken, B. and Summers, R. M.},
doi = {10.1109/TMI.2016.2553401},
eprint = {1603.05959},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Greenspan2016 - Deep learning in medical imaging - overview and future promise of an exciting new technique.pdf:pdf},
isbn = {0278-0062 VO - 35},
issn = {0278-0062},
journal = {IEEE Transactions on Medical Imaging},
number = {5},
pages = {1153--1159},
pmid = {25191215},
title = {{Guest Editorial Deep Learning in Medical Imaging: Overview and Future Promise of an Exciting New Technique}},
url = {http://ieeexplore.ieee.org/ielx7/42/7463083/07463094.pdf?tp={\&}arnumber=7463094{\&}isnumber=7463083{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=7463094},
volume = {35},
year = {2016}
}
@article{Lu2016,
abstract = {Remarkable progress has been made in image recog- nition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfullyemployCNNs tomedical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conductingunsupervisedCNN pre-trainingwith supervised fine-tuning.Another effectivemethod is transfer learning, i.e., fine-tuning CNNmodels pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 mil- lion parameters, and vary in numbers of layers.We then evaluate the influence of dataset scale and spatial image context on perfor- mance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.},
archivePrefix = {arXiv},
arxivId = {1602.03409},
author = {Lu, Le and Shin, Hoo-chang and Roth, Holger R and Gao, Mingchen and Lu, Le and Member, Senior and Xu, Ziyue and Nogues, Isabella and Yao, Jianhua and Mollura, Daniel and Summers, Ronald M},
doi = {10.1109/TMI.2016.2528162},
eprint = {1602.03409},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Shin2016 - Deep convolutional neural networks for computer-aided detection - CNN architectures data set characteristics and transfer learning.pdf:pdf},
isbn = {1558-254X (Electronic)$\backslash$r0278-0062 (Linking)},
issn = {0278-0062},
journal = {IEEE Transactions on Medical Imaging},
number = {5},
pages = {1285--1298},
pmid = {26886976},
title = {{Deep Convolutional Neural Networks for Computer-Aided Detection : CNN Architectures , Dataset Characteristics and Transfer Learning Deep Convolutional Neural Networks for Computer-Aided Detection : CNN Architectures , Dataset Characteristics and Transfer}},
volume = {35},
year = {2016}
}
@inproceedings{Zhang2001,
abstract = {Meshes are dominantly used to represent 3D models as they fit well with graphics rendering hardware. Features such as volume, moments, and Fourier transform coefficients need to be calculated from the mesh representation efficiently. We propose an algorithm to calculate these features without transforming the mesh into other representations such as the volumetric representation. To calculate a feature for a mesh, we show that we can first compute it for each elementary shape such as a triangle or a tetrahedron, and then add up all the values for the mesh. The algorithm is simple and efficient, with many potential applications},
author = {Zhang, Cha and Chen, Tsuhan},
booktitle = {Proceedings 2001 International Conference on Image Processing},
doi = {10.1109/ICIP.2001.958278},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Zhang - Efficient feature extraction for 2D-3D objects in mesh representation.pdf:pdf},
isbn = {0-7803-6725-1},
pages = {935--938},
publisher = {IEEE},
title = {{Efficient feature extraction for 2D/3D objects in mesh representation}},
url = {http://ieeexplore.ieee.org/document/958278/},
volume = {2},
year = {2001}
}
@article{Shafiq-ul-Hassan2017,
abstract = {PURPOSE Many radiomics features were originally developed for non-medical imaging applications and therefore original assumptions may need to be reexamined. In this study, we investigated the impact of slice thickness and pixel spacing (or pixel size) on radiomics features extracted from Computed Tomography (CT) phantom images acquired with different scanners as well as different acquisition and reconstruction parameters. The dependence of CT texture features on gray-level discretization was also evaluated. METHODS AND MATERIALS A texture phantom composed of 10 different cartridges of different materials was scanned on eight different CT scanners from three different manufacturers. The images were reconstructed for various slice thicknesses. For each slice thickness, the reconstruction Field Of View (FOV) was varied to render pixel sizes ranging from 0.39 to 0.98 mm. A fixed spherical region of interest (ROI) was contoured on the images of the shredded rubber cartridge and the 3D printed, 20{\%} fill, acrylonitrile butadiene styrene plastic cartridge (ABS20) for all phantom imaging sets. Radiomic features were extracted from the ROIs using an in-house program. Features categories were: shape (10), intensity (16), GLCM (24), GLZSM (11), GLRLM (11), and NGTDM (5), fractal dimensions (8) and first-order wavelets (128), for a total of 213 features. Voxel-size resampling was performed to investigate the usefulness of extracting features using a suitably chosen voxel size. Acquired phantom image sets were resampled to a voxel size of 1 × 1 × 2 mm(3) using linear interpolation. Image features were therefore extracted from resampled and original datasets and the absolute value of the percent coefficient of variation ({\%}COV) for each feature was calculated. Based on the {\%}COV values, features were classified in 3 groups: (1) features with large variations before and after resampling ({\%}COV {\textgreater}50); (2) features with diminished variation ({\%}COV {\textless}30) after resampling; and (3) features that had originally moderate variation ({\%}COV {\textless}50{\%}) and were negligibly affected by resampling. Group 2 features were further studied by modifying feature definitions to include voxel size. Original and voxel-size normalized features were used for interscanner comparisons. A subsequent analysis investigated feature dependency on gray-level discretization by extracting 51 texture features from ROIs from each of the 10 different phantom cartridges using 16, 32, 64, 128, and 256 gray levels. RESULTS Out of the 213 features extracted, 150 were reproducible across voxel sizes, 42 improved significantly ({\%}COV {\textless}30, Group 2) after resampling, and 21 had large variations before and after resampling (Group 1). Ten features improved significantly after definition modification effectively removed their voxel-size dependency. Interscanner comparison indicated that feature variability among scanners nearly vanished for 8 of these 10 features. Furthermore, 17 out of 51 texture features were found to be dependent on the number of gray levels. These features were redefined to include the number of gray levels which greatly reduced this dependency. CONCLUSION Voxel-size resampling is an appropriate pre-processing step for image datasets acquired with variable voxel sizes to obtain more reproducible CT features. We found that some of the radiomics features were voxel size and gray-level discretization-dependent. The introduction of normalizing factors in their definitions greatly reduced or removed these dependencies.},
author = {Shafiq-Ul-Hassan, Muhammad and Zhang, Geoffrey G. and Latifi, Kujtim and Ullah, Ghanim and Hunt, Dylan C. and Balagurunathan, Yoganand and Abdalah, Mahmoud Abrahem and Schabath, Matthew B. and Goldgof, Dmitry G. and Mackin, Dennis and Court, Laurence Edward and Gillies, Robert James and Moros, Eduardo Gerardo},
doi = {10.1002/mp.12123},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Shafiq-ul-Hassan2017 - Intrinsic dependencies of CT radiomic features on voxel size and number of gray levels.pdf:pdf},
issn = {2473-4209},
journal = {Medical physics},
keywords = {computed tomography,features,gray levels,phantom,radiomics,texture,voxel size},
month = {mar},
number = {3},
pages = {1050--1062},
pmid = {28112418},
title = {{Intrinsic dependencies of CT radiomic features on voxel size and number of gray levels.}},
url = {http://doi.wiley.com/10.1002/mp.12123 http://www.ncbi.nlm.nih.gov/pubmed/28112418 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5462462},
volume = {44},
year = {2017}
}
@article{Havaei2017,
abstract = {In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data.We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test data-set reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster.},
author = {Havaei, Mohammad and Davy, Axel and Warde-Farley, David and Biard, Antoine and Courville, Aaron and Bengio, Yoshua and Pal, Chris and Jodoin, Pierre Marc and Larochelle, Hugo},
doi = {10.1016/j.media.2016.05.004},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Havaei2017 - Brain tumor segmentation with deep neural networks.pdf:pdf},
isbn = {1361-8415},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Brain tumor segmentation,Cascaded convolutional neural networks,Convolutional neural networks,Deep neural networks},
pages = {18--31},
publisher = {Elsevier B.V.},
title = {{Brain tumor segmentation with Deep Neural Networks}},
url = {http://dx.doi.org/10.1016/j.media.2016.05.004},
volume = {35},
year = {2017}
}
@article{Kooi2017,
abstract = {Recent advances in machine learning yielded new techniques to train deep neural networks, which resulted in highly successful applications in many pattern recognition tasks such as object detection and speech recognition. In this paper we provide a head-to-head comparison between a state-of-the art in mammography CAD system, relying on a manually designed feature set and a Convolutional Neural Network (CNN), aiming for a system that can ultimately read mammograms independently. Both systems are trained on a large data set of around 45,000 images and results show the CNN outperforms the traditional CAD system at low sensitivity and performs comparable at high sensitivity. We subsequently investigate to what extent features such as location and patient information and commonly used manual features can still complement the network and see improvements at high specificity over the CNN especially with location and context features, which contain information not available to the CNN. Additionally, a reader study was performed, where the network was compared to certified screening radiologists on a patch level and we found no significant difference between the network and the readers.},
author = {Kooi, Thijs and Litjens, Geert and van Ginneken, Bram and Gubern-M{\'{e}}rida, Albert and S{\'{a}}nchez, Clara I. and Mann, Ritse and den Heeten, Ard and Karssemeijer, Nico},
doi = {10.1016/j.media.2016.07.007},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Kooi2017 - Large scale deep learning for computer aided detection of mammographic lesions.pdf:pdf},
isbn = {1361-8423 (Electronic)$\backslash$r1361-8415 (Linking)},
issn = {1361-8423},
journal = {Medical image analysis},
keywords = {Breast cancer,Computer aided detection,Convolutional neural networks,Deep learning,Machine learning,Mammography},
month = {jan},
pages = {303--312},
pmid = {27497072},
publisher = {Elsevier B.V.},
title = {{Large scale deep learning for computer aided detection of mammographic lesions.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27497072},
volume = {35},
year = {2017}
}
@article{Arindra2016,
abstract = {—We propose a novel Computer-Aided Detection (CAD) system for pulmonary nodules using multi-view convolu-tional networks (ConvNets), for which discriminative features are automatically learnt from the training data. The network is fed with nodule candidates obtained by combining three candidate de-tectors specifically designed for solid, subsolid, and large nodules. For each candidate, a set of 2-D patches from differently oriented planes is extracted. The proposed architecture comprises multiple streams of 2-D ConvNets, for which the outputs are combined using a dedicated fusion method to get the final classification. Data augmentation and dropout are applied to avoid overfitting. On 888 scans of the publicly available LIDC-IDRI dataset, our method reaches high detection sensitivities of 85.4{\%} and 90.1{\%} at 1 and 4 false positives per scan, respectively. An additional evaluation on independent datasets from the ANODE09 challenge and DLCST is performed. We showed that the proposed multi-view ConvNets is highly suited to be used for false positive reduction of a CAD system.},
author = {Arindra, Arnaud and Setio, Adiyoso and Ciompi, Francesco and Litjens, Geert and Gerke, Paul and Jacobs, Colin and {Van Riel}, Sarah J and {Winkler Wille}, Mathilde Marie and Naqibullah, Matiullah and S{\'{a}}nchez, Clara I and van Ginneken, Bram},
doi = {10.1109/TMI.2016.2536809},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Setio2010 - Pulmonary nodule detection in CT-images.pdf:pdf},
issn = {0278-0062},
journal = {Ieee Transactions on Medical Imaging},
keywords = {Index Terms—Computed tomography,computer-aided de-tection,convolutional networks,deep learning,lung cancer,pulmonary nodule},
number = {5},
pages = {1160--1169},
pmid = {26955024},
title = {{Pulmonary Nodule Detection in CT Images: False Positive Reduction Using Multi-View Convolutional Networks}},
volume = {35},
year = {2016}
}
@article{Ghafoorian2016,
abstract = {Convolutional neural networks (CNN) have been widely used for visual recognition tasks including semantic segmentation of images. While the existing methods consider uniformly sampled single- or multi-scale patches from the neighborhood of each voxel, this approach might be sub-optimal as it cap- tures and processes unnecessary details far away from the center of the patch. We instead propose to train CNNs with non-uniformly sampled patches that allow a wider extent for the sampled patches. This results in more captured contextual information, which is in particular of interest for biomedical image analysis, where the anatomical location of imaging fea- tures are often crucial. We evaluate and compare this strategy for white matter hyperintensity segmentation on a test set of 46 MRI scans. We show that the proposed method not only outperforms identical CNNs with uniform patches of the same size (0.780 Dice coefficient compared to 0.736), but also gets very close to the performance of an independent human expert (0.796 Dice coefficient)},
author = {Ghafoorian, M. and Karssemeijer, Nico and Heskes, T. and {Van Uder}, I. W. M. and {De Leeuw}, F. E. and Marchiori, E. and van Ginneken, Bram and Platel, B.},
doi = {10.1109/ISBI.2016.7493532},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Ghafoorian2016 - Non-uniform patch sampling with deep convolutional neural networks for white matter hyperintensity segmentation.pdf:pdf},
isbn = {9781479923502},
issn = {19458452},
journal = {Proceedings - International Symposium on Biomedical Imaging},
keywords = {convolutional neural network,deep learning,non-uniform patch,white matter hyperintensity},
pages = {1414--1417},
title = {{Non-uniform patch sampling with deep convolutional neural networks for white matter hyperintensity segmentation}},
volume = {2016-June},
year = {2016}
}
@article{Lever2016c,
author = {Lever, Jake and Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.3904},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Lever2016 - Logistic regression.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {1548-7091},
journal = {Nature Methods},
month = {jun},
number = {7},
pages = {541--542},
pmid = {25317452},
publisher = {Nature Publishing Group},
title = {{Points of Significance: Logistic regression}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.3335 http://www.nature.com/doifinder/10.1038/nmeth.3904},
volume = {13},
year = {2016}
}
@article{Altman2014,
abstract = {To generalize conclusions to a population, we must sample its variation.},
author = {Altman, Naomi and Krzywinski, Martin},
doi = {10.1038/nmeth.3224},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Altman2015 - Sources of variation.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {1548-7091},
journal = {Nature Methods},
month = {dec},
number = {1},
pages = {5--6},
pmid = {25699313},
publisher = {Nature Publishing Group},
title = {{Points of significance: Sources of variation}},
url = {http://dx.doi.org/10.1038/nmeth.3224 http://www.nature.com/doifinder/10.1038/nmeth.3224},
volume = {12},
year = {2014}
}
@article{Blainey2014,
author = {Blainey, Paul and Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.3091},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Blainey2014 - Replication.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {1548-7091},
journal = {Nature Methods},
month = {aug},
number = {9},
pages = {879--880},
pmid = {25317452},
title = {{Points of Significance: Replication}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.3091},
volume = {11},
year = {2014}
}
@article{Lever2016b,
author = {Lever, Jake and Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.4014},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Lever2016 - Regularization.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {1548-7091},
journal = {Nature Methods},
month = {sep},
number = {10},
pages = {803--804},
pmid = {25317452},
publisher = {Nature Publishing Group},
title = {{Points of Significance: Regularization}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.3335 http://www.nature.com/doifinder/10.1038/nmeth.4014},
volume = {13},
year = {2016}
}
@article{Krzywinski2014a,
abstract = {Nonparametric tests robustly compare skewed or ranked data.},
author = {Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.2937},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Krzywinski2014 - Nonparametric tests.pdf:pdf},
isbn = {1548-7091$\backslash$n1548-7105},
issn = {1548-7091},
journal = {Nature Methods},
month = {apr},
number = {5},
pages = {467--468},
pmid = {24820360},
title = {{Points of significance: Nonparametric tests}},
url = {http://www.nature.com/nmeth/journal/v11/n5/abs/nmeth.2937.html http://www.nature.com/doifinder/10.1038/nmeth.2937},
volume = {11},
year = {2014}
}
@article{Puga2015a,
author = {Puga, Jorge L{\'{o}}pez and Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.3550},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Puga2015 - Bayesian networks.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {1548-7091},
journal = {Nature Methods},
month = {aug},
number = {9},
pages = {799--800},
pmid = {25317452},
publisher = {Nature Publishing Group},
title = {{Points of Significance: Bayesian networks}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.3335 http://www.nature.com/doifinder/10.1038/nmeth.3550},
volume = {12},
year = {2015}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/LeCun2016 - Deep Learning.pdf:pdf},
isbn = {9780521835688},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {10463930},
title = {{Deep learning}},
url = {http://dx.doi.org/10.1038/nature14539},
volume = {521},
year = {2015}
}
@article{Krzywinski2015,
author = {Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.3665},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Altman2015 - Multiple linear regression.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {1548-7091},
journal = {Nature Methods},
month = {dec},
number = {12},
pages = {1103--1104},
pmid = {25317452},
publisher = {Nature Publishing Group},
title = {{Points of Significance: Multiple linear regression}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.3335 http://www.nature.com/doifinder/10.1038/nmeth.3665},
volume = {12},
year = {2015}
}
@inproceedings{VanGinneken2015,
abstract = {Convolutional neural networks (CNNs) have emerged as the most powerful technique for a range of different tasks in computer vision. Recent work suggested that CNN features are generic and can be used for classification tasks outside the exact domain for which the networks were trained. In this work we use the features from one such network, OverFeat, trained for object detection in natural images, for nodule detection in computed tomography scans. We use 865 scans from the publicly available LIDC data set, read by four thoracic radiologists. Nodule candidates are generated by a state-of-the-art nodule detection system. We extract 2D sagittal, coronal and axial patches for each nodule candidate and extract 4096 features from the penultimate layer of OverFeat and classify these with linear support vector machines. We show for various configurations that the off-the-shelf CNN features perform surprisingly well, but not as good as the dedicated detection system. When both approaches are combined, significantly better results are obtained than either approach alone. We conclude that CNN features have great potential to be used for detection tasks in volumetric medical data.},
author = {van Ginneken, Bram and Setio, Arnaud A. A. and Jacobs, Colin and Ciompi, Francesco},
booktitle = {2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)},
doi = {10.1109/ISBI.2015.7163869},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/VanGinneken2015 - Off-the-shelf convolutional neural network features for pulmonary nodula detection in computed tomography scans.pdf:pdf},
isbn = {978-1-4799-2374-8},
issn = {19458452},
keywords = {2D sagittal,Biomedical imaging,CNN features,Cancer,Computed tomography,Design automation,Feature extraction,LIDC data set,Lesions,Lungs,Nodule detection,OverFeat features,axial patches,computed tomography,computed tomography scans,computerised tomography,convolutional neural networks,coronal patches,feature extraction,image classification,linear support vector machines,medical image processing,neural nets,object detection,off-the-shelf convolutional neural network feature,pulmonary nodule detection,support vector machines,thoracic radiologists,volumetric medical data},
month = {apr},
pages = {286--289},
publisher = {IEEE},
title = {{Off-the-shelf convolutional neural network features for pulmonary nodule detection in computed tomography scans}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7163869 http://ieeexplore.ieee.org/document/7163869/},
year = {2015}
}
@article{Altman2015,
abstract = {The statistician knows...that in nature there never was a normal distribution, there never was a straight line, yet with normal and linear assumptions, known to be false, he can often derive results which match, to a useful approximation, those found in the real world.},
author = {Altman, Naomi and Krzywinski, Martin},
doi = {10.1038/nmeth.3627},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Altman2015 - Simple linear regression.pdf:pdf},
isbn = {1548-7091},
issn = {1548-7091},
journal = {Nature Methods},
month = {oct},
number = {11},
pages = {999--1000},
pmid = {26962577},
publisher = {Nature Publishing Group},
title = {{Points of Significance: Simple linear regression}},
url = {http://dx.doi.org/10.1038/nmeth.3627 http://www.nature.com/doifinder/10.1038/nmeth.3627},
volume = {12},
year = {2015}
}
@article{Puga2015,
abstract = {Nature Methods 12, 377 (2015). doi:10.1038/nmeth.3368},
author = {Puga, Jorge L{\'{o}}pez and Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.3368},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Puga2015 - Bayesian statistics.pdf:pdf},
isbn = {2913061001},
issn = {1548-7091},
journal = {Nature Methods},
month = {apr},
number = {5},
pages = {377--378},
pmid = {21830920},
title = {{Points of significance: Bayesian statistics}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.3368{\%}5Cnhttp://dx.doi.org/10.1038/nmeth.3368{\%}5Cnpapers3://publication/doi/10.1038/nmeth.3368 http://www.nature.com/doifinder/10.1038/nmeth.3368},
volume = {12},
year = {2015}
}
@article{Obuchowski2015,
abstract = {Quantitative biomarkers from medical images are becoming important tools for clinical diagnosis, staging, monitoring, treatment planning, and development of new therapies. While there is a rich history of the development of quantitative imaging biomarker (QIB) techniques, little attention has been paid to the validation and comparison of the computer algorithms that implement the QIB measurements. In this paper we provide a framework for QIB algorithm comparisons. We first review and compare various study designs, including designs with the true value (e.g. phantoms, digital reference images, and zero-change studies), designs with a reference standard (e.g. studies testing equivalence with a reference standard), and designs without a reference standard (e.g. agreement studies and studies of algorithm precision). The statistical methods for comparing QIB algorithms are then presented for various study types using both aggregate and disaggregate approaches. We propose a series of steps for establishing the performance of a QIB algorithm, identify limitations in the current statistical literature, and suggest future directions for research.},
author = {Obuchowski, Nancy A. and Reeves, Anthony P. and Huang, Erich P. and Wang, Xiao-feng and Buckler, Andrew J. and Kim, Hyun J. Grace and Barnhart, Huiman X. and Jackson, Edward F. and Giger, Maryellen L. and Pennello, Gene and Toledano, Alicia Y and Kalpathy-Cramer, Jayashree and Apanasovich, Tatiyana V. and Kinahan, Paul E. and Myers, Kyle J. and Goldgof, Dmitry B. and Barboriak, Daniel P. and Gillies, Robert J. and Schwartz, Lawrence H. and Sullivan, Daniel C. and {Algorithm Comparison Working Group}},
doi = {10.1177/0962280214537390},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Obuchowski2015 - Quantitative imaging biomarkers - a review of statistical methods for computer algorithm comparisons.pdf:pdf},
isbn = {0962-2802},
issn = {1477-0334},
journal = {Statistical methods in medical research},
keywords = {agreement,bias,image metrics,imaging biomarkers,precision,quantitative imaging,repeatability,reproducibility},
month = {feb},
number = {1},
pages = {68--106},
pmid = {24919829},
title = {{Quantitative imaging biomarkers: a review of statistical methods for computer algorithm comparisons.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24919829 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4263694},
volume = {24},
year = {2015}
}
@article{Altman2015a,
abstract = {Correlation implies association, but not causation. Conversely, causation implies association, but not correlation.},
author = {Altman, Naomi and Krzywinski, Martin},
doi = {10.1038/nmeth.3587},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Altman2015 - Association correlation and causation.pdf:pdf},
isbn = {1548-7091},
issn = {1548-7091},
journal = {Nature Methods},
month = {sep},
number = {10},
pages = {899--900},
pmid = {26688882},
publisher = {Nature Publishing Group},
title = {{Points of Significance: Association, correlation and causation}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.3587{\%}5Cnhttp://dx.doi.org/10.1038/nmeth.3587 http://www.nature.com/doifinder/10.1038/nmeth.3587},
volume = {12},
year = {2015}
}
@article{Altman2016b,
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Altman, Naomi and Krzywinski, Martin},
doi = {10.1038/nmeth.4120},
eprint = {9605103},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Altman2016 - P values and the search for significance.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {1548-7091},
journal = {Nature Methods},
month = {dec},
number = {1},
pages = {3--4},
pmid = {25699313},
primaryClass = {cs},
publisher = {Nature Publishing Group},
title = {{Points of significance: P values and the search for significance}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.4120},
volume = {14},
year = {2016}
}
@article{Lever2016,
abstract = {With four parameters I can fit an elephant and with five I can make him wiggle his trunk. —John von Neumann},
author = {Lever, Jake and Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.3968},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Lever2016 - Model selection and overfitting.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {1548-7091},
journal = {Nature Methods},
month = {aug},
number = {9},
pages = {703--704},
pmid = {25317452},
publisher = {Nature Publishing Group},
title = {{Points of Significance: Model selection and overfitting}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.3968 http://www.ncbi.nlm.nih.gov/pubmed/25317452},
volume = {13},
year = {2016}
}
@article{Kulesa2015,
abstract = {The bootstrap can be used to assess uncertainty of sample estimates.},
author = {Kulesa, Anthony and Krzywinski, Martin and Blainey, Paul and Altman, Naomi},
doi = {10.1038/nmeth.3414},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Kulesa2015 - Sampling distributions and the bootstrap.pdf:pdf},
isbn = {1548-7091},
issn = {1548-7091},
journal = {Nature Methods},
month = {may},
number = {6},
pages = {477--478},
pmid = {26221652},
publisher = {Nature Publishing Group},
title = {{Points of Significance: Sampling distributions and the bootstrap}},
url = {http://dx.doi.org/10.1038/nmeth.3414 http://www.nature.com/doifinder/10.1038/nmeth.3414},
volume = {12},
year = {2015}
}
@article{Krzywinski2014,
author = {Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.2900},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Krzywinski2014 - Comparing samples part II.pdf:pdf},
isbn = {1548-7091$\backslash$r1548-7105},
issn = {1548-7091},
journal = {Nature Methods},
month = {mar},
number = {4},
pages = {355--356},
pmid = {24344377},
publisher = {Nature Publishing Group},
title = {{Points of significance: Comparing samples—part II}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.2858 http://www.nature.com/doifinder/10.1038/nmeth.2900},
volume = {11},
year = {2014}
}
@article{Lopez2016,
abstract = {PURPOSE To build a framework for investigation of the associations between imaging, clinical target volumes (CTVs), and metabolic tumor volumes (MTVs) features for better understanding of the underlying information in the CTVs and dependencies between these volumes. High-throughput extraction of imaging and metabolomic quantitative features from magnetic resonance imaging (MRI) and magnetic resonance spectroscopic imaging of glioblastoma multiforme (GBM) results in tens of variables per patient. In radiation therapy of GBM the relevant metabolic tumor volumes (MTVs) are related to aberrant levels of N-acetyl aspartate (NAA) and choline (Cho). The corresponding clinical target volumes (CTVs) for radiation therapy are based on contrast-enhanced T1-weighted (CE-T1w) and T2-weighted (T2w)/fluid-attenuated inversion recovery MRI. METHODS AND MATERIALS Necrotic portions, enhancing lesion, and edema were manually contoured on CE-T1w/T2w images for 17 GBM patients. Clinical target volumes and MTVs for NAA (MTVNAA) and Cho (MTVCho) were constructed. Imaging and metabolic features related to size, shape, and signal intensities of the volumes were extracted. Tumors were also scored categorically for 10 semantic imaging traits by a neuroradiologist. All features were investigated for redundancy. Two-way correlations between imaging and CTVs/MTVs features were visualized as heatmaps. Associations between MTVNAA and MTVCho and imaging features were studied using Spearman correlation. RESULTS Forty-eight imaging features were extracted per patient. Half of the imaging traits were replaced with automatically extracted continuous variables. Twenty features were extracted from CTVs and MTVs. A series of semantic imaging traits were replaced with automatically extracted continuous variables. There were multiple (22) significant correlations of imaging measures with CTVs/MTVNAA, whereas there were only 6 with CTVs/MTVCho. CONCLUSIONS A framework for investigation of codependencies between MRI and magnetic resonance spectroscopic imaging radiomic features and CTVs/MTVs has been established. The MTV for NAA was found to be closely associated with MRI volumes, whereas very few imaging features were related to MTVCho, indicating that Cho provides additional information to imaging.},
author = {Lopez, Christopher J. and Nagornaya, Natalya and Parra, Nestor A. and Kwon, Deukwoo and Ishkanian, Fazilat and Markoe, Arnold M. and Maudsley, Andrew and Stoyanova, Radka},
doi = {10.1016/j.ijrobp.2016.11.011},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Lopez2016 - Association of radiomics and metabolic tumor volumes in radiation treatment of glioblastoma multiforme.pdf:pdf},
issn = {1879-355X},
journal = {International journal of radiation oncology, biology, physics},
month = {nov},
number = {3},
pages = {586--595},
pmid = {28011044},
publisher = {Elsevier Inc.},
title = {{Association of Radiomics and Metabolic Tumor Volumes in Radiation Treatment of Glioblastoma Multiforme.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0360301616334332 http://www.ncbi.nlm.nih.gov/pubmed/28011044},
volume = {97},
year = {2016}
}
@article{Lever2016a,
abstract = {It is important to understand both what a classification metric expresses and what it hides. Last month we examined the use of logistic regression for classifica-tion, in which the class of a data point is predicted given training data 1 . This month, we look at how to evaluate classifier performance on a test set—data that were not used for training and for which the true classification is known. Classifiers are commonly evaluated using either a numeric metric, such as accuracy, or a graphical repre-sentation of performance, such as a receiver operating characteristic (ROC) curve. We will examine some common classifier metrics and discuss the pitfalls of relying on a single metric. Metrics help us understand how a classifier performs; many are available, some with numerous tunable parameters. Understanding metrics is also critical for evaluating reports by others—if a study presents a single metric, one might question the performance of the classifier when evaluated using other metrics. To illustrate the pro-cess of choosing a metric, we will simulate a hypothetical diagnostic test. This test classifies a patient as having or not having a deadly disease on the basis of multiple clinical factors. In evaluating the classifier, we consider only the results of the test; neither the underly-ing mechanism of classification nor the underlying clinical factors are relevant. Classification metrics are calculated from true positives (TPs), false positives (FPs), false negatives (FNs) and true negatives (TNs), all of which are tabulated in the so-called confusion matrix (Fig. 1). The relevance of each of these four quantities will depend on the purpose of the classifier and motivate the choice of metric. For a medical test that determines whether patients receive a treatment that is cheap, safe and effective, FPs would not be as important as FNs, which would represent patients who might suffer without ade-quate treatment. In contrast, if the treatment were an experimental drug, then a very conservative test with few FPs would be required to avoid testing the drug on unaffected individuals. In Figure 2 we show three classification scenarios for four differ-ent metrics: accuracy, sensitivity, precision and F 1 . In each panel, all of the scenarios have the same value (0.8) of a given metric. Accuracy is the fraction of predictions that are true. Although this metric is easy to interpret, high accuracy does not necessarily characterize a good classifier. For instance, it tells us nothing about whether FNs or FPs are more common (Fig. 2a). If the disease is rare, predict-ing that all the subjects will be negative offers high accuracy but is not useful for diagnosis. A useful measure for understanding FNs is sensitivity (also called recall or the true positive rate), which is the proportion of known positives that are predicted correctly. However, neither TNs nor FPs affect this metric, and a classifier that simply predicts that all data points are positive has high sensitivity (Fig. 2b). Specificity, which measures the fraction of actual negatives that are correctly predicted, suffers from a similar weakness: not accounting for FNs or TPs. Both TPs and FPs are captured by precision (also called the positive predictive value), which is the proportion of pre-dicted positives that are correct. However, precision captures neither TNs nor FNs (Fig. 2c). A very conservative test that predicts only one subject will have the disease—the case that is most certain—has a perfect precision score, even though it misses any other affected subjects with a less certain diagnosis. Ideally a medical test should have very low numbers of both FNs and FPs. Individuals who do not have the disease should not be given unnecessary treatment or be burdened with the stress of a positive result, and those who do have the disease should not be given false optimism about being disease free. Several aggregate metrics have been proposed for classification evaluation that more completely summarize the confusion matrix. The most popular is the F $\beta$ score, which uses the parameter $\beta$ to control the balance of recall and precision and is defined as F $\beta$ = (1 + $\beta$) 2 (Precision × Recall)/($\beta$ 2 × Precision + Recall). As $\beta$ decreases, precision is given greater weight. With $\beta$ = 1, we have the commonly used F 1 score, which balances recall and precision equally and reduces to the sim-pler equation 2TP/(2TP + FP + FN). The F $\beta$ score does not capture the full confusion matrix because it is based on the recall and precision, neither of which uses TNs, which might be important for tests of very prevalent diseases. One approach that can capture all the data in the confusion matrix is the Matthews correlation coefficient (MCC), which ranges from –1 (when the classification is always wrong) to 0 (when it is no better than random) to 1 (when it is always correct). It should be noted that in a comparison of the results of two classifiers, one Figure 2 | The same value of a metric can correspond to very different classifier performance. (a–d) Each panel shows three different classification scenarios with a table of corresponding values of accuracy (ac), sensitivity (sn), precision (pr), F 1 score (F 1) and Matthews correlation coefficient (MCC). Scenarios in a group have the same value (0.8) for the metric in bold in each table: (a) accuracy, (b) sensitivity (recall), (c) precision and (d) F 1 score. In each panel, those observations that do not contribute to the corresponding metric are struck through with a red line. The color-coding is the same as in Figure 1; for example, blue circles (cases known to be positive) on a gray background (predicted to be negative) are FNs.},
author = {Lever, Jake and Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.3945},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Lever2016 - Classification evaluation.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {1548-7091},
journal = {Nature Methods},
month = {jul},
number = {8},
pages = {603--604},
pmid = {25317452},
title = {{Points of Significance: Classification evaluation}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.3945},
volume = {13},
year = {2016}
}
@article{Krzywinski2013a,
abstract = {Statistics does not tell us whether we are right. It tells us the chances of being wrong.},
author = {Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.2613},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Krzwinsky2013 - Importance of being uncertain.pdf:pdf},
isbn = {1548-7091$\backslash$n1548-7105},
issn = {1548-7091},
journal = {Nature Methods},
month = {aug},
number = {9},
pages = {809--810},
pmid = {24161969},
publisher = {Nature Publishing Group},
title = {{Points of significance: Importance of being uncertain}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.2613},
volume = {10},
year = {2013}
}
@article{Altman2016a,
abstract = {Residual plots can be used to validate assumptions about the regression model.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Altman, Naomi and Krzywinski, Martin},
doi = {10.1038/nmeth.3854},
eprint = {9605103},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Altman2016 - Regression diagnostics.pdf:pdf},
isbn = {0-7803-3213-X},
issn = {1548-7091},
journal = {Nature Methods},
month = {apr},
number = {5},
pages = {385--386},
pmid = {17255001},
primaryClass = {cs},
publisher = {Nature Publishing Group},
title = {{Points of Significance: Regression diagnostics}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.3854},
volume = {13},
year = {2016}
}
@article{Krzywinski2013,
abstract = {The ability to detect experimental effects is undermined in studies that lack power.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.2738},
eprint = {NIHMS150003},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Krzywinski2013 - Power and sample size.pdf:pdf},
isbn = {1548-7091$\backslash$n1548-7105},
issn = {1548-7091},
journal = {Nature Methods},
month = {nov},
number = {12},
pages = {1139--1140},
pmid = {24161969},
title = {{Points of significance: Power and sample size}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.2738},
volume = {10},
year = {2013}
}
@article{Puga2015b,
author = {Puga, Jorge L{\'{o}}pez and Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.3335},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Puga2015 - Bayes theorem.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {1548-7091},
journal = {Nature Methods},
month = {mar},
number = {4},
pages = {277--278},
pmid = {25317452},
publisher = {Nature Publishing Group},
title = {{Points of Significance: Bayes' theorem}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.3335},
volume = {12},
year = {2015}
}
@article{Altman2016,
abstract = {Some outliers influence the regression fit more than others.},
author = {Altman, Naomi and Krzywinski, Martin},
doi = {10.1038/nmeth.3812},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Altman2016 - Analyzing outliers influential or nuisance.pdf:pdf},
isbn = {1548-7091},
issn = {1548-7091},
journal = {Nature Methods},
keywords = {points of significance,s month},
month = {mar},
number = {4},
pages = {281--282},
pmid = {27482566},
title = {{Points of Significance: Analyzing outliers: influential or nuisance?}},
url = {http://dx.doi.org/10.1038/nmeth.3812 http://www.nature.com/doifinder/10.1038/nmeth.3812},
volume = {13},
year = {2016}
}
@article{Gjesteby2016,
author = {Gjesteby, Lars and {De Man}, Bruno and Jin, Yannan and Paganetti, Harald and Verburg, Joost and Giantsoudi, Drosoula and Wang, Ge},
doi = {10.1109/ACCESS.2016.2608621},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Gjesteby2016 - Metal artifact reduction in ct - where are we after four decades.pdf:pdf},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {Biomedical imaging,computed tomography,metal artifact reduction,radiation therapy,reconstruction algorithms},
pages = {5826--5849},
title = {{Metal Artifact Reduction in CT: Where Are We After Four Decades?}},
url = {http://ieeexplore.ieee.org/document/7565564/},
volume = {4},
year = {2016}
}
@inproceedings{Dubuc1987,
abstract = {There are many definitions of the fractal dimension of an object, including box dimension. Bouligand-Minkowski dimension, and intersection dimension. Although they are all equivalent in the continuous domain, when discretized and applied to digitized data, they differ substantially. We show that the standard implementations of these definitions on self-afline curves with known fractal dimension (Weierstrass-Mandelbrot, Kiesswetter, fractional Brownian motion) yield results with errors ranging up to 5 or 10{\%}. An analysis of the source of these errors led to a new algorithm in 1-D. called the variation method, which yielded accurate results. The variation method uses the notion of e-variation to measure the amplitude of the one-dimensional function in an e-neighborhood. The order of growth of the integral of the e-variation, as E tends toward zero, is directly related to the fractal dimension. In this paper, we extend the variation method to higher dimensions and show that, in the limit, it is equivalent to the classical box counting method. The result is an algorithm for reliably estimating the fractal dimension of surfaces or, more generally, graphs of functions of several variables. The algorithm is tested on surfaces with known fractal dimension and is applied to the study of rough surfaces.},
address = {San Diego, CA},
author = {Dubuc, B. and Roques-Carmes, C. and Tricot, C. and Zucker, S. W.},
booktitle = {SPIE Visual communications and image processing II},
doi = {10.1117/12.976511},
editor = {Hsing, T. Russell},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Dubuc1987 - The variation method - a technique to estimate the fractal dimension of surfaces.pdf:pdf},
month = {oct},
pages = {241--248},
title = {{The variation method: a technique to estimate the fractal dimension of surfaces}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1249913},
volume = {845},
year = {1987}
}
@inproceedings{Moller2005,
abstract = {We present a clean algorithm for determining whether a ray intersects a triangle. The algorithm translates the origin of the ray and then changes the base of that vector which yields a vector (t u v)T, where t is the distance to the plane in which the triangle lies and (u, v) represents the coordinates inside the triangle.One advantage of this method is that the plane equation need not be computed on the fly nor be stored, which can amount to significant memory savings for triangle meshes. As we found our method to be comparable in speed to previous methods, we believe it is the fastest ray/triangle intersection routine for triangles which do not have precomputed plane equations.},
address = {New York, New York, USA},
author = {M{\"{o}}ller, Tomas and Trumbore, Ben},
booktitle = {ACM SIGGRAPH 2005 Courses - SIGGRAPH '05},
doi = {10.1145/1198555.1198746},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/M{\"{o}}ller2005 - Fast minimum storage ray triangle intersection.pdf:pdf},
pages = {7},
publisher = {ACM Press},
title = {{Fast, minimum storage ray/triangle intersection}},
url = {http://portal.acm.org/citation.cfm?doid=1198555.1198746},
year = {2005}
}
@article{Pohlman1996,
abstract = {The goal of this study was to develop a technique to distinguish benign and malignant breast lesions in secondarily digitized mammograms. A set of 51 mammograms (two views/patient) containing lesions of known pathology were evaluated using six different morphological descriptors: circularity, $\mu$R/$\sigma$R (where $\mu$R=mean radial distance of tumor boundary, $\sigma$R=standard deviation); compactness, P2/A (where P=perimeter length of tumor boundary and A=area of the tumor); normalized moment classifier; fractal dimension; and a tumor boundary roughness (TBR) measurement (the number of angles in the tumor boundary with more than one boundary point divided by the total number of angles in the boundary). The lesion was segmented from the surrounding background using an adaptive region growing technique. Ninety-seven percent of the lesions were segmented using this approach. An ROC analysis was performed for each parameter and the results of this analysis were compared to each other and to those obtained from a subjective review by two board-certified radiologists who specialize in mammography. The results of the analysis indicate that all six parameters are diagnostic for malignancy with areas under their ROC curves ranging from 0.759 to 0.928. We observed a trend towards increased specificity at low false-negative rates (0.01 and 0.001) with the TBR measurement. Additionally, the diagnostic accuracy of a classification model based on this parameter was similar to that of the subjective reviewers.},
author = {Pohlman, Scott and Powell, Kimerly A. and Obuchowski, Nancy A. and Chilcote, William A. and Grundfest-Broniatowski, Sharon},
doi = {10.1118/1.597707},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Pohlman1996 - Quantitative classification of breast tumors in digitized mammograms.pdf:pdf},
issn = {00942405},
journal = {Medical Physics},
month = {aug},
number = {8},
pages = {1337--1345},
title = {{Quantitative classification of breast tumors in digitized mammograms}},
url = {http://doi.wiley.com/10.1118/1.597707},
volume = {23},
year = {1996}
}
@article{Shen1994,
abstract = {The authors have developed a set of shape factors to measure the roughness of contours of calcifications in mammograms and for use in their classification as malignant or benign. The analysis of mammograms is performed in three stages. First, a region growing technique is used to obtain the contours of calcifications. Then, three measures of shape features, including compactness, moments, and Fourier descriptors are computed for each region. Finally, their applicability for classification is studied by using the three shape measures to form feature vectors. Classification of 143 calcifications from 18 biopsy-proven cases as benign or malignant using the three measures with the nearest-neighbor method was 100{\%} accurate.},
author = {Shen, Liang and Rangayyan, Rangaraj M. and Desautels, J. E. Leo},
doi = {10.1109/42.293919},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Shen1994 - Application of shape analysis to mammographic calcifications.pdf:pdf},
isbn = {0-8186-2742-5},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
number = {2},
pages = {263--274},
pmid = {18218503},
title = {{Application of shape analysis to mammographic calcifications}},
volume = {13},
year = {1994}
}
@article{Zwanenburg2016,
abstract = {While analysis of medical images has practically taken place since the first image was recorded, high throughput analysis of medical images is a more recent phenomenon. The aim of such a radiomics process is to provide decision support based on medical imaging. Part of the radiomics process is the conversion of image data into numerical features which capture different medical image aspects, and can be subsequently correlated as biomarkers to e.g. expected oncological treatment outcome. With the growth of the radiomics field, it has become clear that results are often difficult to reproduce, that standards for image processing and feature extraction are missing, and that reporting guidelines are absent. The image biomarker standardisation initiative (IBSI) seeks to address these issues. The current document provides definitions for a large number of image features as well as common image processing elements.},
archivePrefix = {arXiv},
arxivId = {1612.07003},
author = {Zwanenburg, Alex and Leger, Stefan and Valli{\`{e}}res, Martin and L{\"{o}}ck, Steffen},
eprint = {1612.07003},
journal = {eprint arXiv:1612.07003 [cs.CV]},
month = {dec},
title = {{Image biomarker standardisation initiative}},
url = {http://arxiv.org/abs/1612.07003},
year = {2016}
}
@article{Leijenaar2013,
abstract = {PURPOSE: Besides basic measurements as maximum standardized uptake value (SUV)max or SUVmean derived from 18F-FDG positron emission tomography (PET) scans, more advanced quantitative imaging features (i.e. "Radiomics" features) are increasingly investigated for treatment monitoring, outcome prediction, or as potential biomarkers. With these prospected applications of Radiomics features, it is a requisite that they provide robust and reliable measurements. The aim of our study was therefore to perform an integrated stability analysis of a large number of PET-derived features in non-small cell lung carcinoma (NSCLC), based on both a test-retest and an inter-observer setup.$\backslash$n$\backslash$nMETHODS: Eleven NSCLC patients were included in the test-retest cohort. Patients underwent repeated PET imaging within a one day interval, before any treatment was delivered. Lesions were delineated by applying a threshold of 50{\%} of the maximum uptake value within the tumor. Twenty-three NSCLC patients were included in the inter-observer cohort. Patients underwent a diagnostic whole body PET-computed tomography (CT). Lesions were manually delineated based on fused PET-CT, using a standardized clinical delineation protocol. Delineation was performed independently by five observers, blinded to each other. Fifteen first order statistics, 39 descriptors of intensity volume histograms, eight geometric features and 44 textural features were extracted. For every feature, test-retest and inter-observer stability was assessed with the intra-class correlation coefficient (ICC) and the coefficient of variability, normalized to mean and range. Similarity between test-retest and inter-observer stability rankings of features was assessed with Spearman's rank correlation coefficient.$\backslash$n$\backslash$nRESULTS: Results showed that the majority of assessed features had both a high test-retest (71{\%}) and inter-observer (91{\%}) stability in terms of their ICC. Overall, features more stable in repeated PET imaging were also found to be more robust against inter-observer variability.$\backslash$n$\backslash$nCONCLUSION: Results suggest that further research of quantitative imaging features is warranted with respect to more advanced applications of PET imaging as being used for treatment monitoring, outcome prediction or imaging biomarkers.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Leijenaar, Ralph T. H. and Carvalho, Sara and Rios-Velazquez, Emmanuel and van Elmpt, Wouter J. C. and Parmar, Chintan and Hoekstra, Otto S. and Hoekstra, Corneline J. and Boellaard, Ronald and Dekker, Andr{\'{e}} L. A. J. and Gillies, Robert J. and Aerts, Hugo J. W. L. and Lambin, Philippe},
doi = {10.3109/0284186X.2013.812798},
eprint = {15334406},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Leijenaar2013 - Stability of FDG PET radiomics features.pdf:pdf},
isbn = {1651-226X (Electronic)$\backslash$r0284-186X (Linking)},
issn = {0284-186X},
journal = {Acta Oncologica},
keywords = {Carcinoma,Computer-Assisted,Fluorodeoxyglucose F18,Fluorodeoxyglucose F18: diagnostic use,Humans,Image-Guided,Lung Neoplasms,Lung Neoplasms: pathology,Lung Neoplasms: radionuclide imaging,Lung Neoplasms: radiotherapy,Non-Small-Cell Lung,Non-Small-Cell Lung: pathology,Non-Small-Cell Lung: radionuclide imagi,Non-Small-Cell Lung: radiotherapy,Observer Variation,Positron-Emission Tomography,Prognosis,Radiopharmaceuticals,Radiopharmaceuticals: diagnostic use,Radiotherapy,Radiotherapy Planning,Tomography,X-Ray Computed},
month = {oct},
number = {7},
pages = {1391--1397},
pmid = {24047337},
title = {{Stability of FDG-PET Radiomics features: An integrated analysis of test-retest and inter-observer variability}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24047337 http://www.tandfonline.com/doi/full/10.3109/0284186X.2013.812798},
volume = {52},
year = {2013}
}
@article{Paul2016,
abstract = {Lung cancer is the most common cause of cancer-related deaths in the USA. It can be detected and diagnosed using computed tomography images. For an automated classifier, identifying predictive features from medical images is a key concern. Deep feature extraction using pretrained convolutional neural networks (CNNs) has recently been successfully applied in some image domains. Here, we applied a pretrained CNN to extract deep features from 40 computed tomography images, with contrast, of non-small cell adenocarcinoma lung cancer, and combined deep features with traditional image features and trained classifiers to predict short- and long-term survivors. We experimented with several pretrained CNNs and several feature selection strategies. The best previously reported accuracy when using traditional quantitative features was 77.5{\%} (area under the curve [AUC], 0.712), which was achieved by a decision tree classifier. The best reported accuracy from transfer learning and deep features was 77.5{\%} (AUC, 0.713) using a decision tree classifier. When extracted deep neural network features were combined with traditional quantitative features, we obtained an accuracy of 90{\%} (AUC, 0.935) with the 5 best post-rectified linear unit features extracted from a vgg-f pretrained CNN and the 5 best traditional features. The best results were achieved with the symmetric uncertainty feature ranking algorithm followed by a random forests classifier.},
author = {Paul, Rahul and Hawkins, Samuel H. and Balagurunathan, Yoganand and Schabath, Matthew B. and Gillies, Robert J. and Hall, Lawrence O. and Goldgof, Dmitry B.},
doi = {10.18383/j.tom.2016.00211},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Paul2016 - Deep Feature Transfer Learning in Combination with Traditional Features Predicts Survival Among Patients with Lung Adenocarcinoma.pdf:pdf},
journal = {Tomography},
keywords = {adenocarcinoma,computed tomography,deep features,deep neural network,lung cancer,pre-trained CNN,symmet-ric uncertainty,transfer learning},
number = {4},
pages = {388--395},
title = {{Deep Feature Transfer Learning in Combination with Traditional Features Predicts Survival Among Patients with Lung Adenocarcinoma}},
volume = {2},
year = {2016}
}
@article{VanTimmeren2016,
abstract = {Radiomics is an objective method for extracting quantitative information from medical images. However, in radiomics, standardization, overfitting, and generalization are major challenges to be overcome. Test–retest experiments can be used to select robust radiomic features that have minimal variation. Currently, it is unknown whether they should be identified for each disease (disease specific) or are only imaging device-specific (computed tomography [CT]-specific). Here, we performed a test–retest analysis on CT scans of 40 patients with rectal cancer in a clinical setting. Correlation between radiomic features was assessed using the concordance correlation coefficient (CCC). In total, only 9/542 features have a CCC {\textgreater} 0.85. Furthermore, results were compared with the test–retest results on CT scans of 27 patients with lung cancer with a 15-minute interval. Results show that 446/542 features have a higher CCC for the test–retest analysis of the data set of patients with lung cancer than for patients with rectal cancer. The importance of controlling factors such as scanners, imaging protocol, reconstruction methods, and time points in a radiomics analysis is shown. Moreover, the results imply that test–retest analyses should be performed before each radiomics study. More research is required to independently evaluate the effect of each factor.},
author = {van Timmeren, Janna E. and Leijenaar, Ralph T. H. and van Elmpt, Wouter J. C. and Wang, Jiazhou and Zhang, Zhen and Dekker, Andr{\'{e}} and Lambin, Philippe},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/VanTimmeren2016 - Test–Retest Data for Radiomics Feature Stability Analysis Generalizable or Study-Specific.pdf:pdf},
journal = {Tomography},
keywords = {abbreviations,and generalization are major,ccc,challenges to be overcome,computed tomography,concordance correlation coefficient,ct,however,in,information from medical images,method for extracting quantitative,overfitting,radiomics,radiomics is an objective,retest,standardization,test},
number = {4},
pages = {361--365},
title = {{Test-retest data for radiomics feature stability analysis: generalizable or study specific?}},
volume = {2},
year = {2016}
}
@article{Kalpathy-Cramer2016,
abstract = {Radiomics is to provide quantitative descriptors of normal and abnormal tissues during classification and prediction tasks in radiology and oncology. Quantitative Imaging Network members are developing radiomic “feature” sets to characterize tumors, in general, the size, shape, texture, intensity, margin, and other aspects of the imaging features of nodules and lesions. Efforts are ongoing for developing an ontology to describe radiomic features for lung nodules, with the main classes consisting of size, local and global shape descriptors, margin, intensity, and texture-based features, which are based on wavelets, Laplacian of Gaussians, Law's features, gray-level cooccurrence matrices, and run-length features. The purpose of this study is to investigate the sensitivity of quantitative descriptors of pulmonary nodules to segmentations and to illustrate comparisons across different feature types and features computed by different implementations of feature extraction algorithms. We calculated the concordance correlation coefficients of the features as a measure of their stability with the underlying segmentation; 68{\%} of the 830 features in this study had a concordance CC of ≥0.75. Pairwise correlation coefficients between pairs of features were used to uncover associations between features, particularly as measured by different participants. A graphical model approach was used to enumerate the number of uncorrelated feature groups at given thresholds of correlation. At a threshold of 0.75 and 0.95, there were 75 and 246 subgroups, respectively, providing a measure for the features' redundancy.},
author = {Kalpathy-Cramer, Jayashree and Mamomov, Artem and Zhao, Binsheng and Lu, Lin and Cherezov, Dmitry and Napel, Sandy and Echegaray, Sebastian and Rubin, Daniel and McNitt-Gray, Michale and Lo, Pechin and Sieren, Jessica C. and Uthoff, Johanna and Dilger, Samantha K.N. and Driscoll, Brandan and Yeung, Ivan and Hadjiiski, Lubomir and Cha, Kenny and Balagurunathan, Yoganand and Gillies, Robert J. and Goldgof, Dmitry B.},
doi = {10.18383/j.tom.2016.00235},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Kalpathy2016 - Radiomics of lung nodules - A multi-institutional study of robustness and agreement of quantitative imaging features.pdf:pdf},
issn = {2379-1381},
journal = {Tomography},
keywords = {abbreviations,computed tomography,ct,glcm,global shape descriptors,gray level co-occurrence matrix,gsds,imaging archive,imaging features,local,lsds,lung cancer,qin,quantitative imaging network,radiomics,region of interest,reproducibility,roi,shape descriptors,tcia,the cancer,university of south florida},
month = {dec},
number = {4},
pages = {430--437},
title = {{Radiomics of Lung Nodules: A Multi-Institutional Study of Robustness and Agreement of Quantitative Imaging Features}},
url = {http://digitalpub.tomography.org/i/763956-vol-2-no-4-dec-2016/199},
volume = {2},
year = {2016}
}
@article{Kickingereder2016a,
abstract = {We found associations between established MR imaging features and key molecular characteristics, although not of sufficient strength to allow the generation of machine learning classification models for reliable and clinically meaningful prediction of the assessed molecular characteristics in patients with newly diagnosed glioblastoma.},
author = {Kickingereder, Philipp and Bonekamp, David and Nowosielski, Martha and Kratz, Annekathrin and Sill, Martin and Burth, Sina and Wick, Antje and Eidel, Oliver and Schlemmer, Heinz-Peter and Radbruch, Alexander and Debus, J{\"{u}}rgen and Herold-Mende, Christel and Unterberg, Andreas and Jones, David and Pfister, Stefan and Wick, Wolfgang and von Deimling, Andreas and Bendszus, Martin and Capper, David},
doi = {10.1148/radiol.2016161382},
issn = {0033-8419},
journal = {Radiology},
month = {dec},
number = {3},
pages = {907--918},
pmid = {27636026},
title = {{Radiogenomics of Glioblastoma: Machine Learning–based Classification of Molecular Characteristics by Using Multiparametric and Multiregional MR Imaging Features}},
url = {http://pubs.rsna.org/doi/10.1148/radiol.2016161382},
volume = {281},
year = {2016}
}
@misc{Moshtagh2005,
author = {Moshtagh, Nima},
doi = {10.1.1.116.7691},
issn = {0022-3239},
title = {{Minimum volume enclosing ellipsoids}},
year = {2005}
}
@article{OConnor2016,
author = {O'Connor, James P. B. and Aboagye, Eric O. and Adams, Judith E. and Aerts, Hugo J. W. L. and Barrington, Sally F. and Beer, Ambros J. and Boellaard, Ronald and Bohndiek, Sarah E. and Brady, Michael and Brown, Gina and Buckley, David L. and Chenevert, Thomas L. and Clarke, Laurence P. and Collette, Sandra and Cook, Gary J. and DeSouza, Nandita M. and Dickson, John C. and Dive, Caroline and Evelhoch, Jeffrey L. and Faivre-Finn, Corinne and Gallagher, Ferdia A. and Gilbert, Fiona J. and Gillies, Robert J. and Goh, Vicky and Griffiths, John R. and Groves, Ashley M. and Halligan, Steve and Harris, Adrian L. and Hawkes, David J. and Hoekstra, Otto S. and Huang, Erich P. and Hutton, Brian F. and Jackson, Edward F. and Jayson, Gordon C. and Jones, Andrew and Koh, Dow-Mu and Lacombe, Denis and Lambin, Philippe and Lassau, Nathalie and Leach, Martin O. and Lee, Ting-Yim and Leen, Edward L. and Lewis, Jason S. and Liu, Yan and Lythgoe, Mark F. and Manoharan, Prakash and Maxwell, Ross J. and Miles, Kenneth A. and Morgan, Bruno and Morris, Steve and Ng, Tony and Padhani, Anwar R. and Parker, Geoff J. M. and Partridge, Mike and Pathak, Arvind P. and Peet, Andrew C. and Punwani, Shonit and Reynolds, Andrew R. and Robinson, Simon P. and Shankar, Lalitha K. and Sharma, Ricky A. and Soloviev, Dmitry and Stroobants, Sigrid and Sullivan, Daniel C. and Taylor, Stuart A. and Tofts, Paul S. and Tozer, Gillian M. and van Herk, Marcel and Walker-Samuel, Simon and Wason, James and Williams, Kaye J. and Workman, Paul and Yankeelov, Thomas E. and Brindle, Kevin M. and McShane, Lisa M. and Jackson, Alan and Waterton, John C.},
doi = {10.1038/nrclinonc.2016.162},
issn = {1759-4774},
journal = {Nature Reviews Clinical Oncology},
month = {mar},
number = {3},
pages = {169--186},
publisher = {Nature Publishing Group},
title = {{Imaging biomarker roadmap for cancer studies}},
url = {http://dx.doi.org/10.1038/nrclinonc.2016.162 http://www.nature.com/doifinder/10.1038/nrclinonc.2016.162 http://www.nature.com/articles/nrclinonc.2016.162},
volume = {14},
year = {2017}
}

@article{Todd2007,
author = {Todd, Michael J. and Yıldırım, E. Alper},
doi = {10.1016/j.dam.2007.02.013},
isbn = {0166-218X},
issn = {0166218X},
journal = {Discrete Applied Mathematics},
keywords = {Approximation algorithms,Core sets,Ellipsoid method,L{\"{o}}wner ellipsoid,Rounding of polytopes},
month = {aug},
number = {13},
pages = {1731--1744},
title = {{On Khachiyan's algorithm for the computation of minimum-volume enclosing ellipsoids}},
volume = {155},
year = {2007}
}
@article{Prasanna2016,
abstract = {In this paper, we introduce a new radiomic descriptor, Co-occurrence of Local Anisotropic Gradient Orientations (CoLlAGe) for capturing subtle differences between benign and pathologic phenotypes which may be visually indistinguishable on routine anatomic imaging. CoLlAGe seeks to capture and exploit local anisotropic differences in voxel-level gradient orientations to distinguish similar appearing phenotypes. CoLlAGe involves assigning every image voxel an entropy value associated with the co-occurrence matrix of gradient orientations computed around every voxel. The hypothesis behind CoLlAGe is that benign and pathologic phenotypes even though they may appear similar on anatomic imaging, will differ in their local entropy patterns, in turn reflecting subtle local differences in tissue microarchitecture. We demonstrate CoLlAGe's utility in three clinically challenging classification problems: distinguishing (1) radiation necrosis, a benign yet confounding effect of radiation treatment, from recurrent tumors on T1-w MRI in 42 brain tumor patients, (2) different molecular sub-types of breast cancer on DCE-MRI in 65 studies and (3) non-small cell lung cancer (adenocarcinomas) from benign fungal infection (granulomas) on 120 non-contrast CT studies. For each of these classification problems, CoLlAGE in conjunction with a random forest classifier outperformed state of the art radiomic descriptors (Haralick, Gabor, Histogram of Gradient Orientations).},
author = {Prasanna, Prateek and Tiwari, Pallavi and Madabhushi, Anant},
doi = {10.1038/srep37241},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Prasanna2016 - Co-occurrence of local anisotropic gradient orientations.pdf:pdf},
issn = {2045-2322},
journal = {Scientific Reports},
month = {nov},
number = {February},
pages = {37241},
pmid = {27872484},
publisher = {Nature Publishing Group},
title = {{Co-occurrence of Local Anisotropic Gradient Orientations (CoLlAGe): A new radiomics descriptor}},
url = {http://www.nature.com/articles/srep37241 http://www.ncbi.nlm.nih.gov/pubmed/27872484 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5118705},
volume = {6},
year = {2016}
}
@article{He2016,
abstract = {The Effects of contrast-enhancement, reconstruction slice thickness and convolution kernel on the diagnostic performance of radiomics signature in solitary pulmonary nodule (SPN) remains unclear. 240 patients with SPNs (malignant, n = 180; benign, n = 60) underwent non-contrast CT (NECT) and contrast-enhanced CT (CECT) which were reconstructed with different slice thickness and convolution kernel. 150 radiomics features were extracted separately from each set of CT and diagnostic performance of each feature were assessed. After feature selection and radiomics signature construction, diagnostic performance of radiomics signature for discriminating benign and malignant SPN was also assessed with respect to the discrimination and classification and compared with net reclassification improvement (NRI). Our results showed NECT-based radiomics signature demonstrated better discrimination and classification capability than CECT in both primary (AUC: 0.862 vs. 0.829, p = 0.032; NRI = 0.578) and validation cohort (AUC: 0.750 vs. 0.735, p = 0.014; NRI = 0.023). Thin-slice (1.25 mm) CT-based radiomics signature had better diagnostic performance than thick-slice CT (5 mm) in both primary (AUC: 0.862 vs. 0.785, p = 0.015; NRI = 0.867) and validation cohort (AUC: 0.750 vs. 0.725, p = 0.025; NRI = 0.467). Standard convolution kernel-based radiomics signature had better diagnostic performance than lung convolution kernel-based CT in both primary (AUC: 0.785 vs. 0.770, p = 0.015; NRI = 0.156) and validation cohort (AUC: 0.725 vs.0.686, p = 0.039; NRI = 0.467). Therefore, this study indicates that the contrast-enhancement, reconstruction slice thickness and convolution kernel can affect the diagnostic performance of radiomics signature in SPN, of which non-contrast, thin-slice and standard convolution kernel-based CT is more informative.},
author = {He, Lan and Huang, Yanqi and Ma, Zelan and Liang, Cuishan and Liang, Changhong and Liu, Zaiyi},
doi = {10.1038/srep34921},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/He2016 - Effects of contrast-enhancement reconstruction slice thickness and convolution kernel on the diagnostic performance of radiomics signature in solitarty pulmonary nodule.pdf:pdf},
issn = {2045-2322},
journal = {Scientific reports},
month = {oct},
number = {October},
pages = {34921},
pmid = {27721474},
publisher = {Nature Publishing Group},
title = {{Effects of contrast-enhancement, reconstruction slice thickness and convolution kernel on the diagnostic performance of radiomics signature in solitary pulmonary nodule.}},
url = {http://www.nature.com/articles/srep34921 http://www.ncbi.nlm.nih.gov/pubmed/27721474 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5056507},
volume = {6},
year = {2016}
}
@article{Larue2016,
abstract = {Quantitative analysis of tumour characteristics based on medical imaging is an emerging field of research. In recent years quantitative imaging features derived from CT, PET and MR scans were shown to be of added value in the prediction of outcome parameters in oncology, in what is called the radiomics field. However, results might be difficult to compare due to a lack of standardized methodologies to conduct quantitative image analyses. In this review we aim to present an overview of the current challenges, technical routines and protocols that are involved in quantitative imaging studies. The first issue that should be overcome is the dependency of several features on the scan acquisition and image reconstruction parameters. Adopting consistent methods in the subsequent target segmentation step is evenly crucial. To further establish robust quantitative image analyses, standardization or at least calibration of imaging features based on different feature extraction settings is required, especially for texture and filter-based features. Several open-source and commercial software packages to perform feature extraction are currently available, all with slightly different functionalities which makes benchmarking quite challenging. The number of imaging features calculated is typically larger than the number of patients studied, which emphasizes the importance of proper feature selection and prediction model building routines to prevent overfitting. Even though many of these challenges still need to be addressed before quantitative imaging can be brought into daily clinical practice, radiomics is expected to be a critical component for the integration of image-derived information to personalize treatment in the future.},
author = {Larue, Ruben T.H.M. and Defraene, Gilles and {De Ruysscher}, Dirk and Lambin, Philippe and van Elmpt, Wouter J. C.},
doi = {10.1259/bjr.20160665},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Larue2016 - Quantitative radiomics studies for tissue characterization.pdf:pdf},
issn = {0007-1285},
journal = {The British Journal of Radiology},
month = {dec},
pages = {20160665},
title = {{Quantitative radiomics studies for tissue characterization: A review of technology and methodological procedures}},
url = {http://www.birpublications.org/doi/10.1259/bjr.20160665},
year = {2016}
}
@article{Khachiyan1996,
author = {Khachiyan, Leonid G.},
doi = {10.1287/moor.21.2.307},
issn = {0364-765X},
journal = {Mathematics of Operations Research},
number = {2},
pages = {307--320},
title = {{Rounding of Polytopes in the Real Number Model of Computation}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/moor.21.2.307},
volume = {21},
year = {1996}
}
@article{Ahipasaoglu2015,
author = {Ahipa\c{s}ao\u{g}lu, Selin Damla},
doi = {10.1007/s10898-014-0233-8},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Ahipashaoglu2015 - Fast algorithms for the minimum volume estimator.pdf:pdf},
isbn = {0925-5001},
issn = {0925-5001},
journal = {Journal of Global Optimization},
keywords = {Minimum volume estimator,Outlier detection,Robust regression},
month = {jun},
number = {2},
pages = {351--370},
title = {{Fast algorithms for the minimum volume estimator}},
url = {http://link.springer.com/10.1007/s10898-014-0233-8},
volume = {62},
year = {2015}
}
@article{Tee2004,
author = {Tee, Garry J},
file = {:C$\backslash$:/Users/zwanenbal/Downloads/Tee2004 - Surface area and capacity of ellipsoids in n dimensions.pdf:pdf},
institution = {CITR, The University of Auckland, New Zealand},
keywords = {abelian integral,ams subject classification,capacity,ellipsoid,elliptic integral,hyperelliptic integral,legendre,n dimensions,primary 41a63 41a55,secondary 41-,surface area},
title = {{Surface area and capacity of ellipsoids in n dimensions}},
year = {2004}
}
@misc{Weissteina,
author = {Weisstein, Eric W.},
booktitle = {MathWorld},
title = {{Ellipsoid, http://mathworld.wolfram.com/Ellipsoid.html}},
url = {http://mathworld.wolfram.com/Ellipsoid.html},
year = {2016}
}
@article{Lehmer1950,
author = {Lehmer, Derrick H.},
journal = {Canadian journal of mathematics},
number = {3},
pages = {267--282},
title = {{Approximations to the area of an n-dimensonial ellipsoid}},
volume = {2},
year = {1950}
}
@misc{Weisstein,
author = {Weisstein, Eric W.},
booktitle = {MathWorld},
title = {{Oblate spheroid, http://mathworld.wolfram.com/OblateSpheroid.html}},
url = {http://mathworld.wolfram.com/OblateSpheroid.html},
year = {2016}
}
@article{Stelldinger2007,
abstract = {Digitization is not as easy as it looks. If one digitizes a 3D object even with a dense sampling grid, the reconstructed digital object may have topological distortions and, in general, there exists no upper bound for the Hausdorff distance. This explains why so far no algorithm has been known which guarantees topology preservation. However, as we will show, it is possible to repair the obtained digital image in a locally bounded way so that it is homeomorphic and close to the 3D object. The resulting digital object is always well-composed, which has nice implications for a lot of image analysis problems. Moreover, we will show that the surface of the original object is homeomorphic to the result of the marching cubes algorithm. This is really surprising since it means that the well-known topological problems of the marching cubes reconstruction simply do not occur for digital images of r-regular objects. Based on the trilinear interpolation, we also construct a smooth isosurface from the digital image that has the same topology as the original surface. Finally, we give a surprisingly simple topology preserving reconstruction method by using overlapping balls instead of cubical voxels. This is the first approach of digitizing 3D objects which guarantees topology preservation and gives an upper bound for the geometric distortion. Since the output can be chosen as a pure voxel presentation, a union of balls, a reconstruction by trilinear interpolation, a smooth isosurface, or the piecewise linear marching cubes surface, the results are directly applicable to a huge class of image analysis algorithms. Moreover, we show how one can efficiently estimate the volume and the surface area of 3D objects by looking at their digitizations. Measuring volume and surface area of digital objects are important problems in 3D image analysis. Good estimators should be multigrid convergent, i.e., the error goes to zero with increasing sampling density. We will show that every presented reconstruction method can be used for volume estimation and we will give a solution for the much more difficult problem of multigrid-convergent surface area estimation. Our solution is based on simple counting of voxels and we are the first to be able to give absolute bounds for the surface area.},
author = {Stelldinger, Peer and Latecki, Longin Jan and Siqueira, Marcelo},
doi = {10.1109/TPAMI.2007.21},
file = {:C$\backslash$:/Users/zwanenbal/Downloads/Stelldinger2007 - Topological equivalence between a 3D object and the reconstruction of its digital image.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {3D,Digitization,Marching cubes,Topology,Trilinear interpolation,Well-composed,r-regular},
month = {jan},
number = {1},
pages = {126--40},
pmid = {17108388},
title = {{Topological equivalence between a 3D object and the reconstruction of its digital image.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17108388},
volume = {29},
year = {2007}
}
@article{Barequet2001,
abstract = {We present an efficient O(n+1/$\epsilon$4.5-time algorithm for computing a (1+$\epsilon$)-approximation of the minimum-volume bounding box of n points in R3. We also present a simpler algorithm whose running time is O(nlogn+n/$\epsilon$3). We give some experimental results with implementations of various variants of the second algorithm.},
author = {Barequet, Gill and Har-Peled, Sariel},
doi = {10.1006/jagm.2000.1127},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Barequet2001 - Efficiently approximating the minimum-volume bounding box.pdf:pdf},
isbn = {0-89871-434-6},
issn = {01966774},
journal = {Journal of Algorithms},
keywords = {approximation,bounding box},
month = {jan},
number = {1},
pages = {91--109},
title = {{Efficiently Approximating the Minimum-Volume Bounding Box of a Point Set in Three Dimensions}},
url = {http://www.sciencedirect.com/science/article/pii/S0196677400911271 http://linkinghub.elsevier.com/retrieve/pii/S0196677400911271},
volume = {38},
year = {2001}
}
@article{ORourke1985,
abstract = {The problem of finding minimal volume boxes circumscribing a given set of three-dimensional points is investigated. It is shown that it is not necessary for a minimum volume box to have any sides flush with a face of the convex hull of the set of points, which makes a naive search problematic. Nevertheless, it is proven that at least two adjacent box sides are flush with edges of the hull, and this characterization enables an O(n3) algorithm to find all minimal boxes for a set ofn points.},
author = {O'Rourke, Joseph},
doi = {10.1007/BF00991005},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/ORourke1985 - Finding minimal enclosing boxes.pdf:pdf},
isbn = {0600018350},
issn = {0091-7036},
journal = {International Journal of Computer and Information Sciences},
month = {jun},
number = {3},
pages = {183--199},
title = {{Finding minimal enclosing boxes}},
url = {http://link.springer.com/10.1007/BF00991005},
volume = {14},
year = {1985}
}
@article{Chan2001,
abstract = {This paper describes a method for determining the minimum oriented bounding box of an arbitrary solid. The method simplifies a complex three-dimensional problem by projecting the solid onto the three principal planes and makes use of the projected contours for analysis. The orientations of the contours are determined by rotating them within a specific angle range. These orientations are then used to approximate the orientation of the solid so that its bounding box volume is minimized.},
author = {Chan, C.K. and Tan, S.T.},
doi = {10.1016/S0045-7949(01)00046-3},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Chan2001 - Determination of the minimum bounding box of an arbitrary solid.pdf:pdf},
issn = {00457949},
journal = {Computers and Structures},
keywords = {Bounding box,Convex hull,Iterative,Minimum,Packing,Rapid prototyping},
month = {jun},
number = {15},
pages = {1433--1449},
title = {{Determination of the minimum bounding box of an arbitrary solid: an iterative approach}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0045794901000463},
volume = {79},
year = {2001}
}
@article{Georgiou2007,
abstract = {OBJECTIVE A comprehensive signal analysis approach on the mammographic mass boundary morphology is presented in this article. The purpose of this study is to identify efficient sets of simple yet effective shape features, employed in the original and multi-scaled spectral representations of the boundary, for the characterization of the mammographic mass. These new methods of mass boundary representation and processing in more than one domain greatly improve the information content of the base data that is used for pattern classification purposes, introducing comprehensive spectral and multi-scale wavelet versions of the original boundary signals. The evaluation is conducted against morphological and diagnostic characterization of the mass, using statistical methods, fractal dimension analysis and a wide range of classifier architectures. METHODS AND MATERIALS This study consists of (a) the investigation of the original radial distance measurements under the complete spectrum of signal analysis, (b) the application of curve feature extractors of morphological characteristics and the evaluation of the discriminative power of each one of them, by means of statistical significance analysis and dataset fractal dimension, and (c) the application of a wide range of classifier architectures on these morphological datasets, in order to conduct a comparative evaluation of the efficiency and effectiveness of all architectures, for mammographic mass characterization. Radial distance signal was exploited using the discrete Fourier transform (DFT) and the discrete wavelet transform (DWT) as additional carrier signals. Seven uniresolution feature functions were applied over these carrier signals and multiple shape descriptors were created. Classification was conducted against mass shape type and clinical diagnosis, using a wide range of linear and non-linear classifiers, including linear discriminant analysis (LDA), least-squares minimum distance (LSMD), k-nearest neighbor (k-NN), radial basis function (RBF) and multi-layered perceptron (MLP) neural networks (NN), and support vector machines (SVM). Fractal analysis was employed as a dataset analysis tool in the feature selection phase. The discriminative power of the features produced by this composite analysis is subsequently analyzed by means of multivariate analysis of variance (MANOVA) and tested against two distinct classification targets, namely (a) the morphological shape type of the mass and (b) the histologically verified clinical diagnosis for each mammogram. RESULTS Statistical analysis and classification results have shown that the discrimination value of the features extracted from the DWT components and especially the DFT spectrum, are of great importance. Furthermore, much of the information content of the curve features in the case of DFT and DWT datasets is directly related to the texture and fine-scale details of the corresponding envelope signal of the spectral components. Neural classifiers outperformed all other methods (SVM not used because they are mainly two-class classifiers) with overall success rate of 72.3{\%} for shape type identification, while SVM achieved the overall highest 91.54{\%} for clinical diagnosis. Receiver operating characteristic (ROC) analysis has been employed to present the sensitivity and specificity of the results of this study.},
author = {Georgiou, Harris and Mavroforakis, Michael and Dimitropoulos, Nikos and Cavouras, Dionisis and Theodoridis, Sergios},
doi = {10.1016/j.artmed.2007.06.004},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Georgiou2007 - Multi-scaled morphological features for the characterization of mammographic masses.pdf:pdf},
issn = {0933-3657},
journal = {Artificial intelligence in medicine},
keywords = {Fractal dimension,Mammography,Medical diagnostics,Morphological analysis,Multi-scaled analysis},
month = {sep},
number = {1},
pages = {39--55},
pmid = {17714924},
title = {{Multi-scaled morphological features for the characterization of mammographic masses using statistical classification schemes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17714924},
volume = {41},
year = {2007}
}
@inproceedings{Mazurowski2016,
author = {Mazurowski, Maciej A. and Czarnek, Nicholas M. and Collins, Leslie M. and Peters, Katherine B. and Clark, Kal},
booktitle = {SPIE Medical Imaging},
doi = {10.1117/12.2217098},
editor = {Tourassi, Georgia D. and Armato, Samuel G.},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Mazurowski2016 - Predicting outcomes in glioblastoma patients using computerized analysis of the tumor shape.pdf:pdf},
isbn = {9781510600201},
issn = {16057422},
month = {mar},
pages = {97852T},
title = {{Predicting outcomes in glioblastoma patients using computerized analysis of tumor shape: preliminary data}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2217098},
volume = {9785},
year = {2016}
}
@article{Vaidya2012,
author = {Vaidya, Manushka and Creach, Kimberly M. and Frye, Jennifer and Dehdashti, Farrokh and Bradley, Jeffrey D. and {El Naqa}, Issam},
doi = {10.1016/j.radonc.2011.10.014},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Vaidya2012 -Combined PETCT image characteristics for radiotherapy tumor response in lung cancer.pdf:pdf},
isbn = {1879-0887 (Electronic)$\backslash$r0167-8140 (Linking)},
issn = {1879-0887},
journal = {Radiotherapy and oncology},
keywords = {Lung cancer outcomes,Multimodality analysis,PET/CT imaging,Pattern recognition,Radiotherapy},
month = {feb},
number = {2},
pages = {239--45},
pmid = {22098794},
publisher = {Elsevier Ireland Ltd},
title = {{Combined PET/CT image characteristics for radiotherapy tumor response in lung cancer.}},
url = {http://dx.doi.org/10.1016/j.radonc.2011.10.014 http://www.ncbi.nlm.nih.gov/pubmed/22098794},
volume = {102},
year = {2012}
}
@article{ElNaqa2009,
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {{El Naqa}, Issam and Grigsby, Perry W. and Apte, Aditya and Kidd, Elizabeth and Donnelly, Eric and Khullar, Divya and Chaudhari, Summer and Yang, Deshan and Schmitt, Martin and Laforest, Richard and Thorstad, Wade L. and Deasy, Joseph O.},
doi = {10.1016/j.patcog.2008.08.011},
eprint = {NIHMS150003},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/ElNaqa2009 - Exploring feature-based approaches in PET images for predicting cancer treatment outcomes.pdf:pdf},
isbn = {0031-3203},
issn = {0031-3203},
journal = {Pattern recognition},
keywords = {Co-occurrence matrix,Image morphology,Intensity-volume histograms,Positron emission tomography,Treatment outcomes,Tumor heterogeneity,Tumor shape,Uptake values},
month = {jun},
number = {6},
pages = {1162--1171},
pmid = {20161266},
title = {{Exploring feature-based approaches in PET images for predicting cancer treatment outcomes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20161266 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2701316},
volume = {42},
year = {2009}
}
@article{Sled1998,
author = {Sled, John G. and Zijdenbos, Alex P. and Evans, Alan C.},
doi = {10.1109/42.668698},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Sled1998 - A nonparametric method for automatic correction of intensity nonuniformity in MRI data.pdf:pdf},
isbn = {0278-0062 (Print)$\backslash$r0278-0062 (Linking)},
issn = {0278-0062},
journal = {IEEE transactions on medical imaging},
keywords = {Brain,Humans,Magnetic Resonance Imaging,Models,Theoretical},
month = {feb},
number = {1},
pages = {87--97},
pmid = {9617910},
title = {{A nonparametric method for automatic correction of intensity nonuniformity in MRI data.}},
url = {http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed{\&}DbFrom=pubmed{\&}Cmd=Link{\&}LinkName=pubmed{\_}pubmed{\&}LinkReadableName=Related Articles{\&}IdsFromResult=9617910{\&}ordinalpos=3{\&}itool=EntrezSystem2.PEntrez.Pubmed.Pubmed{\_}ResultsPanel.Pubmed{\_}RVDocSum http://www.ncbi.nl},
volume = {17},
year = {1998}
}
@article{Boussion2009,
author = {Boussion, N. and {Le Rest}, Catherine Cheze and Hatt, Mathieu and Visvikis, Dimitris},
doi = {10.1007/s00259-009-1065-5},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Boussion2009 - Partial volume correction PET imaging.pdf:pdf},
isbn = {0025900910655},
issn = {1619-7089},
journal = {European journal of nuclear medicine and molecular imaging},
keywords = {FDG-PET,Image processing,Partial volume correction,Whole-body PET},
month = {jul},
number = {7},
pages = {1064--75},
pmid = {19224209},
title = {{Incorporation of wavelet-based denoising in iterative deconvolution for partial volume correction in whole-body PET imaging.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19224209},
volume = {36},
year = {2009}
}
@article{Soret2007,
author = {Soret, Marine and Bacharach, Stephen L. and Buvat, Ir{\`{e}}ne},
doi = {10.2967/jnumed.106.035774},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Soret2007 - Partial volume effect in PET tumor imaging.pdf:pdf},
isbn = {0161-5505 (Print)$\backslash$r0161-5505 (Linking)},
issn = {0161-5505},
journal = {Journal of nuclear medicine},
keywords = {PET,partial volume,standardized uptake value,tumor imaging},
month = {jun},
number = {6},
pages = {932--45},
pmid = {17504879},
title = {{Partial-volume effect in PET tumor imaging.}},
url = {http://jnm.snmjournals.org/cgi/doi/10.2967/jnumed.106.035774 http://www.ncbi.nlm.nih.gov/pubmed/17504879},
volume = {48},
year = {2007}
}
@article{Gudbjartsson1995,
abstract = {The image intensity in magnetic resonance magnitude images in the presence of noise is shown to be governed by a Rician distribution. Low signal intensities (SNR {\textless} 2) are therefore biased due to the noise. It is shown how the underlying noise can be estimated from the images and a simple correction scheme is provided to reduce the bias. The noise characteristics in phase images are also studied and shown to be very different from those of the magnitude images. Common to both,however, is that the noise distributions are nearly Gaussian for SNR larger than two.},
author = {Gudbjartsson, H{\'{a}}kon and Patz, Samuel},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Gudbjartsson1995 - Rician distribution of noisy MRI data.pdf:pdf},
issn = {0740-3194},
journal = {Magnetic resonance in medicine},
keywords = {a similar correction scheme,data,gaussian,noise,of the noisy magnitude,rayleigh,rician,studied and compared with,the correction scheme are,the statistical properties of,to the rician distribution},
month = {dec},
number = {6},
pages = {910--4},
pmid = {8598820},
title = {{The Rician distribution of noisy MRI data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/2254141 http://www.ncbi.nlm.nih.gov/pubmed/8598820 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2254141},
volume = {34},
year = {1995}
}
@book{Heiberger2015,
address = {New York, NY},
author = {Heiberger, Richard M and Holland, Burt},
doi = {10.1007/978-1-4939-2122-5},
isbn = {978-1-4939-2121-8},
publisher = {Springer New York},
series = {Springer Texts in Statistics},
title = {{Statistical Analysis and Data Display}},
url = {http://link.springer.com/10.1007/978-1-4939-2122-5},
year = {2015}
}
@article{Ioannidis2005,
archivePrefix = {arXiv},
arxivId = {gr-qc/0208024},
author = {Ioannidis, John P. A.},
doi = {10.1371/journal.pmed.0020124},
eprint = {0208024},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Ioannidis2005 - Why most published research findings are false.PDF:PDF},
isbn = {3540239081},
issn = {1549-1676},
journal = {PLoS medicine},
month = {aug},
number = {8},
pages = {e124},
pmid = {16060722},
primaryClass = {gr-qc},
title = {{Why most published research findings are false.}},
url = {http://dx.plos.org/10.1371/journal.pmed.0020124 http://www.ncbi.nlm.nih.gov/pubmed/16060722 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1182327},
volume = {2},
year = {2005}
}
@article{Jiang2014,
author = {Jiang, Bin and Yin, Junjun},
doi = {10.1080/00045608.2013.834239},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Jiang2013 - Ht-Index for Quantifying the Fractal or Scaling Structure of Geographic Features.pdf:pdf},
issn = {0004-5608},
journal = {Annals of the Association of American Geographers},
keywords = {and head,fractal dimension,nested rank-size plots,richardson plot,scaling of geographic space,tail breaks},
month = {may},
number = {3},
pages = {530--540},
title = {{Ht-Index for Quantifying the Fractal or Scaling Structure of Geographic Features}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00045608.2013.834239},
volume = {104},
year = {2014}
}
@article{Otsu1979,
author = {Otsu, Nobuyuki},
journal = {IEEE Transactions on Systems, Man and Cybernetics},
number = {1},
pages = {62--66},
title = {{A threshold selection method from gray-level histograms}},
volume = {20},
year = {1979}
}
@article{Geary1954,
author = {Geary, Roy C.},
doi = {10.2307/2986645},
issn = {14669404},
journal = {The Incorporated Statistician},
month = {nov},
number = {3},
pages = {115--145},
title = {{The Contiguity Ratio and Statistical Mapping}},
url = {http://www.jstor.org/stable/2986645?origin=crossref},
volume = {5},
year = {1954}
}
@article{Lorensen1987,
abstract = {We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.},
author = {Lorensen, William E. and Cline, Harvey E.},
doi = {10.1145/37402.37422},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lorensen, Cline - 1987 - Marching cubes A high resolution 3D surface construction algorithm.pdf:pdf},
isbn = {0897912276},
issn = {00978930},
journal = {ACM SIGGRAPH Computer Graphics},
month = {aug},
number = {4},
pages = {163--169},
title = {{Marching cubes: A high resolution 3D surface construction algorithm}},
url = {http://portal.acm.org/citation.cfm?doid=37401.37422 http://portal.acm.org/citation.cfm?doid=37402.37422},
volume = {21},
year = {1987}
}
@article{Dale2002,
abstract = {A large number of methods for the analysis of the spatial structure of natural phenomena (for example, the clumping or overdispersion of tree stems, the positions of veins of ore in a rock formation, the arrangement of habitat patches in a landscape, and so on) have been developed in a wide range of scientific fields. This paper reviews many of the methods and describes the relationships among them, both mathematically, using the cross-product as a unifying principle, and conceptually, based on the form of a moving window or template used in calculation. The relationships among these methods suggest that while no single method can reveal all the important characteristics of spatial data, the results of different analyses are not expected to be completely independent of each other.},
author = {Dale, Mark R. T. and Dixon, Philip and Fortin, Marie-Jos{\'{e}}e and Legendre, Pierre and Myers, Donald E. and Rosenberg, Michael S.},
doi = {10.1034/j.1600-0587.2002.250506.x},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dale et al. - 2002 - Conceptual and mathematical relationships among methods for spatial analysis.pdf:pdf},
isbn = {1600-0587},
issn = {09067590},
journal = {Ecography},
number = {5},
pages = {558--577},
title = {{Conceptual and mathematical relationships among methods for spatial analysis}},
url = {http://dx.doi.org/10.1034/j.1600-0587.2002.250506.x},
volume = {25},
year = {2002}
}
@article{Moran1950,
author = {Moran, Patrick A. P.},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moran - 1950 - Biometrika Trust Notes on Continuous Stochastic Phenomena Published by Oxford University Press on behalf of Biometrika T.pdf:pdf},
journal = {Biometrika},
pages = {17--23},
title = {{Notes on continuous stochastic phenomena}},
volume = {37},
year = {1950}
}
@article{Lewiner2003,
abstract = {Marching Cubes methods first offered visual access to experimental and theoretical data. This method s implementation usually relies on a small lookup table. Many enhancements and optimizations of this method still use it. However, this lookup table can lead to cracks and inconsistent topology. This paper introduces a full implementation of Chernyaev s Marching Cubes 33 method to ensure a topologically correct result. It completes the original paper for the ambiguity resolution and for the feasibility of the implementation.},
author = {Lewiner, Thomas and Lopes, H{\'{e}}lio and Vieira, Ant{\^{o}}nio Wilson and Tavares, Geovan},
doi = {10.1080/10867651.2003.10487582},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lewiner et al. - 2003 - Efficient Implementation of Marching Cubes' Cases with Topological Guarantees.pdf:pdf},
issn = {1086-7651},
journal = {Journal of Graphics Tools},
month = {jan},
number = {2},
pages = {1--15},
title = {{Efficient Implementation of Marching Cubes' Cases with Topological Guarantees}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10867651.2003.10487582},
volume = {8},
year = {2003}
}
@article{Chalkidou2015,
author = {Chalkidou, Anastasia and O'Doherty, Michael J. and Marsden, Paul K.},
doi = {10.1371/journal.pone.0124165},
editor = {Rubin, Daniel L},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Chalkidou2015 - False Discovery Rates in PET and CT Studies with Texture Features A Systematic Review.pdf:pdf},
isbn = {1932-6203 (Electronic) 1932-6203 (Linking)},
issn = {1932-6203},
journal = {PLOS ONE},
month = {may},
number = {5},
pages = {e0124165},
pmid = {25938522},
title = {{False Discovery Rates in PET and CT Studies with Texture Features: A Systematic Review}},
url = {http://dx.plos.org/10.1371/journal.pone.0124165},
volume = {10},
year = {2015}
}
@article{Lopes2009,
author = {Lopes, R. and Betrouni, N.},
doi = {10.1016/j.media.2009.05.003},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Lopes2009 - Fractal and multifractal analysis.pdf:pdf},
isbn = {1361-8423 (Electronic)$\backslash$n1361-8415 (Linking)},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Characterization,Fractal analysis,Fractal dimension,Multifractal analysis,Multifractal spectrum,Texture},
number = {4},
pages = {634--649},
pmid = {19535282},
publisher = {Elsevier B.V.},
title = {{Fractal and multifractal analysis: A review}},
url = {http://dx.doi.org/10.1016/j.media.2009.05.003},
volume = {13},
year = {2009}
}
@article{Hou1990,
abstract = {A new topological ordering is defined which significantly reduces the time requirements for the fast box counting method proposed in a recent paper by Liebovitch and Toth. Only one sorting is necessary in this algorithm.},
author = {Hou, Xin-Jun and Gilmore, Robert and Mindlin, Gabriel B. and Solari, Hern{\'{a}}n G.},
doi = {10.1016/0375-9601(90)90844-E},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Hou1990 - An efficient algorithm for fast box-counting.pdf:pdf},
issn = {03759601},
journal = {Physics Letters A},
number = {1-2},
pages = {43--46},
title = {{An efficient algorithm for fast box counting}},
volume = {151},
year = {1990}
}
@article{Jimenez2012,
author = {Jim{\'{e}}nez, J. and {Ruiz de Miras}, J.},
doi = {10.1016/j.cmpb.2012.07.005},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Jim{\'{e}}nez2012 - Fast box-counting algorithm on GPU.pdf:pdf},
isbn = {1872-7565 (Electronic)$\backslash$r0169-2607 (Linking)},
issn = {01692607},
journal = {Computer Methods and Programs in Biomedicine},
keywords = {Box counting,CUDA,Fractal dimension,GPU},
number = {3},
pages = {1229--1242},
pmid = {22917763},
publisher = {Elsevier Ireland Ltd},
title = {{Fast box-counting algorithm on GPU}},
url = {http://dx.doi.org/10.1016/j.cmpb.2012.07.005},
volume = {108},
year = {2012}
}
@article{Bisoi2001,
author = {Bisoi, Ajay Kumar and Mishra, Jibitesh},
doi = {10.1016/S0167-8655(00)00132-X},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Bisoi2001 - On calculation of fractal dimension of images.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Box-counting,Fractual dimension,IFS generated fractal images},
number = {6-7},
pages = {631--637},
title = {{On calculation of fractal dimension of images}},
volume = {22},
year = {2001}
}
@article{DaSilva2008,
author = {{Da Silva}, Erick Corr{\^{e}}a and Silva, Arist{\'{o}}fanes Corr{\^{e}}a and {De Paiva}, Anselmo Cardoso and Nunes, Rodolfo Acatauassu},
doi = {10.1007/s10044-007-0081-y},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Corr{\^{e}}a da Silva2008 - Diagnosis of lung nodule using Moran's index and Geary's coefficient.pdf:pdf},
isbn = {1581138121},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {Diagnosis of lung nodule,Geary's coefficient,Moran's index,Texture analysis},
number = {1},
pages = {89--99},
title = {{Diagnosis of lung nodule using Moran's index and Geary's coefficient in computerized tomography images}},
volume = {11},
year = {2008}
}
@article{Miwa2014,
author = {Miwa, Kenta and Inubushi, Masayuki and Wagatsuma, Kei and Nagao, Michinobu and Murata, Taisuke and Koyama, Masamichi and Koizumi, Mitsuru and Sasaki, Masayuki},
doi = {10.1016/j.ejrad.2013.12.020},
issn = {18727727},
journal = {European Journal of Radiology},
keywords = {Diagnostic accuracy,Non-small cell lung cancer,Texture analysis},
number = {4},
pages = {715--719},
pmid = {24418285},
publisher = {Elsevier Ireland Ltd},
title = {{FDG uptake heterogeneity evaluated by fractal analysis improves the differential diagnosis of pulmonary nodules}},
url = {http://dx.doi.org/10.1016/j.ejrad.2013.12.020},
volume = {83},
year = {2014}
}
@article{Smith1996,
author = {Smith, T. G. and Lange, G. D. and Marks, W. B.},
doi = {10.1016/S0165-0270(96)00080-5},
isbn = {0165-0270},
issn = {01650270},
journal = {Journal of Neuroscience Methods},
keywords = {Cell borders,Fractal dimension,Fractal geometry,Lacunarity,Multifractal,Self-similarity},
number = {2},
pages = {123--136},
pmid = {8946315},
title = {{Fractal methods and results in cellular morphology - Dimensions, lacunarity and multifractals}},
volume = {69},
year = {1996}
}
@article{Walk2004,
author = {Walk, Thomas C. and {Van Erp}, Erik and Lynch, Jonathan P.},
doi = {10.1093/aob/mch116},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Walk2004 - Modelling applicability of fractal analysis to efficiency of soil exploration by roots.pdf:pdf},
isbn = {0305-7364},
issn = {03057364},
journal = {Annals of Botany},
keywords = {Competition,Depletion,Fractal abundance,Fractal dimension,Lacunarity,Phaseolus vulgaris,Phosphorus,SimRoot,Simulation modelling},
number = {1},
pages = {119--128},
pmid = {15145791},
title = {{Modelling applicability of fractal analysis to efficiency of soil exploration by roots}},
volume = {94},
year = {2004}
}
@article{Wahl2009,
author = {Wahl, Richard L. and Jacene, Heather and Kasamon, Yvette and Lodge, Martin A.},
doi = {10.2967/jnumed.108.057307},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Wahl2009 - From RECIST to PERCIST.pdf:pdf},
isbn = {0161-5505 (Print)$\backslash$n0161-5505 (Linking)},
issn = {0161-5505},
journal = {Journal of nuclear medicine},
keywords = {Fluorodeoxyglucose F18,Fluorodeoxyglucose F18: diagnostic use,Forecasting,Humans,Internationality,Neoplasms,Neoplasms: diagnosis,Neoplasms: therapy,Outcome Assessment (Health Care),Outcome Assessment (Health Care): standards,Positron-Emission Tomography,Positron-Emission Tomography: standards,Practice Guidelines as Topic,Prognosis,Radiopharmaceuticals,Radiopharmaceuticals: diagnostic use,Treatment Outcome,World Health Organization},
month = {may},
number = {5},
pages = {122S--50S},
pmid = {19403881},
title = {{From RECIST to PERCIST: Evolving Considerations for PET response criteria in solid tumors.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2755245{\&}tool=pmcentrez{\&}rendertype=abstract http://www.ncbi.nlm.nih.gov/pubmed/19403881 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2755245},
volume = {50 Suppl 1},
year = {2009}
}
@article{VanVelden2011,
author = {van Velden, Floris H. P. and Cheebsumon, Patsuree and Yaqub, Maqsood and Smit, Egbert F. and Hoekstra, Otto S. and Lammertsma, Adriaan A. and Boellaard, Ronald},
doi = {10.1007/s00259-011-1845-6},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/VanVelden2011 - Cumulative SUV volume histograms.pdf:pdf},
isbn = {1619-7070},
issn = {1619-7089},
journal = {European journal of nuclear medicine and molecular imaging},
keywords = {Cumulative SUV-volume histogram (CSH),Intensity-volume histograms (IVH),Intratumoural heterogeneity,Positron emission tomography (PET),Standardized uptake value (SUV)},
month = {sep},
number = {9},
pages = {1636--47},
pmid = {21617975},
title = {{Evaluation of a cumulative SUV-volume histogram method for parameterizing heterogeneous intratumoural FDG uptake in non-small cell lung cancer PET studies.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21617975 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3151405},
volume = {38},
year = {2011}
}
@article{Janmahasatian2005,
author = {Janmahasatian, Sarayut and Duffull, Stephen B. and Ash, Susan and Ward, Leigh C. and Byrne, Nuala M. and Green, Bruce},
doi = {10.2165/00003088-200544100-00004},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Janmahasatian2005 - Quantification of lean bodyweight.pdf:pdf},
issn = {0312-5963},
journal = {Clinical pharmacokinetics},
number = {10},
pages = {1051--65},
pmid = {16176118},
title = {{Quantification of lean bodyweight.}},
url = {http://link.springer.com/10.2165/00003088-200544100-00004 http://www.ncbi.nlm.nih.gov/pubmed/16176118},
volume = {44},
year = {2005}
}
@article{vanVelden2016,
author = {van Velden, Floris H. P. and Kramer, Gerbrand M. and Frings, Virginie and Nissen, Ida A. and Mulder, Emma R. and de Langen, Adrianus J. and Hoekstra, Otto S. and Smit, Egbert F. and Boellaard, Ronald},
doi = {10.1007/s11307-016-0940-2},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/VanVelden2016 - Repeatability of radiomic features in non-small-cell lung cancer 18F FDG-PET CT studies.pdf:pdf},
issn = {1860-2002},
journal = {Molecular imaging and biology},
keywords = {Non-small-cell lung cancer (NSCLC),PET/CT,Radiomics,Repeatability,Tracer uptake heterogeneity},
month = {oct},
number = {5},
pages = {788--95},
pmid = {26920355},
publisher = {Molecular Imaging and Biology},
title = {{Repeatability of Radiomic Features in Non-Small-Cell Lung Cancer [(18)F]FDG-PET/CT Studies: Impact of Reconstruction and Delineation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26920355},
volume = {18},
year = {2016}
}
@article{Boellaard2010,
author = {Boellaard, Ronald and O'Doherty, Mike J. and Weber, Wolfgang A. and Mottaghy, Felix M. and Lonsdale, Markus N. and Stroobants, Sigrid G. and Oyen, Wim J. G. and Kotzerke, Joerg and Hoekstra, Otto S. and Pruim, Jan and Marsden, Paul K. and Tatsch, Klaus and Hoekstra, Corneline J. and Visser, Eric P. and Arends, Bertjan and Verzijlbergen, Fred J. and Zijlstra, Josee M. and Comans, Emile F. I. and Lammertsma, Adriaan A. and Paans, Anne M. and Willemsen, Antoon T. and Beyer, Thomas and Bockisch, Andreas and Schaefer-Prokop, Cornelia and Delbeke, Dominique and Baum, Richard P. and Chiti, Arturo and Krause, Bernd J.},
doi = {10.1007/s00259-009-1297-4},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Boellaard2010 - FDG PETCT EANM procedure guidelines for tumour imaging version 1.pdf:pdf},
isbn = {1619-7070$\backslash$r1619-7089},
issn = {1619-7070},
journal = {European Journal of Nuclear Medicine and Molecular Imaging},
keywords = {FDG,Imaging procedure,Oncology,PET/CT,Quantification,Tumour},
month = {jan},
number = {1},
pages = {181--200},
pmid = {25452219},
title = {{FDG PET and PET/CT: EANM procedure guidelines for tumour PET imaging: version 1.0}},
url = {http://link.springer.com/10.1007/s00259-009-1297-4},
volume = {37},
year = {2010}
}
@article{Boellaard2015,
author = {Boellaard, Ronald and Delgado-Bolton, Roberto and Oyen, Wim J. G. and Giammarile, Francesco and Tatsch, Klaus and Eschner, Wolfgang and Verzijlbergen, Fred J. and Barrington, Sally F. and Pike, Lucy C. and Weber, Wolfgang A. and Stroobants, Sigrid G. and Delbeke, Dominique and Donohoe, Kevin J. and Holbrook, Scott and Graham, Michael M. and Testanera, Giorgio and Hoekstra, Otto S. and Zijlstra, Josee M. and Visser, Eric P. and Hoekstra, Corneline J. and Pruim, Jan and Willemsen, Antoon T. and Arends, Bertjan and Kotzerke, J{\"{o}}rg and Bockisch, Andreas and Beyer, Thomas and Chiti, Arturo and Krause, Bernd J.},
doi = {10.1007/s00259-014-2961-x},
isbn = {1619-7070$\backslash$r1619-7089},
issn = {1619-7089},
journal = {European journal of nuclear medicine and molecular imaging},
keywords = {FDG,Imaging procedure,Oncology,PET/CT,Quantification,Tumour},
month = {feb},
number = {2},
pages = {328--54},
pmid = {25452219},
title = {{FDG PET/CT: EANM procedure guidelines for tumour imaging: version 2.0.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25452219 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4315529},
volume = {42},
year = {2015}
}
@article{Max1960,
author = {Max, Joel},
doi = {10.1109/TIT.1960.1057548},
isbn = {0096-1000},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {1},
pages = {7--12},
title = {{Quantizing for minimum distortion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1057548},
volume = {6},
year = {1960}
}
@article{Hall1971,
abstract = {Feature extraction is one of the more difficult steps in image pattern recognition. Some sources of difficulty are the presence of irrelevant information and the relativity of a feature set to a particular application. Several preprocessing techniques for enhancing selected features and removing irrelevant data are described and compared. The techniques include gray level distribution linearization, digital spatial filtering, contrast enhancement, and image subtraction. Also, several feature extraction techniques are illustrated. The techniques are divided into spatial and Fourier domain operations. The spatial domain operations of directional signatures and contour tracing are first described. Then, the Fourier domain techniques of frequency signatures and template matching are illustrated. Finally, a practical image pattern recognition problem is solved using some of the described techniques.},
author = {Hall, Ernest L. and Kruger, Richard P. and Samuel, J. and Dwyer, D. and McLaren, Robert W. and Hall, David L. and Lodwick, Gwilyms},
doi = {10.1109/T-C.1971.223399},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Hall1971 - A Survey of Preprocessing and Fearure Extration.pdf:pdf},
isbn = {0018-9340},
issn = {00189340},
journal = {IEEE Transactions on Computers},
keywords = {Feature extraction,pattern recognition,preprocessing,signatures,spatial filtering,template matching},
number = {9},
pages = {1032--1044},
title = {{A Survey of Preprocessing and Feature Extraction Techniques for Radiographic Images}},
volume = {C-20},
year = {1971}
}
@article{Lloyd1982,
abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for quanta, , are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
author = {Lloyd, Stuart P.},
doi = {10.1109/TIT.1982.1056489},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Lloyd1982 - Least squares quantization in PCM.pdf:pdf},
isbn = {00189448 (ISSN)},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
number = {2},
pages = {129--137},
title = {{Least Squares Quantization in PCM}},
volume = {28},
year = {1982}
}
@article{Wei2015,
abstract = {Abstract Measuring variable importance for computational models or measured data is an important task in many applications. It has drawn our attention that the variable importance analysis (VIA) techniques were developed independently in many disciplines. We are strongly aware of the necessity to aggregate all the good practices in each discipline, and compare the relative merits of each method, so as to instruct the practitioners to choose the optimal methods to meet different analysis purposes, and to guide current research on VIA. To this end, all the good practices, including seven groups of methods, i.e., the difference-based variable importance measures (VIMs), parametric regression and related VIMs, nonparametric regression techniques, hypothesis test techniques, variance-based VIMs, moment-independent VIMs and graphic VIMs, are reviewed and compared with a numerical test example set in two situations (independent and dependent cases). For ease of use, the recommendations are provided for different types of applications, and packages as well as software for implementing these VIA techniques are collected. Prospects for future study of VIA techniques are also proposed.},
author = {Wei, Pengfei and Lu, Zhenzhou and Song, Jingwen},
doi = {10.1016/j.ress.2015.05.018},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Wei2015 - Variable importance analysis - A comprehensivereview.pdf:pdf},
isbn = {0951-8320},
issn = {09518320},
journal = {Reliability Engineering {\&} System Safety},
keywords = {Difference-based,Graphic variable importance measures,Moment-independent,Random forest,Regression technique,Variable importance analysis,Variance-based},
month = {oct},
pages = {399--432},
title = {{Variable importance analysis: A comprehensive review}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0951832015001672},
volume = {142},
year = {2015}
}
@article{Collewet2004,
abstract = {Texture analysis methods quantify the spatial variations in gray level values within an image and thus can provide useful information on the structures observed. However, they are sensitive to acquisition conditions due to the use of different protocols and to intra- and interscanner variations in the case of MRI. The influence was studied of two protocols and four different conditions of normalization of gray levels on the discrimination power of texture analysis methods applied to soft cheeses. Thirty-two samples of soft cheese were chosen at two different ripening periods (16 young and 16 old samples) in order to obtain two different microscopic structures of the protein gel. Proton density and T(2)-weighted MR images were acquired using a spin echo sequence on a 0.2 T scanner. Gray levels were normalized according to four methods: original gray levels, same maximum for all images, same mean for all images, and dynamics limited to micro +/- 3sigma. Regions of interest were automatically defined, and texture descriptors were then computed for the co-occurrence matrix, run length matrix, gradient matrix, autoregressive model, and wavelet transform. The features with the lowest probability of error and average correlation coefficient were selected and used for classification with 1-nearest neighbor (1-NN) classifier. The best results were obtained when using the limitation of dynamics to micro +/- 3sigma, which enhanced the differences between the two classes. The results demonstrated the influence of the normalization method and of the acquisition protocol on the effectiveness of the classification and also on the parameters selected for classification. These results indicate the need to evaluate sensitivity to MR acquisition protocols and to gray level normalization methods when texture analysis is required.},
author = {Collewet, G. and Strzelecki, M. and Mariette, F.},
doi = {10.1016/j.mri.2003.09.001},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Collewet2004 - Influence of MRI acquisition protocols and imagine intensity normalization methods on texture classification.pdf:pdf},
isbn = {0730725X (ISSN)},
issn = {0730-725X},
journal = {Magnetic resonance imaging},
keywords = {Classification,Gray level normalization,Texture analysis},
month = {jan},
number = {1},
pages = {81--91},
pmid = {14972397},
title = {{Influence of MRI acquisition protocols and image intensity normalization methods on texture classification.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14972397},
volume = {22},
year = {2004}
}
@article{ElNaqa2016,
author = {{El Naqa}, Issam},
doi = {10.1016/j.ymeth.2016.08.010},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/ElNaqa2016 - Perspectives on making big data analytics work for oncology.pdf:pdf},
issn = {10462023},
journal = {Methods},
month = {aug},
publisher = {Elsevier Inc.},
title = {{Perspectives on Making Big Data Analytics Work for Oncology}},
url = {http://dx.doi.org/10.1016/j.ymeth.2016.08.010 http://linkinghub.elsevier.com/retrieve/pii/S1046202316302651},
year = {2016}
}
@incollection{Frank2006,
author = {Frank, Eibe and Pfahringer, Bernhard},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/11731139_14},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Frank2006 - Improving on Bagging with Input Smearing.pdf:pdf},
isbn = {3540332065},
issn = {03029743},
pages = {97--106},
title = {{Improving on Bagging with Input Smearing}},
url = {http://link.springer.com/10.1007/11731139{\_}14},
volume = {3918 LNAI},
year = {2006}
}
@article{Mendes-Moreira2012,
abstract = {The goal of ensemble regression is to combine several models in order to improve the prediction accuracy in learning problems with a numerical target variable. The process of ensemble learning can be divided into three phases: the generation phase, the pruning phase, and the integration phase.We discuss different approaches to each of these phases that are able to deal with the regression problem, categorizing them in terms of their relevant characteristics and linking them to contributions from different fields. Furthermore, this work makes it possible to identify interesting areas for future research.},
author = {Mendes-Moreira, Jo{\~{a}}o and Soares, Carlos and Jorge, Al{\'{i}}pio M{\'{a}}rio and Sousa, Jorge Freire De},
doi = {10.1145/2379776.2379786},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Mendes-Moreira2012 - Ensemble approaches for regression.pdf:pdf},
isbn = {3512250815},
issn = {03600300},
journal = {ACM Computing Surveys},
keywords = {ensembles,regression,supervised learning},
month = {nov},
number = {1},
pages = {1--40},
title = {{Ensemble approaches for regression}},
url = {http://dl.acm.org/citation.cfm?doid=2379776.2379786},
volume = {45},
year = {2012}
}
@article{Boulesteix2009,
abstract = {Ranked gene lists are highly instable in the sense that similar measures of differential gene expression may yield very different rankings, and that a small change of the data set usually affects the obtained gene list considerably. Stability issues have long been under-considered in the literature, but they have grown to a hot topic in the last few years, perhaps as a consequence of the increasing skepticism on the reproducibility and clinical applicability of molecular research findings. In this article, we review existing approaches for the assessment of stability of ranked gene lists and the related problem of aggregation, give some practical recommendations, and warn against potential misuse of these methods. This overview is illustrated through an application to a recent leukemia data set using the freely available Bioconductor package GeneSelector.},
author = {Boulesteix, Anne-Laure and Slawski, Martin},
doi = {10.1093/bib/bbp034},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Boulesteix2009 - Stability and aggregation of ranked gene lists.pdf:pdf},
isbn = {1477-4054 (Electronic)$\backslash$r1467-5463 (Linking)},
issn = {1477-4054},
journal = {Briefings in bioinformatics},
keywords = {Bootstrap,Differential expression,Ranking,Top-list,Univariate analysis,Variability},
month = {sep},
number = {5},
pages = {556--68},
pmid = {19679825},
title = {{Stability and aggregation of ranked gene lists.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19679825},
volume = {10},
year = {2009}
}
@article{Breiman2000,
abstract = {Bagging and boosting reduce error by changing both the inputs and outputs to form perturbed training sets, growing predictors on these perturbed training sets and combining them. An interesting question is whether it is possible to get comparable performance by perturbing the outputs alone. Two methods of randomizing outputs are experimented with. One is called output smearing and the other output flipping. Both are shown to consistently do better than bagging.},
author = {Breiman, Leo},
doi = {10.1023/A:1007682208299},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Breiman2000 - Randomizing outputs to increase prediction accuracy.pdf:pdf},
isbn = {Technical Report No. 518},
issn = {08856125},
journal = {Machine Learning},
keywords = {ensemble,output variability,randomization},
number = {3},
pages = {229--242},
title = {{Randomizing Outputs to Increase Prediction Accuracy}},
url = {http://link.springer.com/10.1023/A:1007682208299},
volume = {40},
year = {2000}
}
@article{Chouldechova2015,
abstract = {We introduce GAMSEL (Generalized Additive Model Selection), a penalized likelihood approach for fitting sparse generalized additive models in high dimension. Our method interpolates between null, linear and additive models by allowing the effect of each variable to be estimated as being either zero, linear, or a low-complexity curve, as determined by the data. We present a blockwise coordinate descent procedure for efficiently optimizing the penalized likelihood objective over a dense grid of the tuning parameter, producing a regularization path of additive models. We demonstrate the performance of our method on both real and simulated data examples, and compare it with existing techniques for additive model selection.},
archivePrefix = {arXiv},
arxivId = {1506.03850},
author = {Chouldechova, Alexandra and Hastie, Trevor},
eprint = {1506.03850},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Chouldechova2015 - Generalized additive model selection.pdf:pdf},
month = {jun},
pages = {1--24},
title = {{Generalized Additive Model Selection}},
url = {http://arxiv.org/abs/1506.03850},
year = {2015}
}
@incollection{Agresti2007,
author = {Agresti, Alan},
booktitle = {An Introduction to Categorical Data Analysis, Second Edition},
chapter = {Logistic R},
doi = {10.4135/9781412984805},
edition = {2},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Agresti2006 - An Introduction to Categorical Data Analysis 2ED - Ch. 4.pdf:pdf},
isbn = {9780761920106},
pages = {99--136},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Logistic Regression}},
year = {2007}
}
@article{VanderSchaaf2012,
abstract = {PURPOSE Multivariate modeling of complications after radiotherapy is frequently used in conjunction with data driven variable selection. This study quantifies the risk of overfitting in a data driven modeling method using bootstrapping for data with typical clinical characteristics, and estimates the minimum amount of data needed to obtain models with relatively high predictive power. MATERIALS AND METHODS To facilitate repeated modeling and cross-validation with independent datasets for the assessment of true predictive power, a method was developed to generate simulated data with statistical properties similar to real clinical data sets. Characteristics of three clinical data sets from radiotherapy treatment of head and neck cancer patients were used to simulate data with set sizes between 50 and 1000 patients. A logistic regression method using bootstrapping and forward variable selection was used for complication modeling, resulting for each simulated data set in a selected number of variables and an estimated predictive power. The true optimal number of variables and true predictive power were calculated using cross-validation with very large independent data sets. RESULTS For all simulated data set sizes the number of variables selected by the bootstrapping method was on average close to the true optimal number of variables, but showed considerable spread. Bootstrapping is more accurate in selecting the optimal number of variables than the AIC and BIC alternatives, but this did not translate into a significant difference of the true predictive power. The true predictive power asymptotically converged toward a maximum predictive power for large data sets, and the estimated predictive power converged toward the true predictive power. More than half of the potential predictive power is gained after approximately 200 samples. Our simulations demonstrated severe overfitting (a predicative power lower than that of predicting 50{\%} probability) in a number of small data sets, in particular in data sets with a low number of events (median: 7, 95th percentile: 32). Recognizing overfitting from an inverted sign of the estimated model coefficients has a limited discriminative value. CONCLUSIONS Despite considerable spread around the optimal number of selected variables, the bootstrapping method is efficient and accurate for sufficiently large data sets, and guards against overfitting for all simulated cases with the exception of some data sets with a particularly low number of events. An appropriate minimum data set size to obtain a model with high predictive power is approximately 200 patients and more than 32 events. With fewer data samples the true predictive power decreases rapidly, and for larger data set sizes the benefit levels off toward an asymptotic maximum predictive power.},
author = {van der Schaaf, Arjen and Xu, Cheng-Jian and van Luijk, Peter and van't Veld, Aart A. and Langendijk, Johannes A. and Schilstra, Cornelis},
doi = {10.1016/j.radonc.2011.12.006},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/VanDerSchaaf2012 - Multivariate modeling of complications with data driven variable selection - guarding against overfitting and effects of data set size.pdf:pdf},
issn = {1879-0887},
journal = {Radiotherapy and oncology},
keywords = {Complication,Multivariate,NTCP modeling,Predictive power,Risk,Variable selection},
month = {oct},
number = {1},
pages = {115--21},
pmid = {22264894},
publisher = {Elsevier Ireland Ltd},
title = {{Multivariate modeling of complications with data driven variable selection: guarding against overfitting and effects of data set size.}},
url = {http://dx.doi.org/10.1016/j.radonc.2011.12.006 http://www.ncbi.nlm.nih.gov/pubmed/22264894},
volume = {105},
year = {2012}
}
@article{Leijenaar2015a,
abstract = {FDG-PET-derived textural features describing intra-tumor heterogeneity are increasingly investigated as imaging biomarkers. As part of the process of quantifying heterogeneity, image intensities (SUVs) are typically resampled into a reduced number of discrete bins. We focused on the implications of the manner in which this discretization is implemented. Two methods were evaluated: (1) R(D), dividing the SUV range into D equally spaced bins, where the intensity resolution (i.e. bin size) varies per image; and (2) R(B), maintaining a constant intensity resolution B. Clinical feasibility was assessed on 35 lung cancer patients, imaged before and in the second week of radiotherapy. Forty-four textural features were determined for different D and B for both imaging time points. Feature values depended on the intensity resolution and out of both assessed methods, R(B) was shown to allow for a meaningful inter- and intra-patient comparison of feature values. Overall, patients ranked differently according to feature values–which was used as a surrogate for textural feature interpretation–between both discretization methods. Our study shows that the manner of SUV discretization has a crucial effect on the resulting textural features and the interpretation thereof, emphasizing the importance of standardized methodology in tumor texture analysis.},
author = {Leijenaar, Ralph T. H. and Nalbantov, Georgi and Carvalho, Sara and van Elmpt, Wouter J. C. and Troost, Esther G. C. and Boellaard, Ronald and Aerts, Hugo J. W. L. and Gillies, Robert J. and Lambin, Philippe},
doi = {10.1038/srep11075},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Leijenaar2015 - The effect of SUV discretization quantitative FDG-PET radiomics.pdf:pdf},
isbn = {doi:10.1038/srep11075},
issn = {2045-2322},
journal = {Scientific reports},
number = {August},
pages = {11075},
pmid = {26242464},
publisher = {Nature Publishing Group},
title = {{The effect of SUV discretization in quantitative FDG-PET Radiomics: the need for standardized methodology in tumor texture analysis.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4525145{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {5},
year = {2015}
}
@article{Clark2013,
abstract = {The National Institutes of Health have placed significant emphasis on sharing of research data to support secondary research. Investigators have been encouraged to publish their clinical and imaging data as part of fulfilling their grant obligations. Realizing it was not sufficient to merely ask investigators to publish their collection of imaging and clinical data, the National Cancer Institute (NCI) created the open source National Biomedical Image Archive software package as a mechanism for centralized hosting of cancer related imaging. NCI has contracted with Washington University in Saint Louis to create The Cancer Imaging Archive (TCIA)-an open-source, open-access information resource to support research, development, and educational initiatives utilizing advanced medical imaging of cancer. In its first year of operation, TCIA accumulated 23 collections (3.3 million images). Operating and maintaining a high-availability image archive is a complex challenge involving varied archive-specific resources and driven by the needs of both image submitters and image consumers. Quality archives of any type (traditional library, PubMed, refereed journals) require management and customer service. This paper describes the management tasks and user support model for TCIA.},
author = {Clark, Kenneth and Vendt, Bruce and Smith, Kirk and Freymann, John and Kirby, Justin and Koppel, Paul and Moore, Stephen and Phillips, Stanley and Maffitt, David and Pringle, Michael and Tarbox, Lawrence and Prior, Fred},
doi = {10.1007/s10278-013-9622-7},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clark et al. - 2013 - The Cancer Imaging Archive (TCIA) maintaining and operating a public information repository.pdf:pdf},
isbn = {1618-727X (Electronic)$\backslash$r0897-1889 (Linking)},
issn = {1618-727X},
journal = {Journal of digital imaging},
keywords = {Biomedical image analysis,Cancer detection,Cancer imaging,Image archive,NBIA,TCIA},
month = {dec},
number = {6},
pages = {1045--57},
pmid = {23884657},
title = {{The Cancer Imaging Archive (TCIA): maintaining and operating a public information repository.}},
url = {http://link.springer.com/10.1007/s10278-013-9622-7 http://www.ncbi.nlm.nih.gov/pubmed/23884657 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3824915},
volume = {26},
year = {2013}
}
@article{Yip2016,
abstract = {Radiomics is an emerging field in quantitative imaging that uses advanced imaging features to objectively and quantitatively describe tumour phenotypes. Radiomic features have recently drawn considerable interest due to its potential predictive power for treatment outcomes and cancer genetics, which may have important applications in personalized medicine. In this technical review, we describe applications and challenges of the radiomic field. We will review radiomic application areas and technical issues, as well as proper practices for the designs of radiomic studies.},
author = {Yip, Stephen S. F. and Aerts, Hugo J. W. L.},
doi = {10.1088/0031-9155/61/13/R150},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yip, Aerts - 2016 - Applications and limitations of radiomics.pdf:pdf},
issn = {1361-6560},
journal = {Physics in medicine and biology},
month = {jul},
number = {13},
pages = {R150--66},
pmid = {27269645},
publisher = {IOP Publishing},
title = {{Applications and limitations of radiomics.}},
url = {http://stacks.iop.org/0031-9155/61/i=13/a=R150?key=crossref.134478778713970aff90f16abe110608 http://www.ncbi.nlm.nih.gov/pubmed/27269645 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4927328},
volume = {61},
year = {2016}
}
@article{Yan2015,
abstract = {UNLABELLED Evaluation of tumor heterogeneity based on texture parameters has recently attracted much interest in the PET imaging community. However, the impact of reconstruction settings on texture parameters is unclear, especially relating to time-of-flight and point-spread function modeling. Their effects on 55 texture features (TFs) and 6 features based on first-order statistics (FOS) were investigated. Standardized uptake value (SUV) measures were also evaluated as peak SUV (SUVpeak), maximum SUV, and mean SUV (SUVmean). METHODS This study retrospectively recruited 20 patients with lesions in the lung who underwent whole-body (18)F-FDG PET/CT. The coefficient of variation (COV) of each feature across different reconstructions was calculated. RESULTS SUVpeak, SUVmean, 18 TFs, and 1 FOS were the most robust (COV ≤ 5{\%}) whereas skewness, cluster shade, and zone percentage were the least robust (COV {\textgreater} 20{\%}) with respect to reconstruction algorithms using default settings. Heterogeneity parameters had different sensitivities to iteration number. Twenty-four parameters including SUVpeak and SUVmean exhibited variation with a COV less than 5{\%}; 28 parameters, including maximum SUV, showed variation with a COV in the range of 5{\%}-10{\%}. In addition, skewness, cluster shade, and zone percentage were the most sensitive to iteration number. In terms of sensitivity to full width at half maximum (FWHM), 15 TFs and 1 FOS had the best performance with a COV less than 5{\%}, whereas SUVpeak and SUVmean had a COV between 5{\%} and 10{\%}. Grid size had the largest impact on image features, which was demonstrated by only 11 features, including SUVpeak and SUVmean, having a COV less than 10{\%}. CONCLUSION Different image features have different sensitivities to reconstruction settings. Iteration number and FWHM of the gaussian filter have a similar impact on the image features. Grid size has a larger impact on the features than iteration number and FWHM. The features that exhibited large variations such as skewness in FOS, cluster shade, and zone percentage should be used with caution. The entropy in FOS, difference entropy, inverse difference normalized, inverse difference moment normalized, low gray-level run emphasis, high gray-level run emphasis, and low gray-level zone emphasis are the most robust features.},
author = {Yan, Jianhua and Chu-Shern, Jason Lim and Loi, Hoi Yin and Khor, Lih Kin and Sinha, Arvind K. and Quek, Swee Tian and Tham, Ivan W. K. and Townsend, David},
doi = {10.2967/jnumed.115.156927},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yan et al. - 2015 - Impact of Image Reconstruction Settings on Texture Features in 18F-FDG PET.pdf:pdf},
issn = {1535-5667},
journal = {Journal of nuclear medicine},
keywords = {18F-FDG PET,point-spread function,time-of-flight,tumor texture},
month = {nov},
number = {11},
pages = {1667--73},
pmid = {26229145},
title = {{Impact of Image Reconstruction Settings on Texture Features in 18F-FDG PET.}},
url = {http://jnm.snmjournals.org/cgi/doi/10.2967/jnumed.115.156927{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/26229145 http://www.ncbi.nlm.nih.gov/pubmed/26229145},
volume = {56},
year = {2015}
}
@article{VanDijk2016,
author = {van Dijk, Lisanne V. and Brouwer, Charlotte L. and van der Schaaf, Arjen and Burgerhof, Johannes G.M. and Beukinga, Roelof J. and Langendijk, Johannes A. and Sijtsema, Nanna M. and Steenbakkers, Roel J.H.M.},
doi = {10.1016/j.radonc.2016.07.007},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/van Dijk et al. - 2016 - CT image biomarkers to improve patient-specific prediction of radiation-induced xerostomia and sticky saliva.pdf:pdf},
issn = {01678140},
journal = {Radiotherapy and Oncology},
month = {feb},
number = {2},
pages = {185--191},
publisher = {The Authors},
title = {{CT image biomarkers to improve patient-specific prediction of radiation-induced xerostomia and sticky saliva}},
url = {http://dx.doi.org/10.1016/j.radonc.2016.07.007 http://linkinghub.elsevier.com/retrieve/pii/S0167814016311999},
volume = {122},
year = {2017}
}
@article{Eklund2016,
abstract = {The most widely used task functional magnetic resonance imaging (fMRI) analyses use parametric statistical methods that depend on a variety of assumptions. In this work, we use real resting-state data and a total of 3 million random task group analyses to compute empirical familywise error rates for the fMRI software packages SPM, FSL, and AFNI, as well as a nonparametric permutation method. For a nominal familywise error rate of 5{\%}, the parametric statistical methods are shown to be conservative for voxelwise inference and invalid for clusterwise inference. Our results suggest that the principal cause of the invalid cluster inferences is spatial autocorrelation functions that do not follow the assumed Gaussian shape. By comparison, the nonparametric permutation test is found to produce nominal results for voxelwise as well as clusterwise inference. These findings speak to the need of validating the statistical methods being used in the field of neuroimaging.},
archivePrefix = {arXiv},
arxivId = {1511.01863},
author = {Eklund, Anders and Nichols, Thomas E. and Knutsson, Hans},
doi = {10.1073/pnas.1602413113},
eprint = {1511.01863},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eklund, Nichols, Knutsson - 2016 - Cluster failure Why fMRI inferences for spatial extent have inflated false-positive rates.pdf:pdf},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
month = {jul},
number = {28},
pages = {7900--7905},
pmid = {27357684},
title = {{Cluster failure: Why fMRI inferences for spatial extent have inflated false-positive rates}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1602413113},
volume = {113},
year = {2016}
}
@article{Carneiro2016,
abstract = {We propose new methods for the prediction of 5-year mortality in elderly individuals using chest computed tomography (CT). The methods consist of a classifi?er that performs this prediction using a set of features extracted from the CT image and segmentation maps of multiple anatomic structures. We explore two approaches: 1) a uni?ed framework based on deep learning, where features and classifier are automatically learned in a single optimisation process; and 2) a multi-stage framework based on the design and selection/extraction of hand-crafted radiomics features, followed by the classifier learning process. Experimental results, based on a dataset of 48 annotated chest CTs, show that the deep learning model produces a mean 5-year mortality prediction accuracy of 68.5{\%}, while radiomics produces a mean accuracy that varies between 56{\%} to 66{\%} (depending on the feature selection/extraction method and classifier). The successful development of the proposed models has the potential to make a profound impact in preventive and personalised healthcare.},
archivePrefix = {arXiv},
arxivId = {1607.00267},
author = {Carneiro, Gustavo and Oakden-Rayner, Luke and Bradley, Andrew P. and Nascimento, Jacinto and Palmer, Lyle},
eprint = {1607.00267},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carneiro et al. - 2016 - Automated 5-year Mortality Prediction using Deep Learning and Radiomics Features from Chest Computed Tomography.pdf:pdf},
keywords = {computed tomography,deep learning,feature learning,features,five-year mortality,hand-designed,radiomics},
month = {jul},
pages = {1--9},
title = {{Automated 5-year Mortality Prediction using Deep Learning and Radiomics Features from Chest Computed Tomography}},
url = {http://arxiv.org/abs/1607.00267},
year = {2016}
}
@incollection{Ojala2000,
abstract = {This paper presents a theoretically very simple yet efficient approach for gray scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The proposed approach is very robust in terms of gray scale variations, since the operators are by definition invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity, as the operators can be realized with a few operations in a small neighborhood and a lookup table. Excellent experimental results obtained in two true problems of rotation invariance, where the classifier is trained at one particular rotation angle and tested with samples from other rotation angles, demonstrate that good discrimination can be achieved with the statistics of simple rotation invariant local binary patterns. These operators characterize the spatial configuration of local image texture and the performance can be further improved by combining them with rotation invariant variance measures that characterize the contrast of local image texture. The joint distributions of these orthogonal measures are shown to be very powerful tools for rotation invariant texture analysis.},
author = {Ojala, Timo and Pietik{\"{a}}inen, Matti and M{\"{a}}enp{\"{a}}{\"{a}}, Topi},
booktitle = {Computer Vision - ECCV 2000},
doi = {10.1007/3-540-45054-8_27},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ojala, Pietik{\"{a}}inen, M{\"{a}}enp{\"{a}}{\"{a}} - 2000 - Gray Scale and Rotation Invariant Texture Classification with Local Binary Patterns.pdf:pdf},
isbn = {978-3-540-67685-0},
pages = {404--420},
title = {{Gray Scale and Rotation Invariant Texture Classification with Local Binary Patterns}},
url = {http://dx.doi.org/10.1007/3-540-45054-8{\_}27 http://link.springer.com/10.1007/3-540-45054-8{\_}27},
volume = {1842},
year = {2000}
}
@article{Kickingereder2016,
abstract = {The authors of this study identified an 11-feature radiomic signature that allows prediction of survival and stratification of patients with newly diagnosed glioblastomas and that demonstrates improved performance compared with that of established clinical and radiologic risk models.},
author = {Kickingereder, Philipp and Burth, Sina and Wick, Antje and G{\"{o}}tz, Michael and Eidel, Oliver and Schlemmer, Heinz-Peter and Maier-Hein, Klaus H. and Wick, Wolfgang and Bendszus, Martin and Radbruch, Alexander and Bonekamp, David},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kickingereder et al. - 2016 - Radiomic Profiling of Glioblastoma Identifying an Imaging Predictor of Patient Survival with Improved Perf.pdf:pdf},
isbn = {2016160845},
journal = {Radiology},
number = {0},
title = {{Radiomic Profiling of Glioblastoma: Identifying an Imaging Predictor of Patient Survival with Improved Performance over Established Clinical and Radiologic Risk Models}},
volume = {000},
year = {2016}
}
@article{Lopes2011,
abstract = {For texture analysis, several features such as co-occurrence matrices, Gabor filters and the wavelet transform are used. Recently, fractal geometry appeared to be an effective feature to analyze texture. But it is often restricted to 2D images, while 3D information can be very important especially in medical image processing. Moreover applications are limited to the use of fractal dimension. This study focuses on the benefits of fractal geometry in a classification method based on volumic texture analysis. The proposed methods make use of fractal and multifractal features for a 3D texture analysis of a voxel neighborhood. They are validated with synthetic data before being applied on real images. Their efficiencies are proved by comparison to some other texture features in supervised classification processes (AdaBoost and support vector machine classifiers). The results showed that features based on fractal geometry (by combining fractal and multifractal features) contributed to new texture characterization. Information on new features was useful and complementary for a classification method. This study suggests that fractal geometry can provide a new useful information in 3D texture analysis, especially in medical imaging. ?? 2011 Elsevier Ltd. All rights reserved.},
author = {Lopes, R. and Dubois, P. and Bhouri, I. and Bedoui, M.H. and Maouche, S. and Betrouni, N.},
doi = {10.1016/j.patcog.2011.02.017},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lopes et al. - 2011 - Local fractal and multifractal features for volumic texture characterization.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {3D fractal,Classification,Multifractal analysis,Texture analysis},
month = {aug},
number = {8},
pages = {1690--1697},
publisher = {Elsevier},
title = {{Local fractal and multifractal features for volumic texture characterization}},
url = {http://dx.doi.org/10.1016/j.patcog.2011.02.017 http://linkinghub.elsevier.com/retrieve/pii/S003132031100080X},
volume = {44},
year = {2011}
}
@inproceedings{Xu2004,
abstract = {With the dramatic increase of 3D imaging techniques, there is a great demand for new approaches in texture analysis of volumetric data. In this paper, we present a new approach for volumetric texture analysis using a run- length encoding matrix and its texture descriptors. We experiment with our approach on the volumetric data generated from two normal Computed Tomography (CT) studies of the chest and abdomen. Our preliminary results show that there are run-length features calculated from the volumetric run-length matrix that are capable of capturing the texture primitives' properties for different structures in 3D image data, such as the homogeneous texture structure of the liver.},
author = {Xu, Dong-Hui and Kurani, Arati S. and Furst, Jacob D. and Raicu, Daniela S.},
booktitle = {International Conference on Visualization, Imaging and Image Processing (VIIP)},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2004 - Run-Length Encoding for Volumetric Texture.pdf:pdf},
isbn = {0889864152 (ISBN); 9780889864153 (ISBN)},
keywords = {and texture features,image processing and analysis,length encoding,run-,volumetric texture},
pages = {452--458},
title = {{Run-Length Encoding for Volumetric Texture}},
year = {2004}
}
@article{Murala2015,
abstract = {In this paper, we propose a new algorithm using spherical symmetric three dimensional local ternary patterns (SS-3D-LTP) for natural, texture and biomedical image retrieval applications. The existing local binary patterns (LBP), local ternary patterns (LTP), local derivative patterns (LDP), local tetra patterns (LTrP) etc., are encode the relationship between the center pixel and its surrounding neighbors in two dimensional (2D) local region of an image. The proposed method encodes the relationship between the center pixel and its surrounding neighbors with five selected directions in 3D plane which is generated from 2D image using multiresolution Gaussian filter bank. In addition, we propose the color SS-3D-LTP (CSS-3D-LTP) where we consider the RGB spaces as three planes of 3D volume. Three experiments have been carried out for proving the worth of our algorithm for natural, texture and biomedical image retrieval applications. It is further mentioned that the databases used for natural, texture and biomedical image retrieval applications are Corel-10K, Brodatz and open access series of imaging studies (OASIS) magnetic resonance databases respectively. The results after being investigated show a significant improvement in terms of their evaluation measures as compared to the start-of-art spatial as well as transform domain techniques on respective databases.},
author = {Murala, Subrahmanyam and Wu, Q. M. Jonathan},
doi = {10.1016/j.neucom.2014.08.042},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murala, Wu - 2015 - Spherical symmetric 3D local ternary patterns for natural, texture and biomedical image indexing and retrieval.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Biomedical image retrieval,Local binary pattern (LBP),Local ternary patterns (LTP),Texture},
month = {feb},
number = {PC},
pages = {1502--1514},
publisher = {Elsevier},
title = {{Spherical symmetric 3D local ternary patterns for natural, texture and biomedical image indexing and retrieval}},
url = {http://dx.doi.org/10.1016/j.neucom.2014.08.042 http://linkinghub.elsevier.com/retrieve/pii/S0925231214010868},
volume = {149},
year = {2015}
}
@article{Ojala2002,
abstract = {Presents a theoretically very simple, yet efficient,$\backslash$nmultiresolution approach to gray-scale and rotation invariant texture$\backslash$nclassification based on local binary patterns and nonparametric$\backslash$ndiscrimination of sample and prototype distributions. The method is$\backslash$nbased on recognizing that certain local binary patterns, termed$\backslash$n"uniform," are fundamental properties of local image texture and their$\backslash$noccurrence histogram is proven to be a very powerful texture feature. We$\backslash$nderive a generalized gray-scale and rotation invariant operator$\backslash$npresentation that allows for detecting the "uniform" patterns for any$\backslash$nquantization of the angular space and for any spatial resolution and$\backslash$npresents a method for combining multiple operators for multiresolution$\backslash$nanalysis. The proposed approach is very robust in terms of gray-scale$\backslash$nvariations since the operator is, by definition, invariant against any$\backslash$nmonotonic transformation of the gray scale. Another advantage is$\backslash$ncomputational simplicity as the operator can be realized with a few$\backslash$noperations in a small neighborhood and a lookup table. Experimental$\backslash$nresults demonstrate that good discrimination can be achieved with the$\backslash$noccurrence statistics of simple rotation invariant local binary patterns$\backslash$n},
author = {Ojala, Timo and Pietik{\"{a}}inen, Matti and M{\"{a}}enp{\"{a}}{\"{a}}, Topi},
doi = {10.1109/TPAMI.2002.1017623},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ojala, Pietik{\"{a}}inen, M{\"{a}}enp{\"{a}}{\"{a}} - 2002 - Multiresolution gray-scale and rotation invariant texture classification with local binary patterns.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Brodatz,Contrast,Distribution,Histogram,Nonparametric,Outex,Texture analysis},
month = {jul},
number = {7},
pages = {971--987},
title = {{Multiresolution gray-scale and rotation invariant texture classification with local binary patterns}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1017623},
volume = {24},
year = {2002}
}
@article{Hatt2015,
abstract = {UNLABELLED Intratumoral uptake heterogeneity in (18)F-FDG PET has been associated with patient treatment outcomes in several cancer types. Textural feature analysis is a promising method for its quantification. An open issue associated with textural features for the quantification of intratumoral heterogeneity concerns its added contribution and dependence on the metabolically active tumor volume (MATV), which has already been shown to be a significant predictive and prognostic parameter. Our objective was to address this question using a larger cohort of patients covering different cancer types. METHODS A single database of 555 pretreatment (18)F-FDG PET images (breast, cervix, esophageal, head and neck, and lung cancer tumors) was assembled. Four robust and reproducible textural feature-derived parameters were considered. The issues associated with the calculation of textural features using co-occurrence matrices (such as the quantization and spatial directionality relationships) were also investigated. The relationship between these features and MATV, as well as among the features themselves, was investigated using Spearman rank coefficients for different volume ranges. The complementary prognostic value of MATV and textural features was assessed through multivariate Cox analysis in the esophageal and non-small cell lung cancer (NSCLC) cohorts. RESULTS A large range of MATVs was included in the population considered (3-415 cm(3); mean, 35; median, 19; SD, 50). The correlation between MATV and textural features varied greatly depending on the MATVs, with reduced correlation for increasing volumes. These findings were reproducible across the different cancer types. The quantization and calculation methods both had an impact on the correlation. Volume and heterogeneity were independent prognostic factors (P = 0.0053 and 0.0093, respectively) along with stage (P = 0.002) in non-small cell lung cancer, but in the esophageal tumors, volume and heterogeneity had less complementary value because of smaller overall volumes. CONCLUSION Our results suggest that heterogeneity quantification and volume may provide valuable complementary information for volumes above 10 cm(3), although the complementary information increases substantially with larger volumes.},
author = {Hatt, Mathieu and Majdoub, Mohamed and Valli{\`{e}}res, Martin and Tixier, Florent and {Le Rest}, Catherine Cheze and Groheux, David and Hindi{\'{e}}, Elif and Martineau, Antoine and Pradier, Olivier and Hustinx, Roland and Perdrisot, Remy and Guillevin, Remy and {El Naqa}, Issam and Visvikis, Dimitris},
doi = {10.2967/jnumed.114.144055},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hatt et al. - 2015 - 18F-FDG PET Uptake Characterization Through Texture Analysis Investigating the Complementary Nature of Heterogeneit.pdf:pdf},
isbn = {0161-5505},
issn = {1535-5667},
journal = {Journal of nuclear medicine},
keywords = {18FDG PET/CT,heterogeneity,metabolically active tumor volume,prognosis,textural features},
month = {jan},
number = {1},
pages = {38--44},
pmid = {25500829},
title = {{18F-FDG PET uptake characterization through texture analysis: investigating the complementary nature of heterogeneity and functional tumor volume in a multi-cancer site patient cohort.}},
url = {http://jnm.snmjournals.org/content/56/1/38.abstractN2 http://www.ncbi.nlm.nih.gov/pubmed/25500829},
volume = {56},
year = {2015}
}
@article{Alberich-Bayarri2010,
abstract = {PURPOSE In vivo two-dimensional (2D) fractal dimension (D2D) analysis of the cancellous bone at 1.5 T has been related to bone structural complexity and shown to be a potential imaging-based biomarker for osteoporosis. The objectives of this study were to assess at 3 T the in vivo feasibility of three-dimensional (3D) bone fractal dimension (D3D) analysis, analyze the relationship of D2D and D3D with osteoporosis, and investigate the relationship of D3D with spinal bone mineral density (BMD). METHODS A total of 24 female subjects (67 +/- 7 yr old, mean +/- SD) was included in this study. The cohort consisted of 12 healthy volunteers and 12 patients with osteoporosis. MR image acquisitions were performed in the nondominant metaphysis of the distal radius with a 3 T MR scanner and an isotropic resolution of 180 microm. After segmentation and structural reconstruction, 2D and 3D box-counting algorithms were applied to calculate the fractal complexity of the cancellous bone. D2D and D3D values were compared between patients with osteoporosis and healthy subjects, and their relationship with radius BV/TV and spinal BMD was also assessed. RESULTS Significant differences between healthy subjects and patients with osteoporosis were obtained for D3D (p {\textless} 0.001), with less differentiation for D2D (p = 0.04). The relationship between fractal dimension and BMD was not significant (r = 0.43, p = 0.16 and r = 0.23, p = 0.48, for D2D and D3D, respectively). CONCLUSIONS The feasibility of trabecular bone D3D calculations at 3 T and the relationship of both D2D and D3D parameters with osteoporosis were demonstrated, with a better differentiation for the 3D method. Furthermore, the D3D parameter has probably a different nature of information regarding the trabecular bone status not directly explained by BMD alone. Future studies with subjects with osteopenia and larger sample sizes are warranted to further establish the potential of D2D and D3D in the study of osteoporosis.},
author = {Alberich-Bayarri, Angel and Marti-Bonmati, Luis and {Angeles P{\'{e}}rez}, Maria and Sanz-Requena, Roberto and Lerma-Garrido, Juan Jos{\'{e}} and Garc{\'{i}}a-Mart{\'{i}}, Graci{\'{a}}n and Moratal, David},
doi = {10.1118/1.3481509},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alberich-Bayarri et al. - 2010 - Assessment of 2D and 3D fractal dimension measurements of trabecular bone from high-spatial resolution.pdf:pdf},
isbn = {0094-2405},
issn = {0094-2405},
journal = {Medical physics},
keywords = {fractal dimension,image processing,magnetic resonance imaging,trabecular bone},
month = {sep},
number = {9},
pages = {4930--7},
pmid = {20964212},
title = {{Assessment of 2D and 3D fractal dimension measurements of trabecular bone from high-spatial resolution magnetic resonance images at 3 T.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20964212},
volume = {37},
year = {2010}
}
@inproceedings{Fehr2008,
abstract = {We present a novel method for the fast computation of rotation invariant {\{}{\~{A}}{\}}‚{\{}{\^{A}}{\}}¿local binary patterns{\{}{\~{A}}{\}}‚{\{}{\^{A}}{\}}¿ (LBP) on 3D volume data. Unlike a previous publication on 3D LBP, this new approach is not limited to {\{}{\~{A}}{\}}‚{\{}{\^{A}}{\}}¿uniform patterns{\{}{\~{A}}{\}}‚{\{}{\^{A}}{\}}¿, providing a real 3D extension of the standard and rotation invariant LBP. We evaluate our methods in the context of 3D texture analysis of biological data.},
author = {Fehr, Janis and Burkhardt, Hans},
booktitle = {2008 19th International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2008.4761098},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fehr, Burkhardt - 2008 - 3D rotation invariant local binary patterns.pdf:pdf},
isbn = {978-1-4244-2174-9},
issn = {1051-4651},
month = {dec},
number = {July},
pages = {1--4},
publisher = {IEEE},
title = {{3D rotation invariant local binary patterns}},
url = {http://dx.doi.org/10.1109/ICPR.2008.4761098{\$}{\%}5C{\$}nhttp://ieeexplore.ieee.org/xpls/abs{\%}7B{\_}{\%}7Dall.jsp?arnumber=4761098{\$}{\%}5C{\$}nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.152.9528{\%}7B{\&}{\%}7Drep=rep1{\%}7B{\&}{\%}7Dtype=pdf{\$}{\%}5C{\$}nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=1},
year = {2008}
}
@article{Chen2007,
abstract = {Automated image analysis aims to extract relevant information from contrast-enhanced magnetic resonance images (CE-MRI) of the breast and improve the accuracy and consistency of image interpretation. In this work, we extend the traditional 2D gray-level co-occurrence matrix (GLCM) method to investigate a volumetric texture analysis approach and apply it for the characterization of breast MR lesions. Our database of breast MR images was obtained using a T1-weighted 3D spoiled gradient echo sequence and consists of 121 biopsy-proven lesions (77 malignant and 44 benign). A fuzzy c-means clustering (FCM) based method is employed to automatically segment 3D breast lesions on CE-MR images. For each 3D lesion, a nondirectional GLCM is then computed on the first postcontrast frame by summing 13 directional GLCMs. Texture features are extracted from the nondirectional GLCMs and the performance of each texture feature in the task of distinguishing between malignant and benign breast lesions is assessed by receiver operating characteristics (ROC) analysis. Our results show that the classification performance of volumetric texture features is significantly better than that based on 2D analysis. Our investigations of the effects of various of parameters on the diagnostic accuracy provided means for the optimal use of the approach.},
author = {Chen, Weijie and Giger, Maryellen L. and Li, Hui and Bick, Ulrich and Newstead, Gillian M.},
doi = {10.1002/mrm.21347},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2007 - Volumetric texture analysis of breast lesions on contrast-enhanced magnetic resonance images.pdf:pdf},
isbn = {1522-2594},
issn = {0740-3194},
journal = {Magnetic resonance in medicine},
keywords = {Breast MRI,Gray-level co-occurrence matrix,Texture analysis},
month = {sep},
number = {3},
pages = {562--71},
pmid = {17763361},
title = {{Volumetric texture analysis of breast lesions on contrast-enhanced magnetic resonance images.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17763361},
volume = {58},
year = {2007}
}
@article{Boellaard2009,
abstract = {Quantitative (18)F-FDG PET is increasingly being recognized as an important tool for diagnosis, determination of prognosis, and response monitoring in oncology. However, PET quantification with, for example, standardized uptake values (SUVs) is affected by many technical and physiologic factors. As a result, some of the variations in the literature on SUV-based patient outcomes are explained by differences in (18)F-FDG PET study methods. Various technical and clinical studies have been performed to understand the factors affecting PET quantification. On the basis of the results of those studies, several recommendations and guidelines have been proposed with the aims of improving the image quality and the quantitative accuracy of (18)F-FDG PET studies. In this contribution, an overview of recommendations and guidelines for quantitative (18)F-FDG PET studies in oncology is provided. Special attention is given to the rationale underlying certain recommendations and to some of the differences in various guidelines.},
author = {Boellaard, Ronald},
doi = {10.2967/jnumed.108.057182},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boellaard - 2009 - Standards for PET image acquisition and quantitative data analysis.pdf:pdf},
isbn = {0161-5505 (Print)$\backslash$n0161-5505 (Linking)},
issn = {0161-5505},
journal = {Journal of Nuclear Medicine},
keywords = {Computer-Assisted,Computer-Assisted: standards,Fluorodeoxyglucose F18,Fluorodeoxyglucose F18: diagnostic use,Humans,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: standards,Neoplasms,Neoplasms: radionuclide imaging,Neoplasms: therapy,Positron-Emission Tomography,Positron-Emission Tomography: methods,Positron-Emission Tomography: standards,Practice Guidelines as Topic,Radiopharmaceuticals,Radiopharmaceuticals: diagnostic use,Subtraction Technique,Treatment Outcome},
month = {may},
pages = {11S--20S},
pmid = {19380405},
title = {{Standards for PET image acquisition and quantitative data analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19380405},
volume = {50 Suppl 1},
year = {2009}
}
@article{Depeursinge2014,
abstract = {Three-dimensional computerized characterization of biomedical solid textures is key to large-scale and high-throughput screening of imaging data. Such data increasingly become available in the clinical and research environments with an ever increasing spatial resolution. In this text we exhaustively analyze the state-of-the-art in 3-D biomedical texture analysis to identify the specific needs of the application domains and extract promising trends in image processing algorithms. The geometrical properties of biomedical textures are studied both in their natural space and on digitized lattices. It is found that most of the tissue types have strong multi-scale directional properties, that are well captured by imaging protocols with high resolutions and spherical spatial transfer functions. The information modeled by the various image processing techniques is analyzed and visualized by displaying their 3-D texture primitives. We demonstrate that non-convolutional approaches are expected to provide best results when the size of structures are inferior to five voxels. For larger structures, it is shown that only multi-scale directional convolutional approaches that are non-separable allow for an unbiased modeling of 3-D biomedical textures. With the increase of high-resolution isotropic imaging protocols in clinical routine and research, these models are expected to best leverage the wealth of 3-D biomedical texture analysis in the future. Future research directions and opportunities are proposed to efficiently model personalized image-based phenotypes of normal biomedical tissue and its alterations. The integration of the clinical and genomic context is expected to better explain the intra class variation of healthy biomedical textures. Using texture synthesis, this provides the exciting opportunity to simulate and visualize texture atlases of normal ageing process and disease progression for enhanced treatment planning and clinical care management.},
author = {Depeursinge, Adrien and Foncubierta-Rodriguez, Antonio and {Van De Ville}, Dimitri and M{\"{u}}ller, Henning},
doi = {10.1016/j.media.2013.10.005},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Depeursinge et al. - 2014 - Three-dimensional solid texture analysis in biomedical imaging Review and opportunities.pdf:pdf},
isbn = {1361-8415},
issn = {1361-8423},
journal = {Medical image analysis},
keywords = {3-D texture,Classification,Solid texture,Texture primitive,Volumetric texture},
month = {jan},
number = {1},
pages = {176--96},
pmid = {24231667},
publisher = {Elsevier B.V.},
title = {{Three-dimensional solid texture analysis in biomedical imaging: review and opportunities.}},
url = {http://dx.doi.org/10.1016/j.media.2013.10.005 http://www.ncbi.nlm.nih.gov/pubmed/24231667},
volume = {18},
year = {2014}
}
@article{Mishkin2016,
abstract = {The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatibility with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the "deficit" is small suggesting independence of their benefits. We show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.},
archivePrefix = {arXiv},
arxivId = {1606.02228},
author = {Mishkin, Dmytro and Sergievskiy, Nikolay and Matas, Jiri},
eprint = {1606.02228},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mishkin, Sergievskiy, Matas - 2016 - Systematic evaluation of CNN advances on the ImageNet.pdf:pdf},
keywords = {benchmark,cnn,imagenet,non-linearity,pooling},
month = {jun},
title = {{Systematic evaluation of CNN advances on the ImageNet}},
url = {http://arxiv.org/abs/1606.02228},
year = {2016}
}
@article{Ohkubo2011,
abstract = {PURPOSE While the acquisition of projection data in a computed tomography (CT) scanner is generally cqrried out once, the projection data is often removed from the system, making further reconstruction with a different reconstruction filter impossible. The reconstruction kernel is one of the most important parameters. To have access to all the reconstructions, either prior reconstructions with multiple kernels must be performed or the projection data must be stored. Each of these requirements would increase the burden on data archiving. This study aimed to design an effective method to achieve similar image quality using an image filtering technique in the image space, instead of a reconstruction filter in the projection space for CT imaging. The authors evaluated the clinical feasibility of the proposed method in lung cancer screening. METHODS The proposed technique is essentially the same as common image filtering, which performs processing in the spatial-frequency domain with a filter function. However, the filter function was determined based on the quantitative analysis of the point spread functions (PSFs) measured in the system. The modulation transfer functions (MTFs) were derived from the PSFs, and the ratio of the MTFs was used as the filter function. Therefore, using an image reconstructed with a kernel, an image reconstructed with a different kernel was obtained by filtering, which used the ratio of the MTFs obtained for the two kernels. The performance of the method was evaluated by using routine clinical images obtained from CT screening for lung cancer in five subjects. RESULTS Filtered images for all combinations of three types of reconstruction kernels ("smooth," "standard," and "sharp" kernels) showed good agreement with original reconstructed images regarded as the gold standard. On the filtered images, abnormal shadows suspected as being lung cancers were identical to those on the reconstructed images. The standard deviations (SDs) for the difference between filtered images and reconstructed images ranged from 1.9 to 23.5 Hounsfield units for all kernel combinations; these SDs were much smaller than the noise SDs in the reconstructed images. CONCLUSIONS The proposed method has good performance and is clinically feasible in lung cancer screening. This method can be applied to images reconstructed on any scanner by measuring the PSFs in each system.},
author = {Ohkubo, Masaki and Wada, Shinichi and Kayugawa, Akihiro and Matsumoto, Toru and Murao, Kohei},
doi = {10.1118/1.3590363},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ohkubo et al. - 2011 - Image filtering as an alternative to the application of a different reconstruction kernel in CT imaging feasibili.pdf:pdf},
isbn = {0094-2405 (Print)$\backslash$r0094-2405 (Linking)},
issn = {0094-2405},
journal = {Medical physics},
keywords = {computed tomography,modulation transfer function,point spread function,reconstruc-,spatial resolution,tion kernel},
month = {jul},
number = {7},
pages = {3915--23},
pmid = {21858988},
title = {{Image filtering as an alternative to the application of a different reconstruction kernel in CT imaging: feasibility study in lung cancer screening.}},
url = {http://scitation.aip.org/content/aapm/journal/medphys/38/7/10.1118/1.3590363 http://www.ncbi.nlm.nih.gov/pubmed/21858988},
volume = {38},
year = {2011}
}
@article{Vanhatalo2012,
abstract = {Gaussian processes (GP) are powerful tools for probabilistic modeling purposes. They can be used to define prior distributions over latent functions in hierarchical Bayesian models. The prior over functions is defined implicitly by the mean and covariance function, which determine the smoothness and variability of the function. The inference can then be conducted directly in the function space by evaluating or approximating the posterior process. Despite their attractive theoretical properties GPs provide practical challenges in their implementation. GPstuff is a versatile collection of computational tools for GP models compatible with Linux and Windows MATLAB and Octave. It includes, among others, various inference methods, sparse approximations and tools for model assessment. In this work, we review these tools and demonstrate the use of GPstuff in several models.},
archivePrefix = {arXiv},
arxivId = {1206.5754},
author = {Vanhatalo, Jarno and Riihim{\"{a}}ki, Jaakko and Hartikainen, Jouni and Jyl{\"{a}}nki, Pasi and Tolvanen, Ville and Vehtari, Aki},
eprint = {1206.5754},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vanhatalo et al. - 2012 - Bayesian Modeling with Gaussian Processes using the GPstuff Toolbox.pdf:pdf},
issn = {1532-4435},
journal = {ArXiv e-prints},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Mathematical Software,Statistics - Machine Learning},
month = {jun},
pages = {48},
title = {{Bayesian Modeling with Gaussian Processes using the GPstuff Toolbox}},
url = {http://arxiv.org/abs/1206.5754},
year = {2012}
}
@book{Barber2012,
author = {Barber, David},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barber - 2012 - Bayesian reasoning and machine learning.pdf:pdf},
isbn = {9780521518147},
publisher = {Cambridge University Press},
title = {{Bayesian reasoning and machine learning}},
url = {http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/181115.pdf},
year = {2012}
}
@article{Clausi2002,
abstract = {In this paper, the effect of grey level quantization on the ability of co-occurrence probability statistics to classify natural textures is studied. Generally, as a function of increasing grey levels, many of the statistics demonstrate a decrease in classification ability while a few maintain constant classification accuracy. None of the individual statistics show increasing classification accuracy throughout all grey levels. Correlation analysis is used to rationalize a preferred subset of statistics. The preferred statistics set (contrast, correlation, and entropy) is demonstrated to be an improvement over using single statistics or using the entire set of statistics. If the feature space dimension only allows for a single statistic, one of contrast, dissimilarity, inverse difference normalized, or inverse difference moment normalized, is recommended. Testing that compares (using all orientations separately), the average of all orientations and look direction averaging, when determining the co-occurrence features, indicates that the look direction or all orientations is preferred. The Fisher linear discriminant method is used for all classification testing. The Fisher criterion is used as a separability index to provide insight into the classification results. Testing is performed on Brodatz imagery as well as two separate SAR sea-ice data sets.},
author = {Clausi, David A.},
doi = {10.5589/m02-004},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clausi - 2002 - An analysis of co-occurrence texture statistics as a function of grey level quantization.pdf:pdf},
isbn = {07038992 (ISSN)},
issn = {07038992},
journal = {Canadian Journal of Remote Sensing},
number = {1},
pages = {45--62},
title = {{An analysis of co-occurrence texture statistics as a function of grey level quantization}},
volume = {28},
year = {2002}
}
@article{Kang2015,
abstract = {Radiation oncology has always been deeply rooted in modeling, from the early days of isoeffect curves to the contemporary Quantitative Analysis of Normal Tissue Effects in the Clinic (QUANTEC) initiative. In recent years, medical modeling for both prognostic and therapeutic purposes has exploded thanks to increasing availability of electronic data and genomics. One promising direction that medical modeling is moving toward is adopting the same machine learning methods used by companies such as Google and Facebook to combat disease. Broadly defined, machine learning is a branch of computer science that deals with making predictions from complex data through statistical models. These methods serve to uncover patterns in data and are actively used in areas such as speech recognition, handwriting recognition, face recognition, "spam" filtering (junk email), and targeted advertising. Although multiple radiation oncology research groups have shown the value of applied machine learning (ML), clinical adoption has been slow due to the high barrier to understanding these complex models by clinicians. Here, we present a review of the use of ML to predict radiation therapy outcomes from the clinician's point of view with the hope that it lowers the "barrier to entry" for those without formal training in ML. We begin by describing 7 principles that one should consider when evaluating (or creating) an ML model in radiation oncology. We next introduce 3 popular ML methods--logistic regression (LR), support vector machine (SVM), and artificial neural network (ANN)--and critique 3 seminal papers in the context of these principles. Although current studies are in exploratory stages, the overall methodology has progressively matured, and the field is ready for larger-scale further investigation.},
author = {Kang, John and Schwartz, Russell and Flickinger, John and Beriwal, Sushil},
doi = {10.1016/j.ijrobp.2015.07.2286},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kang et al. - 2015 - Machine Learning Approaches for Predicting Radiation Therapy Outcomes A Clinician's Perspective.pdf:pdf},
issn = {1879-355X},
journal = {International journal of radiation oncology*biology*physics},
month = {dec},
number = {5},
pages = {1127--35},
pmid = {26581149},
publisher = {Elsevier Inc.},
title = {{Machine Learning Approaches for Predicting Radiation Therapy Outcomes: A Clinician's Perspective.}},
url = {http://dx.doi.org/10.1016/j.ijrobp.2015.07.2286 http://www.ncbi.nlm.nih.gov/pubmed/26581149},
volume = {93},
year = {2015}
}
@article{Hatt2016,
abstract = {After seminal papers over the period 2009 - 2011, the use of texture analysis of PET/CT images for quantification of intratumour uptake heterogeneity has received increasing attention in the last 4 years. Results are difficult to compare due to the heterogeneity of studies and lack of standardization. There are also numerous challenges to address. In this review we provide critical insights into the recent development of texture analysis for quantifying the heterogeneity in PET/CT images, identify issues and challenges, and offer recommendations for the use of texture analysis in clinical research. Numerous potentially confounding issues have been identified, related to the complex workflow for the calculation of textural features, and the dependency of features on various factors such as acquisition, image reconstruction, preprocessing, functional volume segmentation, and methods of establishing and quantifying correspondences with genomic and clinical metrics of interest. A lack of understanding of what the features may represent in terms of the underlying pathophysiological processes and the variability of technical implementation practices makes comparing results in the literature challenging, if not impossible. Since progress as a field requires pooling results, there is an urgent need for standardization and recommendations/guidelines to enable the field to move forward. We provide a list of correct formulae for usual features and recommendations regarding implementation. Studies on larger cohorts with robust statistical analysis and machine learning approaches are promising directions to evaluate the potential of this approach.},
author = {Hatt, Mathieu and Tixier, Florent and Pierce, Larry and Kinahan, Paul E. and {Le Rest}, Catherine Cheze and Visvikis, Dimitris},
doi = {10.1007/s00259-016-3427-0},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hatt et al. - 2016 - Characterization of PETCT images using texture analysis the past, the present{\ldots} any future.pdf:pdf},
issn = {1619-7089},
journal = {European journal of nuclear medicine and molecular imaging},
keywords = {Critical review,Heterogeneity,Image texture,PET/CT,Recommendations},
month = {jan},
number = {1},
pages = {151--165},
pmid = {27271051},
publisher = {European Journal of Nuclear Medicine and Molecular Imaging},
title = {{Characterization of PET/CT images using texture analysis: the past, the present{\ldots} any future?}},
url = {http://link.springer.com/10.1007/s00259-016-3427-0 http://www.ncbi.nlm.nih.gov/pubmed/27271051 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5283691},
volume = {44},
year = {2017}
}
@article{Pentland1984,
abstract = {This paper addresses the problems of 1) representing natural shapes such as mountains, trees, and clouds, and 2) computing their description from image data. To solve these problems, we must be able to relate natural surfaces to their images; this requires a good model of natural surface shapes. Fractal functions are a good choice for modeling 3-D natural surfaces because 1) many physical processes produce a fractal surface shape, 2) fractals are widely used as a graphics tool for generating natural-looking shapes, and 3) a survey of natural imagery has shown that the 3-D fractal surface model, transformed by the image formation process, furnishes an accurate description of both textured and shaded image regions. The 3-D fractal model provides a characterization of 3-D surfaces and their images for which the appropriateness of the model is verifiable. Furthermore, this characterization is stable over transformations of scale and linear transforms of intensity. The 3-D fractal model has been successfully applied to the problems of 1) texture segmentation and classification, 2) estimation of 3-D shape information, and 3) distinguishing between perceptually ``smooth'' and perceptually ``textured'' surfaces in the scene.},
author = {Pentland, Alex P.},
doi = {10.1109/TPAMI.1984.4767591},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pentland - 1984 - Fractal-Based Description of Natural Scenes.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {nov},
number = {6},
pages = {661--674},
title = {{Fractal-Based Description of Natural Scenes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4767591},
volume = {PAMI-6},
year = {1984}
}
@article{Kim2016,
abstract = {Cancer is now increasingly studied from the perspective of dysregulated pathways, rather than as a disease resulting from mutations of individual genes. A pathway-centric view acknowledges the heterogeneity between genomic profiles from different cancer patients while assuming that the mutated genes are likely to belong to the same pathway and cause similar disease phenotypes. Indeed, network-centric approaches have proven to be helpful for finding genotypic causes of diseases, classifying disease subtypes, and identifying drug targets. In this review, we discuss how networks can be used to help understand patient-to-patient variations and how one can leverage this variability to elucidate interactions between cancer drivers.},
author = {Kim, Yoo-Ah and Cho, Dong-Yeon and Przytycka, Teresa M},
doi = {10.1371/journal.pcbi.1004747},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Cho, Przytycka - 2016 - Understanding Genotype-Phenotype Effects in Cancer via Network Approaches.pdf:pdf},
isbn = {10.1371/journal.pcbi.1004747},
issn = {1553-7358},
journal = {PLoS computational biology},
month = {mar},
number = {3},
pages = {e1004747},
pmid = {26963104},
title = {{Understanding Genotype-Phenotype Effects in Cancer via Network Approaches.}},
url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004747 http://www.ncbi.nlm.nih.gov/pubmed/26963104 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4786343},
volume = {12},
year = {2016}
}
@article{Baumann2013,
abstract = {A Random Forest consists of several independent decision trees arranged in a forest. A majority vote over all trees leads to the final decision. In this paper we propose a Random Forest framework which incorporates a cascade structure consisting of several stages together with a bootstrap approach. By introducing the cascade, 99{\%} of the test images can be rejected by the first and second stage with minimal computational effort leading to a massively speeded-up detection framework. Three different cascade voting strategies are implemented and evaluated. Additionally, the training and classification speed-up is analyzed. Several experiments on public available datasets for pedestrian detection, lateral car detection and unconstrained face detection demonstrate the benefit of our contribution.},
author = {Baumann, Florian and Ehlers, Arne and Vogt, Karsten and Rosenhahn, Bodo},
doi = {10.1007/978-3-642-38886-6_13},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baumann et al. - 2013 - Cascaded random forest for fast object detection.pdf:pdf},
isbn = {9783642388859},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
pages = {131--142},
title = {{Cascaded random forest for fast object detection}},
volume = {7944 LNCS},
year = {2013}
}
@article{Baumann2016,
abstract = {Technological advances and clinical research over the past few decades have given radiation oncologists the capability to personalize treatments for accurate delivery of radiation dose based on clinical parameters and anatomical information. Eradication of gross and microscopic tumours with preservation of health-related quality of life can be achieved in many patients. Two major strategies, acting synergistically, will enable further widening of the therapeutic window of radiation oncology in the era of precision medicine: technology-driven improvement of treatment conformity, including advanced image guidance and particle therapy, and novel biological concepts for personalized treatment, including biomarker-guided prescription, combined treatment modalities and adaptation of treatment during its course.},
author = {Baumann, Michael and Krause, Mechthild and Overgaard, Jens and Debus, J{\"{u}}rgen and Bentzen, S{\o}ren M. and Daartz, Juliane and Richter, Christian and Zips, Daniel and Bortfeld, Thomas},
doi = {10.1038/nrc.2016.18},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baumann et al. - 2016 - Radiation oncology in the era of precision medicine(2).pdf:pdf},
issn = {1474-1768},
journal = {Nature reviews. Cancer},
month = {apr},
number = {4},
pages = {234--49},
pmid = {27009394},
publisher = {Nature Publishing Group},
title = {{Radiation oncology in the era of precision medicine.}},
url = {http://dx.doi.org/10.1038/nrc.2016.18 http://www.ncbi.nlm.nih.gov/pubmed/27009394},
volume = {16},
year = {2016}
}
@article{Gallardo-Estrella2016,
abstract = {OBJECTIVES To propose and evaluate a method to reduce variability in emphysema quantification among different computed tomography (CT) reconstructions by normalizing CT data reconstructed with varying kernels. METHODS We included 369 subjects from the COPDGene study. For each subject, spirometry and a chest CT reconstructed with two kernels were obtained using two different scanners. Normalization was performed by frequency band decomposition with hierarchical unsharp masking to standardize the energy in each band to a reference value. Emphysema scores (ES), the percentage of lung voxels below -950 HU, were computed before and after normalization. Bland-Altman analysis and correlation between ES and spirometry before and after normalization were compared. Two mixed cohorts, containing data from all scanners and kernels, were created to simulate heterogeneous acquisition parameters. RESULTS The average difference in ES between kernels decreased for the scans obtained with both scanners after normalization (7.7 ± 2.7 to 0.3 ± 0.7; 7.2 ± 3.8 to -0.1 ± 0.5). Correlation coefficients between ES and FEV1, and FEV1/FVC increased significantly for the mixed cohorts. CONCLUSIONS Normalization of chest CT data reduces variation in emphysema quantification due to reconstruction filters and improves correlation between ES and spirometry. KEY POINTS • Emphysema quantification is sensitive to the reconstruction kernel used. • Normalization allows comparison of emphysema quantification from images reconstructed with varying kernels. • Normalization allows comparison of emphysema quantification obtained with scanners from different manufacturers. • Normalization improves correlation of emphysema quantification with spirometry. • Normalization can be used to compare data from different studies and centers.},
author = {Gallardo-Estrella, Leticia and Lynch, David A. and Prokop, Mathias and Stinson, Douglas and Zach, Jordan and Judy, Philip F. and van Ginneken, Bram and van Rikxoort, Eva M.},
doi = {10.1007/s00330-015-3824-y},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gallardo-Estrella et al. - 2016 - Normalizing computed tomography data reconstructed with different filter kernels effect on emphysema q.pdf:pdf},
issn = {1432-1084},
journal = {European radiology},
keywords = {COPD,Computed tomography,Image reconstruction,Normalization,Pulmonary emphysema},
month = {feb},
number = {2},
pages = {478--86},
pmid = {26002132},
title = {{Normalizing computed tomography data reconstructed with different filter kernels: effect on emphysema quantification.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26002132 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4712239},
volume = {26},
year = {2016}
}
@article{Uno2011,
abstract = {For modern evidence-based medicine, a well thought-out risk scoring system for predicting the occurrence of a clinical event plays an important role in selecting prevention and treatment strategies. Such an index system is often established based on the subject's 'baseline' genetic or clinical markers via a working parametric or semi-parametric model. To evaluate the adequacy of such a system, C-statistics are routinely used in the medical literature to quantify the capacity of the estimated risk score in discriminating among subjects with different event times. The C-statistic provides a global assessment of a fitted survival model for the continuous event time rather than focussing on the prediction of bit-year survival for a fixed time. When the event time is possibly censored, however, the population parameters corresponding to the commonly used C-statistics may depend on the study-specific censoring distribution. In this article, we present a simple C-statistic without this shortcoming. The new procedure consistently estimates a conventional concordance measure which is free of censoring. We provide a large sample approximation to the distribution of this estimator for making inferences about the concordance measure. Results from numerical studies suggest that the new procedure performs well in finite sample.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Uno, Hajime and Cai, Tianxi and Pencina, Michael J. and D'Agostino, Ralph B. and Wei, L. J.},
doi = {10.1002/sim.4154},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Uno et al. - 2011 - On the C-statistics for evaluating overall adequacy of risk prediction procedures with censored survival data.pdf:pdf},
isbn = {1097-0258},
issn = {1097-0258},
journal = {Statistics in medicine},
keywords = {AUC,Cox's proportional hazards model,Framingham risk score,ROC},
month = {may},
number = {10},
pages = {1105--17},
pmid = {21484848},
title = {{On the C-statistics for evaluating overall adequacy of risk prediction procedures with censored survival data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21484848 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3079915},
volume = {30},
year = {2011}
}
@article{Coroller2015,
abstract = {BACKGROUND AND PURPOSE Radiomics provides opportunities to quantify the tumor phenotype non-invasively by applying a large number of quantitative imaging features. This study evaluates computed-tomography (CT) radiomic features for their capability to predict distant metastasis (DM) for lung adenocarcinoma patients. MATERIAL AND METHODS We included two datasets: 98 patients for discovery and 84 for validation. The phenotype of the primary tumor was quantified on pre-treatment CT-scans using 635 radiomic features. Univariate and multivariate analysis was performed to evaluate radiomics performance using the concordance index (CI). RESULTS Thirty-five radiomic features were found to be prognostic (CI{\textgreater}0.60, FDR{\textless}5{\%}) for DM and twelve for survival. It is noteworthy that tumor volume was only moderately prognostic for DM (CI=0.55, p-value=2.77×10(-5)) in the discovery cohort. A radiomic-signature had strong power for predicting DM in the independent validation dataset (CI=0.61, p-value=1.79×10(-17)). Adding this radiomic-signature to a clinical model resulted in a significant improvement of predicting DM in the validation dataset (p-value=1.56×10(-11)). CONCLUSIONS Although only basic metrics are routinely quantified, this study shows that radiomic features capturing detailed information of the tumor phenotype can be used as a prognostic biomarker for clinically-relevant factors such as DM. Moreover, the radiomic-signature provided additional information to clinical data.},
author = {Coroller, Thibaud P. and Grossmann, Patrick and Hou, Ying and Rios-Velazquez, Emmanuel and Leijenaar, Ralph T. H. and Hermann, Gretchen and Lambin, Philippe and Haibe-Kains, Benjamin and Mak, Raymond H. and Aerts, Hugo J. W. L.},
doi = {10.1016/j.radonc.2015.02.015},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Coroller et al. - 2015 - CT-based radiomic signature predicts distant metastasis in lung adenocarcinoma(2).pdf:pdf},
isbn = {1879-0887 (Electronic)$\backslash$r0167-8140 (Linking)},
issn = {1879-0887},
journal = {Radiotherapy and Oncology},
keywords = {Biomarkers,Distant metastasis,Lung adenocarcinoma,NSCLC,Quantitative imaging,Radiomics},
month = {mar},
number = {3},
pages = {345--50},
pmid = {25746350},
publisher = {Elsevier Ireland Ltd},
title = {{CT-based radiomic signature predicts distant metastasis in lung adenocarcinoma.}},
url = {http://dx.doi.org/10.1016/j.radonc.2015.02.015 http://www.ncbi.nlm.nih.gov/pubmed/25746350 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4400248},
volume = {114},
year = {2015}
}
@article{Flohr2006,
abstract = {We present a performance evaluation of a recently introduced dual-source computed tomography (DSCT) system equipped with two X-ray tubes and two corresponding detectors, mounted onto the rotating gantry with an angular offset of 90 degrees . We introduce the system concept and derive its consequences and potential benefits for electrocardiograph [corrected] (ECG)-controlled cardiac CT and for general radiology applications. We evaluate both temporal and spatial resolution by means of phantom scans. We present first patient scans to illustrate the performance of DSCT for ECG-gated cardiac imaging, and we demonstrate first results using a dual-energy acquisition mode. Using ECG-gated single-segment reconstruction, the DSCT system provides 83 ms temporal resolution independent of the patient's heart rate for coronary CT angiography (CTA) and evaluation of basic functional parameters. With dual-segment reconstruction, the mean temporal resolution is 60 ms (minimum temporal resolution 42 ms) for advanced functional evaluation. The z-flying focal spot technique implemented in the evaluated DSCT system allows 0.4 mm cylinders to be resolved at all heart rates. First clinical experience shows a considerably increased robustness for the imaging of patients with high heart rates. As a potential application of the dual-energy acquisition mode, the automatic separation of bones and iodine-filled vessels is demonstrated.},
author = {Flohr, Thomas G. and McCollough, Cynthia H. and Bruder, Herbert and Petersilka, Martin and Gruber, Klaus and S{\"{u}}ss, Christoph and Grasruck, Michael and Stierstorfer, Karl and Krauss, Bernhard and Raupach, Rainer and Primak, Andrew N. and K{\"{u}}ttner, Axel and Achenbach, Stefan and Becker, Christoph R. and Kopp, Andreas and Ohnesorge, Bernd M.},
doi = {10.1007/s00330-005-2919-2},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Flohr et al. - 2006 - First performance evaluation of a dual-source CT (DSCT) system.pdf:pdf},
isbn = {0938-7994 (Print)$\backslash$n0938-7994 (Linking)},
issn = {0938-7994},
journal = {European radiology},
keywords = {CT technology,Cardiac CT,Computed tomography,Dual-source CT,Multidetector-row CT},
month = {feb},
number = {2},
pages = {256--68},
pmid = {16341833},
title = {{First performance evaluation of a dual-source CT (DSCT) system.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16341833},
volume = {16},
year = {2006}
}
@article{Johnson2007,
abstract = {The aim of this study was to assess the feasibility of a differentiation of iodine from other materials and of different body tissues using dual energy CT. Ten patients were scanned on a SOMATOM Definition Dual Source CT (DSCT; Siemens, Forchheim, Germany) system in dual energy mode at tube voltages of 140 and 80 kVp and a ratio of 1:3 between tube currents. Weighted CT Dose Index ranged between 7 and 8 mGy, remaining markedly below reference dose values for the respective body regions. Image post-processing with three-material decomposition was applied to differentiate iodine or collagen from other tissue. The results showed that a differentiation and depiction of contrast material distribution is possible in the brain, the lung, the liver and the kidneys with or without the underlying tissue of the organ. In angiographies, bone structures can be removed from the dataset to ease the evaluation of the vessels. The differentiation of collagen makes it possible to depict tendons and ligaments. Dual energy CT offers a more specific tissue characterization in CT and can improve the assessment of vascular disease. Further studies are required to draw conclusions on the diagnostic value of the individual applications.},
author = {Johnson, Thorsten R. C. and Krauss, Bernhard and Sedlmair, Martin and Grasruck, Michael and Bruder, Herbert and Morhard, Dominik and Fink, Christian and Weckbach, Sabine and Lenhard, Miriam and Schmidt, Bernhard and Flohr, Thomas G. and Reiser, Maximilian F. and Becker, Christoph R.},
doi = {10.1007/s00330-006-0517-6},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson et al. - 2007 - Material differentiation by dual energy CT initial experience.pdf:pdf},
isbn = {0938-7994 (Print)$\backslash$n0938-7994 (Linking)},
issn = {0938-7994},
journal = {European radiology},
keywords = {Angiography,Collagen,Dual-energy CT,Iodine,X-ray computed tomography},
month = {jun},
number = {6},
pages = {1510--7},
pmid = {17151859},
title = {{Material differentiation by dual energy CT: initial experience.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17151859},
volume = {17},
year = {2007}
}
@article{Elmpt2016,
abstract = {a b s t r a c t Dual energy CT (DECT) scanners are nowadays available in many radiology departments. For radiother-apy purposes, new strategies using DECT imaging are investigated to optimize radiation treatment for multiple steps in the radiotherapy chain. This review describes how DECT based methods can be used for electron density estimation, effective atomic number decomposition and contrast material quantifi-cation. Clinical radiotherapy related applications for improved dose calculation accuracy of brachyther-apy and proton therapy, metal artifact reduction techniques and normal tissue characterization are also summarized together with future perspectives on the use of DECT for radiotherapy purposes. {\'{O}} 2016 Elsevier Ireland Ltd. All rights reserved. Radiotherapy and Oncology 119 (2016) 137–144 Imaging is one of the cornerstones for diagnosis, radiation treat-ment planning and follow-up assessment of cancer patients. Com-puted tomography (CT) using X-rays is the most frequently used imaging modality for radiation therapy (RT), i.e. brachytherapy, external photon, electron and proton beam treatment [1]. Mainly the relatively easy calibration of Hounsfield Units from the CT scanner into electron density makes this modality perfectly suited for accurate dose calculation purposes in external photon beam RT. The use of other imaging modalities has increased over the past years such as magnetic resonance imaging (MRI) for regions where a high soft-tissue contrast is necessary, or functional imaging with dedicated radioactive tracers for positron emission tomography (PET). These modalities have for some treatment sites been inte-grated into the routine workflow for cancer patient imaging, but are nowadays typically still used in addition to CT imaging. In recent years, the use of dual energy (DE) CT imaging has gained increased attention in radiology departments. Currently, multiple new strategies for using dual energy CT (DECT) systems for the entire chain of radiotherapy are also being investigated: e.g. improved dose calculation accuracy for brachytherapy and pro-ton therapy, metal artifact reduction techniques and normal tissue characterization. This review article will describe the current technology available for DECT, review the possible applications and show future perspectives on the use of DECT for radiotherapy purposes. Dual energy CT imaging: technology and physics Imaging equipment for dual-energy CT imaging The use of DE in CT scanners is not a recent idea. Already in the early days of CT imaging DE techniques were described [2,3]. Back then mainly technical limitations and computational power restricted the implementation of DE in routine practice. It was only a decade ago that DE imaging was introduced again, when the first clinical DECT scanner in 2005 became available [4,5]. This scanner used a dual-source technique by making use of two orthogonally-mounted X-ray tubes with corresponding detectors installed in the scanner, rotating around the patient independently operated with different kilovoltage settings. Since then, DECT has become a valuable tool in daily routine practice for different clinical mainly diagnostic applications e.g. differentiation of urinary stones, imaging of pulmonary embolism, neuro imaging or differentiation of pulmonary nodules [6–10]. Its easy use and radiation dose neutral application compared to standard routine protocols has made DECT an important new tool in daily routine practice. Furthermore, the use of DECT and further energy decomposition analysis offers a huge potential for improve-ment of image quality and further reduction of radiation dose [11].},
author = {van Elmpt, Wouter J. C. and Landry, Guillaume and Das, Marco and Verhaegen, Frank},
doi = {10.1016/j.radonc.2016.02.026},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Elmpt et al. - 2016 - Dual energy CT in radiotherapy Current applications and future outlook.pdf:pdf},
issn = {01678140},
journal = {Radiotherapy and Oncology},
month = {apr},
number = {1},
pages = {137--144},
publisher = {Elsevier Ireland Ltd},
title = {{Dual energy CT in radiotherapy: Current applications and future outlook}},
url = {http://dx.doi.org/10.1016/j.radonc.2016.02.026 http://linkinghub.elsevier.com/retrieve/pii/S0167814016001146},
volume = {119},
year = {2016}
}
@article{Chen2012,
abstract = {BACKGROUND Cancer survival studies are commonly analyzed using survival-time prediction models for cancer prognosis. A number of different performance metrics are used to ascertain the concordance between the predicted risk score of each patient and the actual survival time, but these metrics can sometimes conflict. Alternatively, patients are sometimes divided into two classes according to a survival-time threshold, and binary classifiers are applied to predict each patient's class. Although this approach has several drawbacks, it does provide natural performance metrics such as positive and negative predictive values to enable unambiguous assessments. METHODS We compare the survival-time prediction and survival-time threshold approaches to analyzing cancer survival studies. We review and compare common performance metrics for the two approaches. We present new randomization tests and cross-validation methods to enable unambiguous statistical inferences for several performance metrics used with the survival-time prediction approach. We consider five survival prediction models consisting of one clinical model, two gene expression models, and two models from combinations of clinical and gene expression models. RESULTS A public breast cancer dataset was used to compare several performance metrics using five prediction models. 1) For some prediction models, the hazard ratio from fitting a Cox proportional hazards model was significant, but the two-group comparison was insignificant, and vice versa. 2) The randomization test and cross-validation were generally consistent with the p-values obtained from the standard performance metrics. 3) Binary classifiers highly depended on how the risk groups were defined; a slight change of the survival threshold for assignment of classes led to very different prediction results. CONCLUSIONS 1) Different performance metrics for evaluation of a survival prediction model may give different conclusions in its discriminatory ability. 2) Evaluation using a high-risk versus low-risk group comparison depends on the selected risk-score threshold; a plot of p-values from all possible thresholds can show the sensitivity of the threshold selection. 3) A randomization test of the significance of Somers' rank correlation can be used for further evaluation of performance of a prediction model. 4) The cross-validated power of survival prediction models decreases as the training and test sets become less balanced.},
author = {Chen, Hung-Chia and Kodell, Ralph L. and Cheng, Kuang Fu and Chen, James J.},
doi = {10.1186/1471-2288-12-102},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2012 - Assessment of performance of survival prediction models for cancer prognosis.pdf:pdf},
isbn = {1471-2288 (Electronic) 1471-2288 (Linking)},
issn = {1471-2288},
journal = {BMC medical research methodology},
keywords = {Area Under Curve,Breast Neoplasms,Breast Neoplasms: diagnosis,Breast Neoplasms: mortality,Computer Simulation,Disease-Free Survival,Female,Humans,Logistic Models,Models,Prognosis,Proportional Hazards Models,ROC Curve,Statistical,Support Vector Machine,Survival Analysis},
number = {1},
pages = {102},
pmid = {22824262},
title = {{Assessment of performance of survival prediction models for cancer prognosis.}},
url = {http://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-12-102 http://www.ncbi.nlm.nih.gov/pubmed/22824262 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3410808},
volume = {12},
year = {2012}
}
@article{Mackin2015,
abstract = {OBJECTIVES The purpose of this study was to determine the significance of interscanner variability in CT image radiomics studies. MATERIALS AND METHODS We compared the radiomics features calculated for non-small cell lung cancer (NSCLC) tumors from 20 patients with those calculated for 17 scans of a specially designed radiomics phantom. The phantom comprised 10 cartridges, each filled with different materials to produce a wide range of radiomics feature values. The scans were acquired using General Electric, Philips, Siemens, and Toshiba scanners from 4 medical centers using their routine thoracic imaging protocol. The radiomics feature studied included the mean and standard deviations of the CT numbers as well as textures derived from the neighborhood gray-tone difference matrix. To quantify the significance of the interscanner variability, we introduced the metric feature noise. To look for patterns in the scans, we performed hierarchical clustering for each cartridge. RESULTS The mean CT numbers for the 17 CT scans of the phantom cartridges spanned from -864 to 652 Hounsfield units compared with a span of -186 to 35 Hounsfield units for the CT scans of the NSCLC tumors, showing that the phantom's dynamic range includes that of the tumors. The interscanner variability of the feature values depended on both the cartridge material and the feature, and the variability was large relative to the interpatient variability in the NSCLC tumors for some features. The feature interscanner noise was greatest for busyness and least for texture strength. Hierarchical clustering produced different clusters of the phantom scans for each cartridge, although there was some consistent clustering by scanner manufacturer. CONCLUSIONS The variability in the values of radiomics features calculated on CT images from different CT scanners can be comparable to the variability in these features found in CT images of NSCLC tumors. These interscanner differences should be considered, and their effects should be minimized in future radiomics studies.},
author = {Mackin, Dennis and Fave, Xenia and Zhang, Lifei and Fried, David V. and Yang, Jinzhong and Taylor, Brian and Rodriguez-Rivera, Edgardo and Dodge, Cristina and Jones, Aaron Kyle and Court, Laurence E.},
doi = {10.1097/RLI.0000000000000180},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mackin et al. - 2015 - Measuring Computed Tomography Scanner Variability of Radiomics Features.pdf:pdf},
isbn = {0000000000000},
issn = {1536-0210},
journal = {Investigative radiology},
keywords = {computed,ct,image features,image texture,radiomics},
month = {nov},
number = {11},
pages = {757--65},
pmid = {26115366},
title = {{Measuring Computed Tomography Scanner Variability of Radiomics Features.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26115366 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4598251},
volume = {50},
year = {2015}
}
@article{Fried2016,
abstract = {PURPOSE To determine whether quantitative imaging features from pretreatment positron emission tomography (PET) can enhance patient overall survival risk stratification beyond what can be achieved with conventional prognostic factors in patients with stage III non-small cell lung cancer (NSCLC). MATERIALS AND METHODS The institutional review board approved this retrospective chart review study and waived the requirement to obtain informed consent. The authors retrospectively identified 195 patients with stage III NSCLC treated definitively with radiation therapy between January 2008 and January 2013. All patients underwent pretreatment PET/computed tomography before treatment. Conventional PET metrics, along with histogram, shape and volume, and co-occurrence matrix features, were extracted. Linear predictors of overall survival were developed from leave-one-out cross-validation. Predictive Kaplan-Meier curves were used to compare the linear predictors with both quantitative imaging features and conventional prognostic factors to those generated with conventional prognostic factors alone. The Harrell concordance index was used to quantify the discriminatory power of the linear predictors for survival differences of at least 0, 6, 12, 18, and 24 months. Models were generated with features present in more than 50{\%} of the cross-validation folds. RESULTS Linear predictors of overall survival generated with both quantitative imaging features and conventional prognostic factors demonstrated improved risk stratification compared with those generated with conventional prognostic factors alone in terms of log-rank statistic (P = .18 vs P = .0001, respectively) and concordance index (0.62 vs 0.58, respectively). The use of quantitative imaging features selected during cross-validation improved the model using conventional prognostic factors alone (P = .007). Disease solidity and primary tumor energy from the co-occurrence matrix were found to be selected in all folds of cross-validation. CONCLUSION Pretreatment PET features were associated with overall survival when adjusting for conventional prognostic factors in patients with stage III NSCLC.},
author = {Fried, David V. and Mawlawi, Osama and Zhang, Lifei and Fave, Xenia and Zhou, Shouhao and Ibbott, Geoffrey and Liao, Zhongxing and Court, Laurence E.},
doi = {10.1148/radiol.2015142920},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fried et al. - 2016 - Stage III Non-Small Cell Lung Cancer Prognostic Value of FDG PET Quantitative Imaging Features Combined with Clini.pdf:pdf},
issn = {1527-1315},
journal = {Radiology},
month = {jan},
number = {1},
pages = {214--22},
pmid = {26176655},
title = {{Stage III Non-Small Cell Lung Cancer: Prognostic Value of FDG PET Quantitative Imaging Features Combined with Clinical Prognostic Factors.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26176655 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4699494},
volume = {278},
year = {2016}
}
@article{Dupuy2007,
abstract = {BACKGROUND Both the validity and the reproducibility of microarray-based clinical research have been challenged. There is a need for critical review of the statistical analysis and reporting in published microarray studies that focus on cancer-related clinical outcomes. METHODS Studies published through 2004 in which microarray-based gene expression profiles were analyzed for their relation to a clinical cancer outcome were identified through a Medline search followed by hand screening of abstracts and full text articles. Studies that were eligible for our analysis addressed one or more outcomes that were either an event occurring during follow-up, such as death or relapse, or a therapeutic response. We recorded descriptive characteristics for all the selected studies. A critical review of outcome-related statistical analyses was undertaken for the articles published in 2004. RESULTS Ninety studies were identified, and their descriptive characteristics are presented. Sixty-eight (76{\%}) were published in journals of impact factor greater than 6. A detailed account of the 42 studies (47{\%}) published in 2004 is reported. Twenty-one (50{\%}) of them contained at least one of the following three basic flaws: 1) in outcome-related gene finding, an unstated, unclear, or inadequate control for multiple testing; 2) in class discovery, a spurious claim of correlation between clusters and clinical outcome, made after clustering samples using a selection of outcome-related differentially expressed genes; or 3) in supervised prediction, a biased estimation of the prediction accuracy through an incorrect cross-validation procedure. CONCLUSIONS The most common and serious mistakes and misunderstandings recorded in published studies are described and illustrated. Based on this analysis, a proposal of guidelines for statistical analysis and reporting for clinical microarray studies, presented as a checklist of "Do's and Don'ts," is provided.},
author = {Dupuy, Alain and Simon, Richard M.},
doi = {10.1093/jnci/djk018},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dupuy, Simon - 2007 - Critical review of published microarray studies for cancer outcome and guidelines on statistical analysis and repo.pdf:pdf},
isbn = {1460-2105 (Electronic)$\backslash$r0027-8874 (Linking)},
issn = {1460-2105},
journal = {Journal of the National Cancer Institute},
month = {jan},
number = {2},
pages = {147--57},
pmid = {17227998},
title = {{Critical review of published microarray studies for cancer outcome and guidelines on statistical analysis and reporting.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17227998},
volume = {99},
year = {2007}
}
@article{Hunter2013,
abstract = {PURPOSE For nonsmall cell lung cancer (NSCLC) patients, quantitative image features extracted from computed tomography (CT) images can be used to improve tumor diagnosis, staging, and response assessment. For these findings to be clinically applied, image features need to have high intra and intermachine reproducibility. The objective of this study is to identify CT image features that are reproducible, nonredundant, and informative across multiple machines. METHODS Noncontrast-enhanced, test-retest CT image pairs were obtained from 56 NSCLC patients imaged on three CT machines from two institutions. Two machines ("M1" and "M2") used cine 4D-CT and one machine ("M3") used breath-hold helical 3D-CT. Gross tumor volumes (GTVs) were semiautonomously segmented then pruned by removing voxels with CT numbers less than a prescribed Hounsfield unit (HU) cutoff. Three hundred and twenty eight quantitative image features were extracted from each pruned GTV based on its geometry, intensity histogram, absolute gradient image, co-occurrence matrix, and run-length matrix. For each machine, features with concordance correlation coefficient values greater than 0.90 were considered reproducible. The Dice similarity coefficient (DSC) and the Jaccard index (JI) were used to quantify reproducible feature set agreement between machines. Multimachine reproducible feature sets were created by taking the intersection of individual machine reproducible feature sets. Redundant features were removed through hierarchical clustering based on the average correlation between features across multiple machines. RESULTS For all image types, GTV pruning was found to negatively affect reproducibility (reported results use no HU cutoff). The reproducible feature percentage was highest for average images (M1 = 90.5{\%}, M2 = 94.5{\%}, M1∩M2 = 86.3{\%}), intermediate for end-exhale images (M1 = 75.0{\%}, M2 = 71.0{\%}, M1∩M2 = 52.1{\%}), and lowest for breath-hold images (M3 = 61.0{\%}). Between M1 and M2, the reproducible feature sets generated from end-exhale images were relatively machine-sensitive (DSC = 0.71, JI = 0.55), and the reproducible feature sets generated from average images were relatively machine-insensitive (DSC = 0.90, JI = 0.87). Histograms of feature pair correlation distances indicated that feature redundancy was machine-sensitive and image type sensitive. After hierarchical clustering, 38 features, 28 features, and 33 features were found to be reproducible and nonredundant for M1∩M2 (average images), M1∩M2 (end-exhale images), and M3, respectively. When blinded to the presence of test-retest images, hierarchical clustering showed that the selected features were informative by correctly pairing 55 out of 56 test-retest images using only their reproducible, nonredundant feature set values. CONCLUSIONS Image feature reproducibility and redundancy depended on both the CT machine and the CT image type. For each image type, the authors found a set of cross-machine reproducible, nonredundant, and informative image features that would be useful for future image-based models. Compared to end-exhale 4D-CT and breath-hold 3D-CT, average 4D-CT derived image features showed superior multimachine reproducibility and are the best candidates for clinical correlation.},
author = {Hunter, Luke A. and Krafft, Shane and Stingo, Francesco and Choi, Haesun and Martel, Mary K. and Kry, Stephen F. and Court, Laurence E.},
doi = {10.1118/1.4829514},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hunter et al. - 2013 - High quality machine-robust image features identification in nonsmall cell lung cancer computed tomography images.pdf:pdf},
issn = {0094-2405},
journal = {Medical physics},
keywords = {Carcinoma,Computer-Assisted,Computer-Assisted: methods,Four-Dimensional Computed Tomography,Humans,Image Processing,Lung Neoplasms,Lung Neoplasms: radiography,Non-Small-Cell Lung,Non-Small-Cell Lung: radiography,Reproducibility of Results,Spiral Computed,Tomography,X-Ray Computed},
month = {dec},
number = {12},
pages = {121916},
pmid = {24320527},
title = {{High quality machine-robust image features: identification in nonsmall cell lung cancer computed tomography images.}},
url = {http://scitation.aip.org/content/aapm/journal/medphys/40/12/10.1118/1.4829514 http://www.ncbi.nlm.nih.gov/pubmed/24320527 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4108720},
volume = {40},
year = {2013}
}
@article{Tawfik2012,
abstract = {OBJECTIVES Mixing low- and high-voltage acquisitions of dual-energy CT (DECT) scan using different weighting factors leads to differences in attenuation values and image quality. The aim of this work was to evaluate whether average weighting of DECT acquisitions could improve delineation of head and neck cancer and image quality. MATERIALS AND METHODS Among 60 consecutive patients who underwent DECT scan of the head and neck, 35 patients had positive findings and were included in the study. Images were reconstructed as pure 80 kVp, pure Sn140 kVp, and weighted-average (WA) image datasets from low- and high-voltage acquisitions using 3 different weighting factors (0.3, 0.6, 0.8) incorporating 30{\%}, 60{\%}, 80{\%} from the 80 kVp data, respectively. Lesion contrast-to-noise ratio (CNR), attenuation measurements, and objective noise were compared between different image datasets. Two independent blinded radiologists subjectively rated the overall image quality of each image dataset on a 5-point grading scale comprising lesion delineation, image sharpness, and subjective noise. RESULTS Mean venous and tumor enhancement and muscle attenuation increased stepwise with decreasing tube voltage from Sn140 kVp through 80 kVp. CNR increased significantly from Sn140 kVp to weighting factor 0.3 then to weighting factor 0.6 (P {\textless} 0.0001). The increase in CNR from weighting factor 0.6 to 0.8 then to 80 kVp was nonsignificant (P = 1.00). The 0.6 weighted-average image dataset received the best image quality score by the 2 readers. CONCLUSION Mixing the DE data from the 80 kVp and Sn140 kVp tubes using weighting factor 0.6 (60{\%} from 80 kVp data) could improve lesion CNR and subjective overall image quality (including lesion delineation). This weighting factor was significantly superior to the 0.3 weighting factor which simulates standard 120 kVp acquisition.},
author = {Tawfik, Ahmed M. and Kerl, J. Matthias and Bauer, Ralf W. and Nour-Eldin, Nour-Eldin and Naguib, Nagy N. N. and Vogl, Thomas J. and Mack, Martin G.},
doi = {10.1097/RLI.0b013e31821e3062},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tawfik et al. - 2012 - Dual-energy CT of head and neck cancer average weighting of low- and high-voltage acquisitions to improve lesion.pdf:pdf},
issn = {1536-0210},
journal = {Investigative radiology},
keywords = {Algorithms,Computer-Assisted,Computer-Assisted: methods,Female,Head and Neck Neoplasms,Head and Neck Neoplasms: radiography,Humans,Male,Middle Aged,Radiographic Image Enhancement,Radiographic Image Enhancement: methods,Radiographic Image Interpretation,Reproducibility of Results,Sensitivity and Specificity,Tomography,X-Ray Computed,X-Ray Computed: methods},
month = {may},
number = {5},
pages = {306--11},
pmid = {21577123},
title = {{Dual-energy CT of head and neck cancer: average weighting of low- and high-voltage acquisitions to improve lesion delineation and image quality-initial clinical experience.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21577123},
volume = {47},
year = {2012}
}
@article{Lian2015,
abstract = {In this paper, we investigate ways to learn efficiently from uncertain data using belief functions. In order to extract more knowledge from imperfect and insufficient information and to improve classification accuracy, we propose a supervised learning method composed of a feature selection procedure and a two-step classification strategy. Using training information, the proposed feature selection procedure automatically determines the most informative feature subset by minimizing an objective function. The proposed two-step classification strategy further improves the decision-making accuracy by using complementary information obtained during the classification process. The performance of the proposed method was evaluated on various synthetic and real datasets. A comparison with other classification methods is also presented.},
author = {Lian, Chunfeng and Ruan, Su and Den{\oe}ux, Thierry},
doi = {10.1016/j.patcog.2015.01.019},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lian, Ruan, Den{\oe}ux - 2015 - An evidential classifier based on feature selection and two-step classification strategy.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Belief functions,Classification,Dempster-Shafer theory,Evidence theory,Feature selection,Uncertain data},
month = {jul},
number = {7},
pages = {2318--2327},
title = {{An evidential classifier based on feature selection and two-step classification strategy}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320315000382},
volume = {48},
year = {2015}
}
@article{Fave2015,
abstract = {Several recent studies have demonstrated the potential for quantitative imaging features to classify non-small cell lung cancer (NSCLC) patients as high or low risk. However applying the results from one institution to another has been difficult because of the variations in imaging techniques and feature measurement. Our study was designed to determine the effect of some of these sources of uncertainty on image features extracted from computed tomography (CT) images of non-small cell lung cancer (NSCLC) tumors. CT images from 20 NSCLC patients were obtained for investigating the impact of four sources of uncertainty: Two region of interest (ROI) selection conditions (breathing phase and single-slice vs. whole volume) and two imaging protocol parameters (peak tube voltage and current). Texture values did not vary substantially with the choice of breathing phase; however, almost half (12 out of 28) of the measured textures did change significantly when measured from the average images compared to the end-of-exhale phase. Of the 28 features, 8 showed a significant variation when measured from the largest cross sectional slice compared to the entire tumor, but 14 were correlated to the entire tumor value. While simulating a decrease in tube voltage had a negligible impact on texture features, simulating a decrease in mA resulted in significant changes for 13 of the 23 texture values. Our results suggest that substantial variation exists when textures are measured under different conditions, and thus the development of a texture analysis standard would be beneficial for comparing features between patients and institutions.},
author = {Fave, Xenia and Cook, Molly and Frederick, Amy and Zhang, Lifei and Yang, Jinzhong and Fried, David V. and Stingo, Francesco and Court, Laurence E.},
doi = {10.1016/j.compmedimag.2015.04.006},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fave et al. - 2015 - Preliminary investigation into sources of uncertainty in quantitative imaging features.pdf:pdf},
issn = {1879-0771},
journal = {Computerized medical imaging and graphics},
month = {sep},
pages = {54--61},
pmid = {26004695},
publisher = {Elsevier Ltd},
title = {{Preliminary investigation into sources of uncertainty in quantitative imaging features.}},
url = {http://dx.doi.org/10.1016/j.compmedimag.2015.04.006 http://www.ncbi.nlm.nih.gov/pubmed/26004695},
volume = {44},
year = {2015}
}
@article{Harrell1996,
abstract = {Multivariable regression models are powerful tools that are used frequently in studies of clinical outcomes. These models can use a mixture of categorical and continuous variables and can handle partially observed (censored) responses. However, uncritical application of modelling techniques can result in models that poorly fit the dataset at hand, or, even more likely, inaccurately predict outcomes on new subjects. One must know how to measure qualities of a model's fit in order to avoid poorly fitted or overfitted models. Measurement of predictive accuracy can be difficult for survival time data in the presence of censoring. We discuss an easily interpretable index of predictive discrimination as well as methods for assessing calibration of predicted survival probabilities. Both types of predictive accuracy should be unbiasedly validated using bootstrapping or cross-validation, before using predictions in a new data series. We discuss some of the hazards of poorly fitted and overfitted regression models and present one modelling strategy that avoids many of the problems discussed. The methods described are applicable to all regression models, but are particularly needed for binary, ordinal, and time-to-event outcomes. Methods are illustrated with a survival analysis in prostate cancer using Cox regression.},
author = {Harrell, Frank E. and Lee, Kerry L. and Mark, Daniel B.},
doi = {10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harrell, Lee, Mark - 1996 - Multivariable prognostic models issues in developing models, evaluating assumptions and adequacy, and measur.pdf:pdf},
issn = {0277-6715},
journal = {Statistics in medicine},
month = {feb},
number = {4},
pages = {361--87},
pmid = {8668867},
title = {{Multivariable prognostic models: issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8668867},
volume = {15},
year = {1996}
}
@inproceedings{Caruana2004,
abstract = {We present a method for constructing ensem- bles from libraries of thousands of models. Model libraries are generated using different learning algorithms and parameter settings. Forward stepwise selection is used to add to the ensemble the models that maximize its performance. Ensemble selection allows en- sembles to be optimized to performance met- ric such as accuracy, cross entropy, mean precision, or ROC Area. Experiments with seven test problems and ten metrics demon- strate the benefit of ensemble selection.},
address = {New York, New York, USA},
author = {Caruana, Rich and Niculescu-Mizil, Alexandru and Crew, Geoff and Ksikes, Alex},
booktitle = {Twenty-first international conference on Machine learning - ICML '04},
doi = {10.1145/1015330.1015432},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Caruana et al. - 2004 - Ensemble selection from libraries of models.pdf:pdf},
isbn = {1581138285},
number = {1996},
pages = {18},
publisher = {ACM Press},
title = {{Ensemble selection from libraries of models}},
url = {http://dl.acm.org/citation.cfm?id=1015432 http://portal.acm.org/citation.cfm?doid=1015330.1015432},
year = {2004}
}
@article{Lian2016,
abstract = {As a vital task in cancer therapy, accurately predicting the treatment outcome is valuable for tailoring and adapting a treatment planning. To this end, multi-sources of information (radiomics, clinical characteristics, genomic expressions, etc) gathered before and during treatment are potentially profitable. In this paper, we propose such a prediction system primarily using radiomic features (e.g., texture features) extracted from FDG-PET images. The proposed system includes a feature selection method based on Dempster-Shafer theory, a powerful tool to deal with uncertain and imprecise information. It aims to improve the prediction accuracy, and reduce the imprecision and overlaps between different classes (treatment outcomes) in a selected feature subspace. Considering that training samples are often small-sized and imbalanced in our applications, a data balancing procedure and specified prior knowledge are taken into account to improve the reliability of the selected feature subsets. Finally, the Evidential K-NN (EK-NN) classifier is used with selected features to output prediction results. Our prediction system has been evaluated by synthetic and clinical datasets, consistently showing good performance.},
author = {Lian, Chunfeng and Ruan, Su and Den{\oe}ux, Thierry and Jardin, Fabrice and Vera, Pierre},
doi = {10.1016/j.media.2016.05.007},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lian et al. - 2016 - Selecting radiomic features from FDG-PET images for cancer treatment outcome prediction.pdf:pdf},
issn = {13618415},
journal = {Medical Image Analysis},
month = {aug},
pages = {257--268},
title = {{Selecting radiomic features from FDG-PET images for cancer treatment outcome prediction}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1361841516300366},
volume = {32},
year = {2016}
}
@article{Thibault2014,
abstract = {This paper presents new structural statistical matrices which are gray level size zone matrix (SZM) texture descriptor variants. The SZM is based on the cooccurrences of size/intensity of each flat zone (connected pixels with the same gray level). The first improvement increases the information processed by merging multiple gray-level quantizations and reduces the required parameter numbers. New improved descriptors were especially designed for supervised cell texture classification. They are illustrated thanks to two different databases built from quantitative cell biology. The second alternative characterizes the DNA organization during the mitosis, according to zone intensities radial distribution. The third variant is a matrix structure generalization for the fibrous texture analysis, by changing the intensity/size pair into the length/orientation pair of each region.},
author = {Thibault, Guillaume and Angulo, Jes{\'{u}}s and Meyer, Fernand},
doi = {10.1109/TBME.2013.2284600},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thibault, Angulo, Meyer - 2014 - Advanced statistical matrices for texture characterization application to cell classification.pdf:pdf},
isbn = {9781457713033},
issn = {1558-2531},
journal = {IEEE transactions on bio-medical engineering},
keywords = {Gray-level size zone matrix (SZM),quantitative cytology,structural statistical matrices,texture characterization and classification},
month = {mar},
number = {3},
pages = {630--7},
pmid = {24108747},
title = {{Advanced statistical matrices for texture characterization: application to cell classification.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24108747},
volume = {61},
year = {2014}
}
@article{Suzuki2006,
abstract = {SUMMARY: Pvclust is an add-on package for a statistical software R to assess the uncertainty in hierarchical cluster analysis. Pvclust can be used easily for general statistical problems, such as DNA microarray analysis, to perform the bootstrap analysis of clustering, which has been popular in phylogenetic analysis. Pvclust calculates probability values (p-values) for each cluster using bootstrap resampling techniques. Two types of p-values are available: approximately unbiased (AU) p-value and bootstrap probability (BP) value. Multiscale bootstrap resampling is used for the calculation of AU p-value, which has superiority in bias over BP value calculated by the ordinary bootstrap resampling. In addition the computation time can be enormously decreased with parallel computing option.},
author = {Suzuki, Ryota and Shimodaira, Hidetoshi},
doi = {10.1093/bioinformatics/btl117},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Suzuki, Shimodaira - 2006 - Pvclust An R package for assessing the uncertainty in hierarchical clustering.pdf:pdf},
isbn = {1367-4803 (Print)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {12},
pages = {1540--1542},
pmid = {16595560},
title = {{Pvclust: An R package for assessing the uncertainty in hierarchical clustering}},
volume = {22},
year = {2006}
}
@article{Zupan2000,
abstract = {Machine learning techniques have recently received considerable attention, especially when used for the construction of prediction models from data. Despite their potential advantages over standard statistical methods, like their ability to model non-linear relationships and construct symbolic and interpretable models, their applications to survival analysis are at best rare, primarily because of the difficulty to appropriately handle censored data. In this paper we propose a schema that enables the use of classification methods — including machine learning classifiers — for survival analysis. To appropriately consider the follow-up time and censoring, we propose a technique that, for the patients for which the event did not occur and have short follow-up times, estimates their probability of event and assigns them a distribution of outcome accordingly. Since most machine learning techniques do not deal with outcome distributions, the schema is implemented using weighted examples. To show the utility of the proposed technique, we investigate a particular problem of building prognostic models for prostate cancer recurrence, where the sole prediction of the probability of event (and not its probability dependency on time) is of interest. A case study on preoperative and postoperative prostate cancer recurrence prediction shows that by incorporating this weighting technique the machine learning tools stand beside modern statistical methods and may, by inducing symbolic recurrence models, provide further insight to relationships within the modeled data.},
author = {Zupan, Bla{\v{z}} and Dem{\v{s}}ar, Janez and Kattan, Michael W. and Beck, J. Robert and Bratko, I.},
doi = {10.1016/S0933-3657(00)00053-1},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zupan et al. - 2000 - Machine learning for survival analysis a case study on recurrence of prostate cancer.pdf:pdf},
isbn = {0933-3657},
issn = {09333657},
journal = {Artificial Intelligence in Medicine},
keywords = {Censored data,Data weighting,Machine learning,Outcome prediction after radical prostatectomy,Prognostic models in medicine,Prostate cancer recurrence,Survival analysis},
number = {1},
pages = {59--75},
pmid = {11185421},
title = {{Machine learning for survival analysis: a case study on recurrence of prostate cancer}},
url = {http://www.sciencedirect.com/science/article/pii/S0933365700000531},
volume = {20},
year = {2000}
}
@article{Tolosi2011,
abstract = {Classification and feature selection of genomics or transcriptomics data is often hampered by the large number of features as compared with the small number of samples available. Moreover, features represented by probes that either have similar molecular functions (gene expression analysis) or genomic locations (DNA copy number analysis) are highly correlated. Classical model selection methods such as penalized logistic regression or random forest become unstable in the presence of high feature correlations. Sophisticated penalties such as group Lasso or fused Lasso can force the models to assign similar weights to correlated features and thus improve model stability and interpretability. In this article, we show that the measures of feature relevance corresponding to the above-mentioned methods are biased such that the weights of the features belonging to groups of correlated features decrease as the sizes of the groups increase, which leads to incorrect model interpretation and misleading feature ranking.},
author = {Tolosi, Laura and Lengauer, Thomas},
doi = {10.1093/bioinformatics/btr300},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tolosi, Lengauer - 2011 - Classification with correlated features unreliability of feature ranking and solutions.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = {jul},
number = {14},
pages = {1986--1994},
pmid = {21576180},
title = {{Classification with correlated features: unreliability of feature ranking and solutions}},
url = {http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/btr300},
volume = {27},
year = {2011}
}
@article{Gillies2015,
abstract = {In the past decade, the field of medical image analysis has grown exponentially, with an increased number of pattern recognition tools and an increase in data set sizes. These advances have facilitated the development of processes for high-throughput extraction of quantitative features that result in the conversion of images into mineable data and the subsequent analysis of these data for decision support; this practice is termed radiomics. This is in contrast to the traditional practice of treating medical images as pictures intended solely for visual interpretation. Radiomic data contain first-, second-, and higher-order statistics. These data are combined with other patient data and are mined with sophisticated bioinformatics tools to develop models that may potentially improve diagnostic, prognostic, and predictive accuracy. Because radiomics analyses are intended to be conducted with standard of care images, it is conceivable that conversion of digital images to mineable data will eventually become routine practice. This report describes the process of radiomics, its challenges, and its potential power to facilitate better clinical decision making, particularly in the care of patients with cancer.},
author = {Gillies, Robert J. and Kinahan, Paul E. and Hricak, Hedvig},
doi = {10.1148/radiol.2015151169},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gillies, Kinahan, Hricak - 2015 - Radiomics Images Are More than Pictures, They Are Data.pdf:pdf},
issn = {1527-1315 (Electronic)},
journal = {Radiology},
number = {2},
pages = {151169},
pmid = {26579733},
title = {{Radiomics: Images Are More than Pictures, They Are Data.}},
volume = {278},
year = {2015}
}
@article{Royston2013,
abstract = {BACKGROUND: A prognostic model should not enter clinical practice unless it has been demonstrated that it performs a useful role. External validation denotes evaluation of model performance in a sample independent of that used to develop the model. Unlike for logistic regression models, external validation of Cox models is sparsely treated in the literature. Successful validation of a model means achieving satisfactory discrimination and calibration (prediction accuracy) in the validation sample. Validating Cox models is not straightforward because event probabilities are estimated relative to an unspecified baseline function.$\backslash$n$\backslash$nMETHODS: We describe statistical approaches to external validation of a published Cox model according to the level of published information, specifically (1) the prognostic index only, (2) the prognostic index together with Kaplan-Meier curves for risk groups, and (3) the first two plus the baseline survival curve (the estimated survival function at the mean prognostic index across the sample). The most challenging task, requiring level 3 information, is assessing calibration, for which we suggest a method of approximating the baseline survival function.$\backslash$n$\backslash$nRESULTS: We apply the methods to two comparable datasets in primary breast cancer, treating one as derivation and the other as validation sample. Results are presented for discrimination and calibration. We demonstrate plots of survival probabilities that can assist model evaluation.$\backslash$n$\backslash$nCONCLUSIONS: Our validation methods are applicable to a wide range of prognostic studies and provide researchers with a toolkit for external validation of a published Cox model.},
author = {Royston, Patrick and Altman, Douglas G.},
doi = {10.1186/1471-2288-13-33},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Royston, Altman - 2013 - External validation of a Cox prognostic model principles and methods.pdf:pdf},
isbn = {1471-2288 (Electronic)$\backslash$r1471-2288 (Linking)},
issn = {1471-2288},
journal = {BMC Medical Research Methodology},
keywords = {Adult,Aged,Breast Neoplasms,Breast Neoplasms: diagnosis,Breast Neoplasms: drug therapy,Breast Neoplasms: pathology,Calibration,Calibration: standards,Female,Humans,Kaplan-Meier Estimate,Menopause,Menopause: physiology,Middle Aged,Models,Prognosis,Proportional Hazards Models,Reproducibility of Results,Sensitivity and Specificity,Statistical,Survival Analysis,Time Factors},
pages = {33},
pmid = {23496923},
title = {{External validation of a Cox prognostic model: principles and methods.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3667097{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {13},
year = {2013}
}
@article{VanHouwelingen2000,
abstract = {The problem of assessing the validity and value of prognostic survival models presented in the literature for a particular population for which some data has been collected is discussed. Methods are sketched to perform validation through 'calibration', that is by embedding the literature model in a larger calibration model. This general approach is exemplified for x-year survival probabilities, Cox regression and general non-proportional hazards models. Some comments are made on basic structural changes to the model, described as 'revision'. Finally, general methods are discussed to combine models from different sources. The methods are illustrated with a model for non-Hodgkin's lymphoma validated on a Dutch data set.},
author = {van Houwelingen, Hans C.},
doi = {10.1002/1097-0258(20001230)19:24<3401::AID-SIM554>3.0.CO;2-2},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Houwelingen - 2000 - Validation, calibration, revision and combination of prognostic survival models.pdf:pdf},
isbn = {0277-6715 (Print)$\backslash$r0277-6715 (Linking)},
issn = {02776715},
journal = {Statistics in Medicine},
number = {24},
pages = {3401--3415},
pmid = {11122504},
title = {{Validation, calibration, revision and combination of prognostic survival models}},
volume = {19},
year = {2000}
}
@inproceedings{Kohavi995,
author = {Kohavi, Ron},
booktitle = {International Joint Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kohavi - 995 - A study of cross-validation and bootstrap for accuracy estimation and model selection.pdf:pdf},
number = {2},
pages = {1137--1145},
title = {{A study of cross-validation and bootstrap for accuracy estimation and model selection}},
year = {1995}
}
@article{Vickers2006,
abstract = {Background. Diagnostic and prognostic models are typically evaluated with measures of accuracy that do not address clinical consequences. Decision-analytic techniques allow assessment of clinical outcomes but often require collection of additional information and may be cumbersome to apply to models that yield a continuous result. The authors sought a method for evaluating and comparing prediction models that incorporates clinical consequences, requires only the data set on which the models are tested, and can be applied to models that have either continuous or dichotomous results, Method. The authors describe decision curve analysis, a simple, novel method of evaluating predictive models. They start by assuming that the threshold probability of a disease or event at which a patient would opt for treatment is informative of how the patient weighs the relative harms of a false-positive and a false-negative prediction. This theoretical relationship is then used to derive the net benefit of the model across different threshold probabilities. Plotting net benefit against threshold probability yields the "decision curve." The authors apply the method to models for the prediction of seminal vesicle invasion in prostate cancer patients. Decision curve analysis identified the range of threshold probabilities in which a model was of value, the magnitude of benefit, and which of several models was optimal. Conclusion. Decision curve analysis is a suitable method for evaluating alternative diagnostic and prognostic strategies that has advantages over other commonly used measures and techniques.},
author = {Vickers, Andrew J. and Elkin, Elena B.},
doi = {10.1177/0272989X06295361},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vickers, Elkin - 2006 - Decision Curve Analysis A Novel Method for Evaluating Prediction Models.pdf:pdf},
isbn = {0272-989X},
issn = {0272-989X},
journal = {Medical Decision Making},
month = {nov},
number = {6},
pages = {565--574},
pmid = {17099194},
title = {{Decision Curve Analysis: A Novel Method for Evaluating Prediction Models}},
url = {http://mdm.sagepub.com/cgi/doi/10.1177/0272989X06295361},
volume = {26},
year = {2006}
}
@article{Parker2007,
abstract = {BACKGROUND: When analysing microarray and other small sample size biological datasets, care is needed to avoid various biases. We analyse a form of bias, stratification bias, that can substantially affect analyses using sample-reuse validation techniques and lead to inaccurate results. This bias is due to imperfect stratification of samples in the training and test sets and the dependency between these stratification errors, i.e. the variations in class proportions in the training and test sets are negatively correlated. RESULTS: We show that when estimating the performance of classifiers on low signal datasets (i.e. those which are difficult to classify), which are typical of many prognostic microarray studies, commonly used performance measures can suffer from a substantial negative bias. For error rate this bias is only severe in quite restricted situations, but can be much larger and more frequent when using ranking measures such as the receiver operating characteristic (ROC) curve and area under the ROC (AUC). Substantial biases are shown in simulations and on the van 't Veer breast cancer dataset. The classification error rate can have large negative biases for balanced datasets, whereas the AUC shows substantial pessimistic biases even for imbalanced datasets. In simulation studies using 10-fold cross-validation, AUC values of less than 0.3 can be observed on random datasets rather than the expected 0.5. Further experiments on the van 't Veer breast cancer dataset show these biases exist in practice. CONCLUSION: Stratification bias can substantially affect several performance measures. In computing the AUC, the strategy of pooling the test samples from the various folds of cross-validation can lead to large biases; computing it as the average of per-fold estimates avoids this bias and is thus the recommended approach. As a more general solution applicable to other performance measures, we show that stratified repeated holdout and a modified version of k-fold cross-validation, balanced, stratified cross-validation and balanced leave-one-out cross-validation, avoids the bias. Therefore for model selection and evaluation of microarray and other small biological datasets, these methods should be used and unstratified versions avoided. In particular, the commonly used (unbalanced) leave-one-out cross-validation should not be used to estimate AUC for small datasets.},
author = {Parker, Brian J. and G{\"{u}}nter, Simon and Bedo, Justin},
doi = {10.1186/1471-2105-8-326},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Parker, G{\"{u}}nter, Bedo - 2007 - Stratification bias in low signal microarray studies.pdf:pdf},
isbn = {1471-2105},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {326},
pmid = {17764577},
title = {{Stratification bias in low signal microarray studies}},
url = {http://www.biomedcentral.com/1471-2105/8/326},
volume = {8},
year = {2007}
}
@article{Pencina2011,
abstract = {Appropriate quantification of added usefulness offered by new markers included in risk prediction algorithms is a problem of active research and debate. Standard methods, including statistical significance and c statistic are useful but not sufficient. Net reclassification improvement (NRI) offers a simple intuitive way of quantifying improvement offered by new markers and has been gaining popularity among researchers. However, several aspects of the NRI have not been studied in sufficient detail. In this paper we propose a prospective formulation for the NRI which offers immediate application to survival and competing risk data as well as allows for easy weighting with observed or perceived costs. We address the issue of the number and choice of categories and their impact on NRI. We contrast category-based NRI with one which is category-free and conclude that NRIs cannot be compared across studies unless they are defined in the same manner. We discuss the impact of differing event rates when models are applied to different samples or definitions of events and durations of follow-up vary between studies. We also show how NRI can be applied to case-control data. The concepts presented in the paper are illustrated in a Framingham Heart Study example. In conclusion, NRI can be readily calculated for survival, competing risk, and case-control data, is more objective and comparable across studies using the category-free version, and can include relative costs for classifications. We recommend that researchers clearly define and justify the choices they make when choosing NRI for their application.},
author = {Pencina, Michael J. and D'Agostino, Ralph B. and Steyerberg, Ewout W.},
doi = {10.1002/sim.4085},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pencina, D'Agostino, Steyerberg - 2011 - Extensions of net reclassification improvement calculations to measure usefulness of new biomar.pdf:pdf},
isbn = {1097-0258 (Electronic)$\backslash$n0277-6715 (Linking)},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Biomarker,Discrimination,Model performance,NRI,Risk prediction},
number = {1},
pages = {11--21},
pmid = {21204120},
title = {{Extensions of net reclassification improvement calculations to measure usefulness of new biomarkers}},
volume = {30},
year = {2011}
}
@article{Cook2007,
abstract = {The c statistic, or area under the receiver operating characteristic (ROC) curve, achieved popularity in diagnostic testing, in which the test characteristics of sensitivity and specificity are relevant to discriminating diseased versus nondiseased patients. The c statistic, however, may not be optimal in assessing models that predict future risk or stratify individuals into risk categories. In this setting, calibration is as important to the accurate assessment of risk. For example, a biomarker with an odds ratio of 3 may have little effect on the c statistic, yet an increased level could shift estimated 10-year cardiovascular risk for an individual patient from 8{\%} to 24{\%}, which would lead to different treatment recommendations under current Adult Treatment Panel III guidelines. Accepted risk factors such as lipids, hypertension, and smoking have only marginal impact on the c statistic individually yet lead to more accurate reclassification of large proportions of patients into higher-risk or lower-risk categories. Perfectly calibrated models for complex disease can, in fact, only achieve values for the c statistic well below the theoretical maximum of 1. Use of the c statistic for model selection could thus naively eliminate established risk factors from cardiovascular risk prediction scores. As novel risk factors are discovered, sole reliance on the c statistic to evaluate their utility as risk predictors thus seems ill-advised.},
author = {Cook, Nancy R.},
doi = {10.1161/CIRCULATIONAHA.106.672402},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cook - 2007 - Use and misuse of the receiver operating characteristic curve in risk prediction.pdf:pdf},
isbn = {1524-4539 (Electronic)$\backslash$n0009-7322 (Linking)},
issn = {00097322},
journal = {Circulation},
keywords = {Cardiovascular diseases,Epidemiology,Follow-up studies,Prevention,Risk,Risk factors,Statistics},
number = {7},
pages = {928--935},
pmid = {17309939},
title = {{Use and misuse of the receiver operating characteristic curve in risk prediction}},
volume = {115},
year = {2007}
}
@article{Sun1983,
abstract = {A new approach, neighboring gray level dependence matrix (NGLDM), for texture classification is presented. The major properties of this approach are as follows: (a) texture features can be easily computed; (b) they are essentially invariant under spatial rotation; (c) they are invariant under linear gray level transformation and can be made insensitive to monotonic gray level transformation. These properties have enhanced the practical applications of the texture features. The accuracies of the classification are comparable with those found in the literature. ?? 1983.},
author = {Sun, Chengjun and Wee, William G.},
doi = {10.1016/0734-189X(83)90032-4},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun, Wee - 1983 - Neighboring gray level dependence matrix for texture classification.pdf:pdf},
isbn = {0734-189X},
issn = {0734189X},
journal = {Computer Vision, Graphics, and Image Processing},
month = {sep},
number = {3},
pages = {341--352},
title = {{Neighboring gray level dependence matrix for texture classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0734189X83900324},
volume = {23},
year = {1983}
}
@inproceedings{Airola2010,
author = {Airola, Antti and Pahikkala, Tapio and Waegeman, Willem and de Baets, Bernard and Salakoski, Tapio},
booktitle = {JMLR - Workshop on Machine Learning in Systems Biology},
editor = {Dzeroski, Saso and Geurts, Pierre and Rousu, Juho},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Airola et al. - 2010 - A comparison of AUC estimators in small-sample studies.pdf:pdf},
pages = {3--13},
title = {{A comparison of AUC estimators in small-sample studies}},
url = {https://biblio.ugent.be/publication/860678/file/860679.pdf},
year = {2010}
}
@article{Petrick1999,
abstract = {As an ongoing effort to develop a computer aid for detection of masses on mammograms, we recently designed an object-based region-growing technique to improve mass segmentation. This segmentation method utilizes the density-weighted contrast enhancement (DWCE) filter as a pre-processing step. The DWCE filter adaptively enhances the contrast between the breast structures and the background. Object-based region growing was then applied to each of the identified structures. The region-growing technique uses gray-scale and gradient information to adjust the initial object borders and to reduce merging between adjacent or overlapping structures. Each object is then classified as a breast mass or normal tissue based on extracted morphological and texture features. In this study we evaluated the sensitivity of this combined segmentation scheme and its ability to reduce false positive (FP) detections on a data set of 253 digitized mammograms, each of which contained a biopsy-proven breast mass. It was found that the segmentation scheme detected 98{\%} of the 253 biopsy-proven breast masses in our data set. After final FP reduction, the detection resulted in 4.2 FP per image at a 90{\%} true positive (TP) fraction and 2.0 FPs per image at an 80{\%} TP fraction. The combined DWCE and object-based region growing technique increased the initial detection sensitivity, reduced merging between neighboring structures, and reduced the number of FP detections in our automated breast mass detection scheme.},
author = {Petrick, Nicholas and Chan, Heang-Ping and Sahiner, Berkman and Helvie, Mark A.},
doi = {10.1118/1.598658},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Petrick et al. - 1999 - Combined adaptive enhancement and region-growing segmentation of breast masses on digitized mammograms.pdf:pdf},
isbn = {0094-2405 (Print)},
issn = {00942405},
journal = {Medical Physics},
keywords = {breast mass detection,computer-aided diagnosis,density-,digital mammography,region growing,weight contrast enhancement},
number = {8},
pages = {1642},
pmid = {10501064},
title = {{Combined adaptive enhancement and region-growing segmentation of breast masses on digitized mammograms}},
url = {http://scitation.aip.org/content/aapm/journal/medphys/26/8/10.1118/1.598658},
volume = {26},
year = {1999}
}
@article{Kilday1993,
author = {Kilday, Judy and Palmieri, Francesco and Fox, Martin D.},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kilday, Palmieri, Fox - 1993 - Classifying mammographic lesion using computerized image analysis.pdf:pdf},
journal = {IEEE Transactions on Medical Imaging},
number = {4},
pages = {664--669},
title = {{Classifying mammographic lesion using computerized image analysis}},
volume = {12},
year = {1993}
}
@article{Hand2001,
abstract = {The area under the ROC curve, or the equivalent Gini index, is a widely used measure of performance of supervised classification rules. It has the attractive property that it side-steps the need to specify the costs of the different kinds of misclassification. However, the simple form is only applicable to the case of two classes. We extend the definition to the case of more than two classes by averaging pairwise comparisons. This measure reduces to the standard form in the two class case. We compare its properties with the standard measure of proportion correct and an alternative definition of proportion correct based on pairwise comparison of classes for a simple artificial case and illustrate its application on eight data sets. On the data sets we examined, the measures produced similar, but not identical results, reflecting the different aspects of performance that they were measuring. Like the area under the ROC curve, the measure we propose is useful in those many situations where it is impossible to give costs for the different kinds of misclassification.},
author = {Hand, David J. and Till, Robert J.},
doi = {10.1023/A:1010920819831},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hand, Till - 2001 - A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {AUC,Error rate,Gini index,ROC curve,Receiver operating characteristic},
number = {2},
pages = {171--186},
title = {{A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems}},
volume = {45},
year = {2001}
}
@incollection{Xie2008,
author = {Xie, Xianghua and Mirmehdi, Majid},
booktitle = {World Scientific Review Volume},
chapter = {13},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie, Mirmehdi - 2008 - A Galaxy of Texture Features.pdf:pdf},
title = {{A Galaxy of Texture Features}},
year = {2008}
}
@article{Hofheinz2014,
abstract = {PURPOSE: In a previous study, we demonstrated the first evidence that the asphericity (ASP) of pretherapeutic FDG uptake in the primary tumor provides independent prognostic information in patients with head and neck cancer. The aim of this work was to confirm these results in an independent patient group examined at a different site. METHODS: FDG-PET/CT was performed in 37 patients. The primary tumor was delineated by an automatic algorithm based on adaptive thresholding. For the resulting ROIs, the metabolically active part of the tumor (MTV), SUVmax, SUVmean, total lesion glycolysis (TLG) and ASP were computed. Univariate Cox regression with respect to progression free survival (PFS) and overall survival (OS) was performed. For survival analysis, patients were divided in groups of high and low risk according to the parameter cut-offs defined in our previous work. In a second step, the cut-offs were adjusted to the present data. Univariate and multivariate Cox regression was performed for the pooled data consisting of the current and the previously described patient group (N = 68). In multivariate Cox regression, clinically relevant parameters were included. RESULTS: Univariate Cox regression using the previously published cut-off values revealed TLG (hazard ratio (HR) = 3) and ASP (HR = 3) as significant predictors for PFS. For OS MTV (HR = 2.7) and ASP (HR = 5.9) were significant predictors. Using the adjusted cutoffs MTV (HR = 2.9/3.3), TLG (HR = 3.1/3.3) and ASP (HR = 3.1/5.9) were prognostic for PFS/OS. In the pooled data, multivariate Cox regression revealed a significant prognostic value with respect to PFS/OS for MTV (HR = 2.3/2.1), SUVmax (HR = 2.1/2.5), TLG (HR = 3.5/3.6), and ASP (HR = 3.4/4.4). CONCLUSIONS: Our results confirm the independent prognostic value of ASP of the pretherapeutic FDG uptake in the primary tumor in patients with head and neck cancer. Moreover, these results demonstrate that ASP can be determined unambiguously across different sites.},
author = {Hofheinz, Frank and Lougovski, Alexandr and Z{\"{o}}phel, Klaus and Hentschel, Maria and Steffen, Ingo G. and Apostolova, Ivayla and Wedel, Florian and Buchert, Ralph and Baumann, Michael and Brenner, Winfried and Kotzerke, J{\"{o}}rg and van den Hoff, J{\"{o}}rg},
doi = {10.1007/s00259-014-2953-x},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hofheinz et al. - 2014 - Increased evidence for the prognostic value of primary tumor asphericity in pretherapeutic FDG PET for risk str.pdf:pdf},
isbn = {1619-7089 (Electronic) 1619-7070 (Linking)},
issn = {16197089},
journal = {European Journal of Nuclear Medicine and Molecular Imaging},
keywords = {Asphericity,FDG,Head and neck cancer,PET,Prognostic value,Tumor heterogeneity},
number = {3},
pages = {429--437},
pmid = {25416633},
title = {{Increased evidence for the prognostic value of primary tumor asphericity in pretherapeutic FDG PET for risk stratification in patients with head and neck cancer}},
volume = {42},
year = {2014}
}
@article{Wang2016,
abstract = {Biological networks provide additional information for the analysis of human diseases, beyond the traditional analysis that focuses on single variables. Gaussian graphical model (GGM), a probability model that characterizes the conditional dependence structure of a set of random variables by a graph, has wide applications in the analysis of biological networks, such as inferring interaction or comparing differential networks. However, existing approaches are either not statistically rigorous or are inefficient for high-dimensional data that include tens of thousands of variables for making inference. In this study, we propose an efficient algorithm to implement the estimation of GGM and obtain p-value and confidence interval for each edge in the graph, based on a recent proposal by Ren et al., 2015. Through simulation studies, we demonstrate that the algorithm is faster by several orders of magnitude than the current implemented algorithm for Ren et al. without losing any accuracy. Then, we apply our algorithm to two real data sets: transcriptomic data from a study of childhood asthma and proteomic data from a study of Alzheimer's disease. We estimate the global gene or protein interaction networks for the disease and healthy samples. The resulting networks reveal interesting interactions and the differential networks between cases and controls show functional relevance to the diseases. In conclusion, we provide a computationally fast algorithm to implement a statistically sound procedure for constructing Gaussian graphical model and making inference with high-dimensional biological data. The algorithm has been implemented in an R package named "FastGGM".},
author = {Wang, Ting and Ren, Zhao and Ding, Ying and Fang, Zhou and Sun, Zhe and MacDonald, Matthew L. and Sweet, Robert A. and Wang, Jieru and Chen, Wei},
doi = {10.1371/journal.pcbi.1004755},
editor = {Listgarten, Jennifer},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2016 - FastGGM An Efficient Algorithm for the Inference of Gaussian Graphical Model in Biological Networks.pdf:pdf},
issn = {1553-7358},
journal = {PLOS Computational Biology},
month = {feb},
number = {2},
pages = {e1004755},
pmid = {26872036},
title = {{FastGGM: An Efficient Algorithm for the Inference of Gaussian Graphical Model in Biological Networks}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1004755},
volume = {12},
year = {2016}
}
@article{Liu2009,
abstract = {Microarray data has a high dimension of variables but available datasets usually have only a small number of samples, thereby making the study of such datasets interesting and challenging. In the task of analyzing microarray data for the purpose of, e.g., predicting gene-disease association, feature selection is very important because it provides a way to handle the high dimensionality by exploiting information redundancy induced by associations among genetic markers. Judicious feature selection in microarray data analysis can result in significant reduction of cost while maintaining or improving the classification or prediction accuracy of learning machines that are employed to sort out the datasets. In this paper, we propose a gene selection method called Recursive Feature Addition (RFA), which combines supervised learning and statistical similarity measures. We compare our method with the following gene selection methods: Support Vector Machine Recursive Feature Elimination (SVMRFE), Leave-One-Out Calculation Sequential Forward Selection (LOOCSFS), Gradient based Leave-one-out Gene Selection (GLGS). To evaluate the performance of these gene selection methods, we employ several popular learning classifiers on the MicroArray Quality Control phase II on predictive modeling (MAQC-II) breast cancer dataset and the MAQC-II multiple myeloma dataset. Experimental results show that gene selection is strictly paired with learning classifier. Overall, our approach outperforms other compared methods. The biological functional analysis based on the MAQC-II breast cancer dataset convinced us to apply our method for phenotype prediction. Additionally, learning classifiers also play important roles in the classification of microarray data and our experimental results indicate that the Nearest Mean Scale Classifier (NMSC) is a good choice due to its prediction reliability and its stability across the three performance measurements: Testing accuracy, MCC values, and AUC errors.},
author = {Liu, Qingzhong and Sung, Andrew H. and Chen, Zhongxue and Liu, Jianzhong and Huang, Xudong and Deng, Youping},
doi = {10.1371/journal.pone.0008250},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2009 - Feature selection and classification of MAQC-II breast cancer and multiple myeloma microarray gene expression data.pdf:pdf},
issn = {1932-6203},
journal = {PLoS ONE},
number = {12},
pages = {e8250},
pmid = {20011240},
title = {{Feature selection and classification of MAQC-II breast cancer and multiple myeloma microarray gene expression data.}},
volume = {4},
year = {2009}
}
@article{Parmar2015a,
abstract = {Radiomics extracts and mines large number of medical imaging features quantifying tumor phenotypic characteristics. Highly accurate and reliable machine-learning approaches can drive the success of radiomic applications in clinical care. In this radiomic study, fourteen feature selection methods and twelve classification methods were examined in terms of their performance and stability for predicting overall survival. A total of 440 radiomic features were extracted from pre-treatment computed tomography (CT) images of 464 lung cancer patients. To ensure the unbiased evaluation of different machine-learning methods, publicly available implementations along with reported parameter configurations were used. Furthermore, we used two independent radiomic cohorts for training (n = 310 patients) and validation (n = 154 patients). We identified that Wilcoxon test based feature selection method WLCX (stability = 0.84 ± 0.05, AUC = 0.65 ± 0.02) and a classification method random forest RF (RSD = 3.52{\%}, AUC = 0.66 ± 0.03) had highest prognostic performance with high stability against data perturbation. Our variability analysis indicated that the choice of classification method is the most dominant source of performance variation (34.21{\%} of total variance). Identification of optimal machine-learning methods for radiomic applications is a crucial step towards stable and clinically relevant radiomic biomarkers, providing a non-invasive way of quantifying and monitoring tumor-phenotypic characteristics in clinical practice.},
author = {Parmar, Chintan and Grossmann, Patrick and Bussink, Johan and Lambin, Philippe and Aerts, Hugo J. W. L.},
doi = {10.1038/srep13087},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Parmar et al. - 2015 - Machine Learning methods for Quantitative Radiomic Biomarkers.pdf:pdf;:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Parmar et al. - 2015 - Machine Learning methods for Quantitative Radiomic Biomarkers(2).pdf:pdf},
issn = {2045-2322},
journal = {Scientific reports},
keywords = {BREAST-CANCER,CELL LUNG-CANCER,CHEMOTHERAPY,CLASSIFICATION,FEATURE-SELECTION,GLIOBLASTOMA,PREDICTION,SURVIVAL,TEXTURAL FEATURES,TUMOR},
pages = {13087},
pmid = {26278466},
publisher = {Nature Publishing Group},
title = {{Machine Learning methods for Quantitative Radiomic Biomarkers.}},
url = {http://apps.webofknowledge.com.myaccess.library.utoronto.ca/full{\_}record.do?product=UA{\&}search{\_}mode=GeneralSearch{\&}qid=6{\&}SID=1Dwk2hTMrAk8FpjVhyn{\&}page=11{\&}doc=102 http://www.nature.com/doifinder/10.1038/srep13087},
volume = {5},
year = {2015}
}
@article{Yang1999,
abstract = {Abstract A feature /input selection method is proposed based on joint mutual information . The new method is better than the existing methods based on mutual information in eliminating redundancy in the inputs. It is applied in a real world application to nd 2-D viewing ... $\backslash$n},
author = {Yang, Howard Hua and Moody, John},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Moody - 1999 - Feature selection based on joint mutual information.pdf:pdf},
journal = {Proceedings of International ICSC Symposium on Advances in Intelligent Data Analysis},
keywords = {classi cation,feature selection,joint mutual information,visualization},
pages = {22--25},
title = {{Feature selection based on joint mutual information}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.41.4424{\&}rep=rep1{\&}type=pdf{\%}5Cnpapers2://publication/uuid/E0508640-D7DF-41A9-BFA0-794AE77B3E0F},
year = {1999}
}
@article{Baldi2000a,
abstract = {We provide a unified overview of methods that currently are widely used to assess the accuracy of prediction algorithms, from raw percentages, quadratic error measures and other distances, and correlation coefficients, and to information theoretic measures such as relative entropy and mutual information. We briefly discuss the advantages and disadvantages of each approach. For classification tasks, we derive new learning algorithms for the design of prediction systems by directly optimising the correlation coefficient. We observe and prove several results relating sensitivity and specificity of optimal systems. While the principles are general, we illustrate the applicability on specific problems such as protein secondary structure and signal peptide prediction.},
author = {Baldi, Pierre and Brunak, S{\o}ren and Chauvin, Yves and Andersen, Claus A. F. and Nielsen, Henrik},
doi = {10.1093/bioinformatics/16.5.412},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baldi et al. - 2000 - Assessing the accuracy of prediction algorithms for classification an overview.pdf:pdf},
isbn = {1367-4803 (Print)},
issn = {1367-4803},
journal = {Bioinformatics},
number = {5},
pages = {412--424},
pmid = {10871264},
title = {{Assessing the accuracy of prediction algorithms for classification: an overview.}},
volume = {16},
year = {2000}
}
@misc{Lozano2010,
author = {Lozano, Jose A. and Santaf{\'{e}}, Guzm{\'{a}}n and Inza, I{\~{n}}aki},
booktitle = {Machine Learning},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lozano, Santaf{\'{e}}, Inza - 2010 - Classifier performance evaluation and comparison Outline of the Tutorial.pdf:pdf},
number = {Icmla},
title = {{Classifier performance evaluation and comparison Outline of the Tutorial}},
year = {2010}
}
@article{Robnik-Sikonja2003,
author = {Robnik-{\v{S}}ikonja, Marko and Kononenko, Igor},
doi = {10.1023/A:1025667309714},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Robnik-{\v{S}}ikonja, Kononenko - 2003 - Theoretical and empirical analysis of ReliefF and RReliefF.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {attribute estimation,classification,feature selection,regression,relief algorithm},
number = {1/2},
pages = {23--69},
title = {{Theoretical and empirical analysis of ReliefF and RReliefF}},
url = {http://link.springer.com/10.1023/A:1025667309714},
volume = {53},
year = {2003}
}
@article{Garcia2013,
abstract = {Discretization is an essential preprocessing technique used in many knowledge discovery and data mining tasks. Its main goal is to transform a set of continuous attributes into discrete ones, by associating categorical values to intervals and thus transforming quantitative data into qualitative data. In this manner, symbolic data mining algorithms can be applied over continuous data and the representation of information is simplified, making it more concise and specific. The literature provides numerous proposals of discretization and some attempts to categorize them into a taxonomy can be found. However, in previous papers, there is a lack of consensus in the definition of the properties and no formal categorization has been established yet, which may be confusing for practitioners. Furthermore, only a small set of discretizers have been widely considered, while many other methods have gone unnoticed. With the intention of alleviating these problems, this paper provides a survey of discretization methods proposed in the literature from a theoretical and empirical perspective. From the theoretical perspective, we develop a taxonomy based on the main properties pointed out in previous research, unifying the notation and including all the known methods up to date. Empirically, we conduct an experimental study in supervised classification involving the most representative and newest discretizers, different types of classifiers, and a large number of data sets. The results of their performances measured in terms of accuracy, number of intervals, and inconsistency have been verified by means of nonparametric statistical tests. Additionally, a set of discretizers are highlighted as the best performing ones.},
author = {Garc{\'{i}}a, Salvador and Luengo, Juli{\'{a}}n and Sáez, José Antonio and López, Victoria and Herrera, Francisco},
doi = {10.1109/TKDE.2012.35},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garc{\'{i}}a et al. - 2013 - A Survey of Discretization Techniques Taxonomy and Empirical Analysis in Supervised Learning.pdf:pdf},
isbn = {1041-4347},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Discretization,classification,continuous attributes,data mining,data preprocessing,decision trees,taxonomy},
month = {apr},
number = {4},
pages = {734--750},
title = {{A Survey of Discretization Techniques: Taxonomy and Empirical Analysis in Supervised Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6152258},
volume = {25},
year = {2013}
}
@article{Kononenko1997,
abstract = {. Current inductive machine learning algorithms typically use greedy search with limited lookahead. This prevents them to detect significant conditional dependencies between the attributes that describe training objects. Instead of myopic impurity functions and lookahead, we propose to use RELIEFF, an extension of RELIEF developed by Kira and Rendell 10, 11, for heuristic guidance of inductive learning algorithms. We have reimplemented Assistant, a system for top down induction of decision...},
author = {Kononenko, Igor and {\v{S}}imec, E. and Robnik-{\v{S}}ikonja, Marko},
doi = {10.1023/A:1008280620621},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kononenko, {\v{S}}imec, Robnik-{\v{S}}ikonja - 1997 - Overcoming the myopia of inductive learning algorithms with RELIEFF.pdf:pdf},
isbn = {0924-669X},
issn = {0924-669X},
journal = {Applied Intelligence},
keywords = {and the square of,ber of iterations in,empirical eval-,estimating attributes,impurity function,instances,learning from examples,num-,reduced by limiting the,relief,relieff,the number of training,this can be further,while the original},
number = {1},
pages = {39--55},
title = {{Overcoming the myopia of inductive learning algorithms with RELIEFF}},
url = {http://www.springerlink.com/index/W174714344273004.pdf{\%}5Cnhttp://link.springer.com/article/10.1023/A:1008280620621},
volume = {7},
year = {1997}
}
@article{Dasarathy1991,
abstract = {Previous run length based texture analysis studies have mostly relied upon the use of run length or gray level distributions of the number of runs for characterizing the textures of images. In this study, some new joint run length-gray level distributions are proposed which offer additional insight into the image characterization problem and serve as effective features for a texturebased classification of images. Experimental evidence is offered to demonstrate the utilitarian value of these new concepts.},
author = {Dasarathy, Belur V. and Holder, Edwin B.},
doi = {10.1016/0167-8655(91)80014-2},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dasarathy, Holder - 1991 - Image characterizations based on joint gray level—run length distributions.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {gray level distributions,image characterization,run length distributions,textural features},
number = {8},
pages = {497--502},
title = {{Image characterizations based on joint gray level-run length distributions}},
volume = {12},
year = {1991}
}
@inproceedings{Ding2002,
address = {New York, New York, USA},
author = {Ding, Chris H. Q.},
booktitle = {Proceedings of the sixth annual international conference on Computational biology - RECOMB '02},
doi = {10.1145/565196.565212},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding - 2002 - Analysis of gene expression profiles.pdf:pdf},
isbn = {1581134983},
pages = {127--136},
publisher = {ACM Press},
title = {{Analysis of gene expression profiles}},
url = {http://portal.acm.org/citation.cfm?doid=565196.565212},
year = {2002}
}
@inproceedings{Kononenko1995,
address = {Montreal},
author = {Kononenko, Igor},
booktitle = {Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kononenko - 1995 - On Biases in Estimating Multi-Valued Attributes.pdf:pdf},
pages = {1034--1040},
title = {{On Biases in Estimating Multi-Valued Attributes}},
year = {1995}
}
@article{Jurman2012,
abstract = {We show that the Confusion Entropy, a measure of performance in multiclass problems has a strong (monotone) relation with the multiclass generalization of a classical metric, the Matthews Correlation Coefficient. Analytical results are provided for the limit cases of general no-information (n-face dice rolling) of the binary classification. Computational evidence supports the claim in the general case.},
archivePrefix = {arXiv},
arxivId = {1008.2908},
author = {Jurman, Giuseppe and Riccadonna, Samantha and Furlanello, Cesare},
doi = {10.1371/journal.pone.0041882},
eprint = {1008.2908},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jurman, Riccadonna, Furlanello - 2012 - A comparison of MCC and CEN error measures in multi-class prediction.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {8},
pages = {1--8},
pmid = {22905111},
title = {{A comparison of MCC and CEN error measures in multi-class prediction}},
volume = {7},
year = {2012}
}
@article{Haralick1979,
abstract = {In this survey we review the image processing literature on the various approaches and models investigators have used for texture. These include statistical approaches of autocorrelation function, optical transforms, digital transforms, textural edgeness, structural element, gray tone cooccurrence, run lengths, and autoregressive models. We discuss and generalize some structural approaches to texture based on more complex primitives than gray tone. We conclude with some structural-statistical generalizations which apply the statistical techniques to the structural primitives.},
author = {Haralick, Robert M.},
doi = {10.1109/PROC.1979.11328},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haralick - 1979 - Statistical and structural approaches to texture.pdf:pdf},
isbn = {0018-9219 VO - 67},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {Biomedical measurements,Biomedical optical imaging,Data mining,Image analysis,Image texture,Microscopy,Optical sensors,Remote sensing,Satellites,Shape},
number = {5},
pages = {786--804},
title = {{Statistical and structural approaches to texture}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1455597},
volume = {67},
year = {1979}
}
@article{Haralick1973,
abstract = {Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image. This paper describes some easily computable textural features based on graytone spatial dependencies, and illustrates their application in category identification tasks of 3 different kinds of image data: photomicrographs of 5 kinds of sandstones, 1:20,000 panchromatic aerial photographs of 8 land use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing 7 land use categories. The authors use 2 kinds of decision rules: one for which the decision regions are convex polyhedra ( a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into 2 parts, a training set and a test set. Test set identification accuracy is 89{\%} for the photomicrographs, 82{\%} for the aerial photographic imagery, and 83{\%} for the satellite imagery. These results indicate that the easily computable textural features probably have a general use in a wide variety of image classification applications.},
author = {Haralick, Robert M. and Shanmugam, K. and Dinstein, Its'Hak},
doi = {10.1109/TSMC.1973.4309314},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haralick, Shanmugam, Dinstein - 1973 - Textural features for image classification.pdf:pdf},
isbn = {00189472 (ISSN)},
issn = {0018-9472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
month = {nov},
number = {6},
pages = {610--621},
title = {{Textural Features for Image Classification}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0015680481{\&}partnerID=tZOtx3y1 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4309314},
volume = {3},
year = {1973}
}
@inproceedings{Albregtsen2000,
abstract = {We constructed class distance matrices for the gray level run length texture analysis method. For a four-class problem of liver cell nuclei, we found that there exist areas of consistently high values in the class distance matrices. We combined the information from the entries of the normalized run length matrix, based on the class distance matrices, to obtain adaptive features for texture classification. Using this procedure, we extracted only two features, which halved the classification error when compared to the best pair of classical gray level run length matrix features},
author = {Albregtsen, F. and Nielsen, B. and Danielsen, H.E.},
booktitle = {Proceedings 15th International Conference on Pattern Recognition. ICPR-2000},
doi = {10.1109/ICPR.2000.903650},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Albregtsen, Nielsen, Danielsen - 2000 - Adaptive gray level run length features from class distance matrices.pdf:pdf},
isbn = {0-7695-0750-6},
issn = {1051-4651},
keywords = {adaptive features,biology computing,distance matrices,feature extraction,gray level run length texture,image classification,image texture,liver cell nuclei,run length matrix},
pages = {738--741},
publisher = {IEEE Comput. Soc},
title = {{Adaptive gray level run length features from class distance matrices}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=903650},
volume = {3},
year = {2000}
}
@article{Galloway1975,
abstract = {A set of texture features based on gray level run lengths is described. Good classification results are obtained with these features on a set of samples representing nine terrain types.},
author = {Galloway, Mary M.},
doi = {10.1016/S0146-664X(75)80008-6},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Galloway - 1975 - Texture analysis using gray level run lengths.pdf:pdf},
isbn = {0146-664X},
issn = {0146664X},
journal = {Computer Graphics and Image Processing},
number = {2},
pages = {172--179},
title = {{Texture analysis using gray level run lengths}},
url = {http://www.sciencedirect.com/science/article/pii/S0146664X75800086},
volume = {4},
year = {1975}
}
@article{Amadasun1989,
abstract = {Five properties of texture, namely, coarseness, contrast, business, complexity, and texture strength, are given conceptual definitions in terms of spatial changes in intensity. These conceptual definitions are then approximated in computational forms. In comparison with human perceptual measurements, the computational measures have shown good correspondences in the rank ordering of ten natural textures. The extent to which the measures approximate visual perception was investigated in the form of texture similarity measurements. These results were also encouraging, although not as good as in the rank ordering of the textures. The differences may be due to the complex mechanism of human usage of multiple cues. Improved classification results were obtained using the above features as compared with two existing texture analysis techniques. The application of the features in agricultural land-use classification is considered},
author = {Amadasun, Moses and King, Robert},
doi = {10.1109/21.44046},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Amadasun, King - 1989 - Textural features corresponding to textural properties.pdf:pdf},
isbn = {0018-9472 VO  - 19},
issn = {00189472},
journal = {IEEE Transactions on Systems, Man and Cybernetics},
number = {5},
pages = {1264--1273},
title = {{Textural features corresponding to textural properties}},
volume = {19},
year = {1989}
}
@article{Unser1986,
abstract = {The sum and difference of two random variables with same variances are decorrelated and define the principal axes of their associated joint probability function. Therefore, sum and difference histograms are introduced as an alternative to the usual co-occurrence matrices used for texture analysis. Two maximum likelihood texture classifiers are presented depending on the type of object used for texture characterization (sum and difference histograms or some associated global measures). Experimental results indicate that sum and difference histograms used conjointly are nearly as powerful as cooccurrence matrices for texture discrimination. The advantage of the proposed texture analysis method over the conventional spatial gray level dependence method is the decrease in computation time and memory storage.},
author = {Unser, M},
doi = {10.1109/TPAMI.1986.4767760},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unser - 1986 - Sum and difference histograms for texture classification.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {1},
pages = {118--125},
pmid = {21869331},
title = {{Sum and difference histograms for texture classification.}},
volume = {8},
year = {1986}
}
@article{Chu1990,
abstract = {Most of the texture measures based on run lengths use only the lengths of the runs and their distribution. We propose to use the gray value distribution of the runs to define two new features, viz., low gray level run emphasis (LGRE) and high gray level run emphasis (HGRE).},
author = {Chu, A. and Sehgal, C. M. and Greenleaf, J. F.},
doi = {10.1016/0167-8655(90)90112-F},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chu, Sehgal, Greenleaf - 1990 - Use of gray value distribution of run lengths for texture analysis.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {run length,texture measure,tissue characterization},
number = {6},
pages = {415--419},
title = {{Use of gray value distribution of run lengths for texture analysis}},
volume = {11},
year = {1990}
}
@article{Kumar2012,
author = {Kumar, Virendra and Gu, Yuhua and Basu, Satrajit and Berglund, Anders E. and Eschrich, Steven A. and Schabath, Matthew B. and Forster, Kenneth and Aerts, Hugo J. W. L. and Dekker, Andr{\'{e}} and Fenstermacher, David and Goldgof, Dmitry B. and Hall, Lawrence O. and Lambin, Philippe and Balagurunathan, Yoganand and Gatenby, Robert A. and Gillies, Robert J.},
doi = {10.1016/j.mri.2012.06.010},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumar et al. - 2012 - Radiomics the process and the challenges.pdf:pdf},
issn = {0730725X},
journal = {Magnetic Resonance Imaging},
keywords = {Image features,Imaging,Radiomics,Segmentation,Tumor,image features,imaging,radiomics,segmentation,tumor},
number = {9},
pages = {1234--1248},
publisher = {Elsevier Inc.},
title = {{Radiomics: the process and the challenges}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0730725X12002202},
volume = {30},
year = {2012}
}
@article{Shi2010,
author = {Shi, Leming and Campbell, Gregory and Jones, Wendell D. and Campagne, Fabien and Wen, Zhining and Walker, Stephen J. and Su, Zhenqiang and Chu, Tzu-Ming and Goodsaid, Federico M. and Pusztai, Lajos and Shaughnessy, John D. and Oberthuer, Andr{\'{e}} and Thomas, Russell S. and Paules, Richard S. and Fielden, Mark and Barlogie, Bart and Chen, Weijie and Du, Pan and Fischer, Matthias and Furlanello, Cesare and Gallas, Brandon D. and Ge, Xijin and Megherbi, Dalila B. and Symmans, W. Fraser and Wang, May D and Zhang, John and Bitter, Hans and Brors, Benedikt and Bushel, Pierre R. and Bylesjo, Max and Chen, Minjun and Cheng, Jie and Cheng, Jing and Chou, Jeff and Davison, Timothy S. and Delorenzi, Mauro and Deng, Youping and Devanarayan, Viswanath and Dix, David J. and Dopazo, Joaquin and Dorff, Kevin C. and Elloumi, Fathi and Fan, Jianqing and Fan, Shicai and Fan, Xiaohui and Fang, Hong and Gonzaludo, Nina and Hess, Kenneth R. and Hong, Huixiao and Huan, Jun and Irizarry, Rafael A. and Judson, Richard and Juraeva, Dilafruz and Lababidi, Samir and Lambert, Christophe G. and Li, Li and Li, Yanen and Li, Zhen and Lin, Simon M. and Liu, Guozhen and Lobenhofer, Edward K. and Luo, Jun and Luo, Wen and McCall, Matthew N. and Nikolsky, Yuri and Pennello, Gene A. and Perkins, Roger G. and Philip, Reena and Popovici, Vlad and Price, Nathan D. and Qian, Feng and Scherer, Andreas and Shi, Tieliu and Shi, Weiwei and Sung, Jaeyun and Thierry-Mieg, Danielle and Thierry-Mieg, Jean and Thodima, Venkata and Trygg, Johan and Vishnuvajjala, Lakshmi and Wang, Sue Jane and Wu, Jianping and Wu, Yichao and Xie, Qian and Yousef, Waleed A. and Zhang, Liang and Zhang, Xuegong and Zhong, Sheng and Zhou, Yiming and Zhu, Sheng and Arasappan, Dhivya and Bao, Wenjun and Lucas, Anne Bergstrom and Berthold, Frank and Brennan, Richard J. and Buness, Andreas and Catalano, Jennifer G. and Chang, Chang and Chen, Rong and Cheng, Yiyu and Cui, Jian and Czika, Wendy and Demichelis, Francesca and Deng, Xutao and Dosymbekov, Damir and Eils, Roland and Feng, Yang and Fostel, Jennifer and Fulmer-Smentek, Stephanie and Fuscoe, James C. and Gatto, Laurent and Ge, Weigong and Goldstein, Darlene R. and Guo, Li and Halbert, Donald N. and Han, Jing and Harris, Stephen C. and Hatzis, Christos and Herman, Damir and Huang, Jianping and Jensen, Roderick V. and Jiang, Rui and Johnson, Charles D. and Jurman, Giuseppe and Kahlert, Yvonne and Khuder, Sadik A. and Kohl, Matthias and Li, Jianying and Li, Li and Li, Menglong and Li, Quan-Zhen and Li, Shao and Li, Zhiguang and Liu, Jie and Liu, Ying and Liu, Zhichao and Meng, Lu and Madera, Manuel and Martinez-Murillo, Francisco and Medina, Ignacio and Meehan, Joseph and Miclaus, Kelci and Moffitt, Richard A. and Montaner, David and Mukherjee, Piali and Mulligan, George J. and Neville, Padraic and Nikolskaya, Tatiana and Ning, Baitang and Page, Grier P. and Parker, Joel and Parry, R. Mitchell and Peng, Xuejun and Peterson, Ron L. and Phan, John H. and Quanz, Brian and Ren, Yi and Riccadonna, Samantha and Roter, Alan H. and Samuelson, Frank W. and Schumacher, Martin M. and Shambaugh, Joseph D. and Shi, Qiang and Shippy, Richard and Si, Shengzhu and Smalter, Aaron and Sotiriou, Christos and Soukup, Mat and Staedtler, Frank and Steiner, Guido and Stokes, Todd H. and Sun, Qinglan and Tan, Pei-Yi and Tang, Rong and Tezak, Zivana and Thorn, Brett and Tsyganova, Marina and Turpaz, Yaron and Vega, Silvia C and Visintainer, Roberto and von Frese, Juergen and Wang, Charles and Wang, Eric and Wang, Junwei and Wang, Wei and Westermann, Frank and Willey, James C. and Woods, Matthew and Wu, Shujian and Xiao, Nianqing and Xu, Joshua and Xu, Lei and Yang, Lun and Zeng, Xiao and Zhang, Jialu and Zhang, Li and Zhang, Min and Zhao, Chen and Puri, Raj K. and Scherf, Uwe and Tong, Weida and Wolfinger, Russell D.},
doi = {10.1038/nbt.1665},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi et al. - 2010 - The MicroArray Quality Control (MAQC)-II study of common practices for the development and validation of microarray-.pdf:pdf},
issn = {1087-0156},
journal = {Nature Biotechnology},
month = {aug},
number = {8},
pages = {827--838},
title = {{The MicroArray Quality Control (MAQC)-II study of common practices for the development and validation of microarray-based predictive models}},
url = {http://www.nature.com/doifinder/10.1038/nbt.1665},
volume = {28},
year = {2010}
}
@article{Grove2015,
abstract = {Two CT features were developed to quantitatively describe lung adenocarcinomas by scoring tumor shape complexity (feature 1: convexity) and intratumor density variation (feature 2: entropy ratio) in routinely obtained diagnostic CT scans. The developed quantitative features were analyzed in two independent cohorts (cohort 1: n = 61; cohort 2: n = 47) of patients diagnosed with primary lung adenocarcinoma, retrospectively curated to include imaging and clinical data. Preoperative chest CTs were segmented semi-automatically. Segmented tumor regions were further subdivided into core and boundary sub-regions, to quantify intensity variations across the tumor. Reproducibility of the features was evaluated in an independent test-retest dataset of 32 patients. The proposed metrics showed high degree of reproducibility in a repeated experiment (concordance, CCC≥0.897; dynamic range, DR≥0.92). Association with overall survival was evaluated by Cox proportional hazard regression, Kaplan-Meier survival curves, and the log-rank test. Both features were associated with overall survival (convexity: p = 0.008; entropy ratio: p = 0.04) in Cohort 1 but not in Cohort 2 (convexity: p = 0.7; entropy ratio: p = 0.8). In both cohorts, these features were found to be descriptive and demonstrated the link between imaging characteristics and patient survival in lung adenocarcinoma.},
author = {Grove, Olya and Berglund, Anders E. and Schabath, Matthew B. and Aerts, Hugo J. W. L. and Dekker, Andr{\'{e}} and Wang, Hua and Rios-Velazquez, Emmanuel and Lambin, Philippe and Gu, Yuhua and Balagurunathan, Yoganand and Eikman, Edward and Gatenby, Robert A. and Eschrich, Steven A. and Gillies, Robert J.},
doi = {10.1371/journal.pone.0118261},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grove et al. - 2015 - Quantitative Computed Tomographic Descriptors Associate Tumor Shape Complexity and Intratumor Heterogeneity with P.pdf:pdf},
issn = {1932-6203},
journal = {PLoS ONE},
number = {3},
pages = {e0118261},
pmid = {25739030},
title = {{Quantitative computed tomographic descriptors associate tumor shape complexity and intratumor heterogeneity with prognosis in lung adenocarcinoma.}},
url = {http://dx.plos.org/10.1371/journal.pone.0118261 http://www.ncbi.nlm.nih.gov/pubmed/25739030 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4349806},
volume = {10},
year = {2015}
}
@article{Lambin2012,
author = {Lambin, Philippe and Rios-Velazquez, Emmanuel and Leijenaar, Ralph T. H. and Carvalho, Sara and van Stiphout, Ruud G. P. M. and Granton, Patrick and Zegers, Catharina M. L. and Gillies, Robert J. and Boellard, Ronald and Dekker, Andr{\'{e}} and Aerts, Hugo J. W. L.},
doi = {10.1016/j.ejca.2011.11.036},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lambin et al. - 2012 - Radiomics Extracting more information from medical images using advanced feature analysis.pdf:pdf},
issn = {09598049},
journal = {European Journal of Cancer},
number = {4},
pages = {441--446},
title = {{Radiomics: Extracting more information from medical images using advanced feature analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0959804911009993},
volume = {48},
year = {2012}
}
@book{Duda2001,
address = {New York, NY},
author = {Duda, Richard O. and Hart, Peter E. and Stork, David G.},
edition = {Second},
isbn = {0471056693},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Pattern Classification}},
year = {2001}
}
@misc{Grove2014,
author = {Grove, Olya and Gillies, Robert J.},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grove, Gillies - 2014 - Quantitative predictors of tumor severity.pdf:pdf},
institution = {H. Lee Moffitt Cancer Center and Research Institute, Inc},
title = {{Quantitative predictors of tumor severity}},
url = {https://www.google.com/patents/US20150356730},
year = {2014}
}
@article{John1995,
author = {John, George H. and Langley, Pat},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/John, Langley - 1995 - Estimating continuous distributions in Bayesian classifiers.pdf:pdf},
isbn = {1-55860-385-9},
journal = {Proceedings of the Eleventh conference on Uncertainty in artificial intelligence},
pages = {338--345},
title = {{Estimating continuous distributions in Bayesian classifiers}},
year = {1995}
}
@article{Hino2015,
abstract = {Estimators for differential entropy are proposed. The estimators are based on the second order expansion of the probability mass around the inspection point with respect to the distance from the point. Simple linear regression is utilized to estimate the values of density function and its second derivative at a point. After estimating the values of the probability density function at each of the given sample points, by taking the empirical average of the negative logarithm of the density estimates, two entropy estimators are derived. Other entropy estimators which directly estimate entropy by linear regression, are also proposed. The proposed four estimators are shown to perform well through numerical experiments for various probability distributions.},
author = {Hino, Hideitsu and Koshijima, Kensuke and Murata, Noboru},
doi = {10.1016/j.csda.2015.03.011},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hino, Koshijima, Murata - 2015 - Non-parametric entropy estimators based on simple linear regression.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics {\&} Data Analysis},
keywords = {Entropy estimation,Non-parametric,Simple linear regression},
pages = {72--84},
publisher = {Elsevier B.V.},
title = {{Non-parametric entropy estimators based on simple linear regression}},
url = {http://www.sciencedirect.com/science/article/pii/S0167947315000791},
volume = {89},
year = {2015}
}
@article{Bennasar2015,
author = {Bennasar, Mohamed and Hicks, Yulia and Setchi, Rossitza},
doi = {10.1016/j.eswa.2015.07.007},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bennasar, Hicks, Setchi - 2015 - Feature selection using Joint Mutual Information Maximisation.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Feature selection,Mutual information,Joint mutual},
number = {22},
pages = {8520--8532},
publisher = {Elsevier Ltd.},
title = {{Feature selection using Joint Mutual Information Maximisation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417415004674},
volume = {42},
year = {2015}
}
@article{Dietterich1990,
abstract = {Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.},
author = {Dietterich, Thomas G.},
doi = {10.1007/3-540-45014-9_1},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dietterich - 1990 - Ensemble Methods in Machine Learning.pdf:pdf},
isbn = {3-540-67704-6},
issn = {0010-4485},
journal = {Multiple Classifier Systems},
number = {1},
pages = {1--15},
title = {{Ensemble Methods in Machine Learning}},
year = {2000}
}
@article{Fernandez-Delgado2014,
abstract = {We evaluate 179 classifers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classi ers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearestneighbors, partial least squares and principal component regression, logistic and multinomial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Matlab, including all the relevant classi ers available today. We use 121 data sets, which represent the whole UCI data base (excluding the large-scale problems) and other own real problems, in order to achieve signi cant conclusions about the classi er behavior, not dependent on the data set collection. The classi ers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1{\%} of the maximum accuracy overcoming 90{\%} in the 84.3{\%} of the data sets. However, the difference is not statistically signi cant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3{\%} of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classi ers (3 out of 5 bests classi ers are RF), followed by SVM (4 classi ers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively).},
author = {Fern{\'{a}}ndez-Delgado, Manuel and Cernadas, Eva and Barro, Sen{\'{e}}n and Amorim, Dinani},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fern{\'{a}}ndez-Delgado et al. - 2014 - Do we Need Hundreds of Classifiers to Solve Real World Classification Problems.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {3133--3181},
title = {{Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?}},
url = {http://jmlr.org/papers/v15/delgado14a.html},
volume = {15},
year = {2014}
}
@article{Kuncheva2007,
abstract = {Sequential forward selection (SFS) is one of the most widely used feature selection procedures. It starts with an empty set and adds one feature at each step. The estimate of the quality of the candidate subsets usually depends on the training/testing split of the data. Therefore different sequences of features may be returned from repeated runs of SFS. A substantial discrepancy between such sequences will signal a problem with the selection. A stability index is proposed here based on cardinality of the intersection and a correction for chance. The experimental results with 10 real data sets indicate that the index can be useful for selecting the final feature subset. If stability is high, then we should return a subset of features based on their total rank across the SFS runs. If stability is low, then it is better to return the feature subset which gave the minimum error across all SFS runs.},
author = {Kuncheva, Ludmilla I.},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuncheva - 2007 - A stability index for feature selection.pdf:pdf},
isbn = {9780889866317},
journal = {Proceedings of the 25th conference on Proceedings of the 25th IASTED International Multi-Conference: artificial intelligence and applications},
keywords = {and result in a,feature selection,features being se-,pattern recognition,quential forward selection,se-,sfs,stability index,very different subset of},
pages = {395},
title = {{A stability index for feature selection}},
url = {papers2://publication/uuid/FC5277DF-B494-4316-8D02-E8CE794BAE37},
year = {2007}
}
@article{Guyon2003,
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Guyon, Isabelle and Elisseeff, Andre},
doi = {10.1023/A:1012487302797},
eprint = {1111.6189v1},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guyon, Elisseeff - 2003 - An Introduction to Variable and Feature Selection.pdf:pdf},
isbn = {0885-6125},
issn = {0003-6951},
journal = {The Journal of Machine Learning Research},
keywords = {bioinformatics,clustering,computational biology,ery,feature selection,filters,gene expression,genomics,information retrieval,information theory,microarray,model selection,pattern discov-,proteomics,qsar,space dimensionality reduction,statistical testing,support vector machines,text classification,variable selection,wrappers},
pages = {1157--1182},
pmid = {21889629},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}
@article{Lafortune2012,
abstract = {In this paper, a highly parallel coupled electromechanical model of the heart is presented and assessed. The parallel-coupled model is thoroughly discussed, with scalability proven up to hundreds of cores. This work focuses on the mechanical part, including the constitutive model (proposing some modifications to pre-existent models), the numerical scheme and the coupling strategy. The model is next assessed through two examples. First, the simulation of a small piece of cardiac tissue is used to introduce the main features of the coupled model and calibrate its parameters against experimental evidence. Then, a more realistic prob- lem is solved using those parameters, with a mesh of the Oxford ventricular rabbit model. The results of both examples demonstrate the capability of the model to run efficiently in hundreds of processors and to reproduce some basic characteristic of cardiac deformation.},
author = {Lafortune, Pierre and Aris, R.},
doi = {10.1002/cnm},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lafortune, Aris - 2012 - Coupled electromechanical model of the heart parallel finite element formulation.pdf:pdf},
isbn = {2040-7947},
issn = {20407939},
journal = {International Journal for Numerical Methods in Biomedical Engineering},
keywords = {Cardiac mechanics,Electrophysiology,Finite element methods,Parallelization},
number = {1},
pages = {72--86},
pmid = {24459034},
title = {{Coupled electromechanical model of the heart: parallel finite element formulation}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/cnm.1494/full},
volume = {28},
year = {2012}
}
@article{Brown2012,
abstract = {We present a unifying framework for information theoretic feature selection, bringing almost two decades of research on heuristic filter criteria under a single theoretical interpretation. This is in response to the question: "what are the implicit statistical assumptions of feature selection criteria based on mutual information?". To answer this, we adopt a different strategy than is usual in the feature selection literature−instead of trying to define a criterion, we derive one, directly from a clearly specified objective function: the conditional likelihood of the training labels. While many hand-designed heuristic criteria try to optimize a definition of feature 'relevancy' and 'redundancy', our approach leads to a probabilistic framework which naturally incorporates these concepts. As a result we can unify the numerous criteria published over the last two decades, and show them to be low-order approximations to the exact (but intractable) optimisation problem. The primary contribution is to show that common heuristics for information based feature selection (including Markov Blanket algorithms as a special case) are approximate iterative maximisers of the conditional likelihood. A large empirical study provides strong evidence to favour certain classes of criteria, in particular those that balance the relative size of the relevancy/redundancy terms. Overall we conclude that the JMI criterion (Yang and Moody, 1999; Meyer et al., 2008) provides the best tradeoff in terms of accuracy, stability, and flexibility with small data samples.},
author = {Brown, Gavin and Pocock, Adam and Zhao, Ming-Jie and Lujan, Mikel},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brown et al. - 2012 - Conditional Likelihood Maximisation A Unifying Framework for Information Theoretic Feature Selection.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {Computational,Information-Theoretic Learning with,Learning/Statistics {\&} Optimisation,Machine Vision,Theory {\&} Algorithms},
number = {1},
pages = {27--66},
title = {{Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection}},
url = {http://eprints.pascal-network.org/archive/00009102/},
volume = {13},
year = {2012}
}
@article{Haury2011,
abstract = {Motivation: Biomarker discovery from high-dimensional data is a crucial problem with enormous applications in biology and medicine. It is also extremely challenging from a statistical viewpoint, but surprisingly few studies have investigated the relative strengths and weaknesses of the plethora of existing feature selection methods. Methods: We compare 32 feature selection methods on 4 public gene expression datasets for breast cancer prognosis, in terms of predictive performance, stability and functional interpretability of the signatures they produce. Results: We observe that the feature selection method has a significant influence on the accuracy, stability and interpretability of signatures. Simple filter methods generally outperform more complex embedded or wrapper methods, and ensemble feature selection has generally no positive effect. Overall a simple Student's t-test seems to provide the best results. Availability: Code and data are publicly available at http://cbio.ensmp.fr/{\~{}}ahaury/.},
archivePrefix = {arXiv},
arxivId = {1101.5008},
author = {Haury, Anne Claire and Gestraud, Pierre and Vert, Jean Philippe},
doi = {10.1371/journal.pone.0028210},
eprint = {1101.5008},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haury, Gestraud, Vert - 2011 - The influence of feature selection methods on accuracy, stability and interpretability of molecular signa.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {12},
pages = {1--12},
pmid = {22205940},
title = {{The influence of feature selection methods on accuracy, stability and interpretability of molecular signatures}},
volume = {6},
year = {2011}
}
@article{Pencina2004,
abstract = {The assessment of the discrimination ability of a survival analysis model is a problem of considerable theoretical interest and important practical applications. This issue is, however, more complex than evaluating the performance of a linear or logistic regression. Several different measures have been proposed in the biostatistical literature. In this paper we investigate the properties of the overall C index introduced by Harrell as a natural extension of the ROC curve area to survival analysis. We develop the overall C index as a parameter describing the performance of a given model applied to the population under consideration and discuss the statistic used as its sample estimate. We discover a relationship between the overall C and the modified Kendall's tau and construct a confidence interval for our measure based on the asymptotic normality of its estimate. Then we investigate via simulations the length and coverage probability of this interval. Finally, we present a real life example evaluating the performance of a Framingham Heart Study model.},
author = {Pencina, Michael J. and D'Agostino, Ralph B.},
doi = {10.1002/sim.1802},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pencina, D'Agostino - 2004 - Overall C as a measure of discrimination in survival analysis model specific population value and confidenc.pdf:pdf},
isbn = {0277-6715},
issn = {0277-6715},
journal = {Statistics in medicine},
keywords = {Cohort Studies,Confidence Intervals,Hawaii,Heart Diseases,Heart Diseases: physiopathology,Humans,Male,ROC Curve,Survival Analysis},
number = {13},
pages = {2109--2123},
pmid = {15211606},
title = {{Overall C as a measure of discrimination in survival analysis: model specific population value and confidence interval estimation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15211606},
volume = {23},
year = {2004}
}
@article{Vallieres2015,
abstract = {This study aims at developing a joint FDG-PET and MRI texture-based model for the early evaluation of lung metastasis risk in soft-tissue sarcomas (STSs). We investigate if the creation of new composite textures from the combination of FDG-PET and MR imaging information could better identify aggressive tumours. Towards this goal, a cohort of 51 patients with histologically proven STSs of the extremities was retrospectively evaluated. All patients had pre-treatment FDG-PET and MRI scans comprised of T1-weighted and T2-weighted fat-suppression sequences (T2FS). Nine non-texture features (SUV metrics and shape features) and forty-one texture features were extracted from the tumour region of separate (FDG-PET, T1 and T2FS) and fused (FDG-PET/T1 and FDG-PET/T2FS) scans. Volume fusion of the FDG-PET and MRI scans was implemented using the wavelet transform. The influence of six different extraction parameters on the predictive value of textures was investigated. The incorporation of features into multivariable models was performed using logistic regression. The multivariable modeling strategy involved imbalance-adjusted bootstrap resampling in the following four steps leading to final prediction model construction: (1) feature set reduction; (2) feature selection; (3) prediction performance estimation; and (4) computation of model coefficients. Univariate analysis showed that the isotropic voxel size at which texture features were extracted had the most impact on predictive value. In multivariable analysis, texture features extracted from fused scans significantly outperformed those from separate scans in terms of lung metastases prediction estimates. The best performance was obtained using a combination of four texture features extracted from FDG-PET/T1 and FDG-PET/T2FS scans. This model reached an area under the receiver-operating characteristic curve of 0.984 ± 0.002, a sensitivity of 0.955 ± 0.006, and a specificity of 0.926 ± 0.004 in bootstrapping evaluations. Ultimately, lung metastasis risk assessment at diagnosis of STSs could improve patient outcomes by allowing better treatment adaptation.},
author = {Valli{\`{e}}res, Martin and Freeman, Carolyn R. and Skamene, Sonia R. and {El Naqa}, Issam},
doi = {10.1088/0031-9155/60/14/5471},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Valli{\`{e}}res et al. - 2015 - A radiomics model from joint FDG-PET and MRI texture features for the prediction of lung metastases in soft-ti.pdf:pdf},
issn = {1361-6560},
journal = {Physics in medicine and biology},
month = {jul},
number = {14},
pages = {5471--96},
pmid = {26119045},
title = {{A radiomics model from joint FDG-PET and MRI texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26119045},
volume = {60},
year = {2015}
}
@article{Yu2008,
abstract = {Many feature selection algorithms have been proposed in the past focusing on improving classification accuracy. In this work, we point out the importance of stable feature selection for knowledge discovery from high-dimensional data, and identify two causes of instability of feature selection algorithms: selection of a minimum subset without redundant features and small sample size. We propose a general framework for stable feature selection which emphasizes both good generalization and stability of feature selection results. The framework identifies dense feature groups based on kernel density estimation and treats features in each dense group as a coherent entity for feature selection. An efficient algorithm DRAGS (Dense Relevant Attribute Group Selector) is developed under this framework. We also introduce a general measure for assessing the stability of feature selection algorithms. Our empirical study based on microarray data verifies that dense feature groups remain stable under random sample hold out, and the DRAGS algorithm is effective in identifying a set of feature groups which exhibit both high classification accuracy and stability.},
author = {Yu, Lei and Ding, Chris H. Q. and Loscalzo, Steven},
doi = {10.1145/1401890.1401986},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu, Ding, Loscalzo - 2008 - Stable feature selection via dense feature groups.pdf:pdf},
isbn = {978-1-60558-193-4},
issn = {1367-4811},
journal = {ACM SIGKDD International Conference on Knowledge Discovery in Data Mining (KDD)},
keywords = {classification,density estimation,feature selection,high-dimensional data,kernel,stability},
pages = {803--811},
title = {{Stable feature selection via dense feature groups}},
url = {http://portal.acm.org/citation.cfm?id=1401986},
year = {2008}
}
@article{Castaldi2011,
author = {Castaldi, Peter J. and Dahabreh, Issa J. and Ioannidis, John P. A.},
doi = {10.1093/bib/bbq073},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Castaldi, Dahabreh, Ioannidis - 2011 - An empirical assessment of validation practices for molecular classifiers.pdf:pdf},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
keywords = {gene expression,genes,predictive medicine,proteomics},
number = {3},
pages = {189--202},
title = {{An empirical assessment of validation practices for molecular classifiers}},
url = {http://bib.oxfordjournals.org/cgi/doi/10.1093/bib/bbq073},
volume = {12},
year = {2011}
}
@article{Aerts,
author = {Aerts, Hugo J. W. L. and Rios-Velazquez, Emmanuel and Leijenaar, Ralph T. H. and Parmar, Chintan and Grossmann, Patrick and Cavalho, Sara and Bussink, Johan and Monshouwer, Ren{\'{e}} and Haibe-Kains, Benjamin and Rietveld, Derek and Hoebers, Frank J. P. and Rietbergen, Michelle M. and Leemans, C. Ren{\'{e}} and Dekker, Andr{\'{e}} and Quackenbush, John and Gillies, Robert J. and Lambin, Philippe},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aerts et al. - Unknown - Supplementary Figures.pdf:pdf},
title = {{Supplementary Figures}}
}
@article{Carlier2015,
author = {Carlier, Thomas and Bailly, Cl{\'{e}}ment},
doi = {10.3389/fmed.2015.00018},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlier, Bailly - 2015 - State-Of-The-Art and Recent Advances in Quantification for Therapeutic Follow-Up in Oncology Using PET.pdf:pdf},
issn = {2296-858X},
journal = {Frontiers in Medicine},
keywords = {PET,follow-up,nuclear medicine,oncology,pet,quanti,quantification},
number = {March},
pages = {1--12},
title = {{State-Of-The-Art and Recent Advances in Quantification for Therapeutic Follow-Up in Oncology Using PET}},
url = {http://journal.frontiersin.org/Article/10.3389/fmed.2015.00018/abstract},
volume = {2},
year = {2015}
}
@article{Aerts2014,
abstract = {Human cancers exhibit strong phenotypic differences that can be visualized noninvasively by medical imaging. Radiomics refers to the comprehensive quantification of tumour phenotypes by applying a large number of quantitative image features. Here we present a radiomic analysis of 440 features quantifying tumour image intensity, shape and texture, which are extracted from computed tomography data of 1,019 patients with lung or head-and-neck cancer. We find that a large number of radiomic features have prognostic power in independent data sets of lung and head-and-neck cancer patients, many of which were not identified as significant before. Radiogenomics analysis reveals that a prognostic radiomic signature, capturing intratumour heterogeneity, is associated with underlying gene-expression patterns. These data suggest that radiomics identifies a general prognostic phenotype existing in both lung and head-and-neck cancer. This may have a clinical impact as imaging is routinely used in clinical practice, providing an unprecedented opportunity to improve decision-support in cancer treatment at low cost.},
author = {Aerts, Hugo J. W. L. and Rios-Velazquez, Emmanuel and Leijenaar, Ralph T. H. and Parmar, Chintan and Grossmann, Patrick and Cavalho, Sara and Bussink, Johan and Monshouwer, Ren{\'{e}} and Haibe-Kains, Benjamin and Rietveld, Derek and Hoebers, Frank J. P. and Rietbergen, Michelle M. and Leemans, C. Ren{\'{e}} and Dekker, Andr{\'{e}} and Quackenbush, John and Gillies, Robert J. and Lambin, Philippe},
doi = {10.1038/ncomms5006},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aerts et al. - 2014 - Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach.pdf:pdf},
isbn = {2041-1723 (Electronic)$\backslash$r2041-1723 (Linking)},
issn = {2041-1723},
journal = {Nature communications},
pages = {4006},
pmid = {24892406},
title = {{Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4059926{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {5},
year = {2014}
}
@article{Saeys2008,
abstract = {Robustness or stability of feature selection techniques is a topic of recent interest, and is an important issue when selected feature subsets are subsequently analysed by domain experts to gain more insight into the problem modelled. In this work, we investigate the use of ensemble feature selection techniques, where multiple feature selection methods are combined to yield more robust results. We show that these techniques show great promise for high-dimensional domains with small sample sizes, and provide more robust feature subsets than a single feature selection technique. In addition, we also investigate the effect of ensemble feature selection techniques on classification performance, giving rise to a new model selection strategy.},
author = {Saeys, Yvan and Abeel, Thomas and Peer, Yves},
doi = {10.1007/978-3-540-87481-2_21},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saeys, Abeel, Peer - 2008 - Robust Feature Selection Using Ensemble Feature Selection Techniques.pdf:pdf},
isbn = {978-3-540-87480-5},
issn = {0302-9743},
journal = {European conference on Machine Learning and Knowledge Discovery in Databases (ECML/PKDD)},
pages = {313--325},
pmid = {19535010},
title = {{Robust Feature Selection Using Ensemble Feature Selection Techniques}},
url = {http://portal.acm.org/citation.cfm?id=1432021{\%}5Cnhttp://www.springerlink.com/content/b00625100k51w7kn},
volume = {5212},
year = {2008}
}
@article{Beirlant1997,
abstract = {An overview is given of the several methods in use for the nonparametric estimation of the differential entropy of a continuous random variable. The properties of various methods are compared. Several applications are given such as tests for goodness-of-fit, parameter estimation, quantization theory and spectral estimation},
author = {Beirlant, Jan and Dudewicz, Edward J. and Gy{\"{o}}rfi, L{\'{a}}szl{\'{o}} and van der Meulen, Edward C.},
doi = {10.1.1.87.5281},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Beirlant et al. - 1997 - Nonparametric Entropy Estimation An Overview.pdf:pdf},
issn = {1055-7490},
journal = {International Journal of Mathematical and Statistical Sciences},
pages = {17--40},
title = {{Nonparametric Entropy Estimation: An Overview}},
url = {http://jimbeck.caltech.edu/summerlectures/references/Entropy estimation.pdf},
volume = {6},
year = {1997}
}
@article{Abeel2010,
abstract = {MOTIVATION: Biomarker discovery is an important topic in biomedical applications of computational biology, including applications such as gene and SNP selection from high-dimensional data. Surprisingly, the stability with respect to sampling variation or robustness of such selection processes has received attention only recently. However, robustness of biomarkers is an important issue, as it may greatly influence subsequent biological validations. In addition, a more robust set of markers may strengthen the confidence of an expert in the results of a selection method.$\backslash$n$\backslash$nRESULTS: Our first contribution is a general framework for the analysis of the robustness of a biomarker selection algorithm. Secondly, we conducted a large-scale analysis of the recently introduced concept of ensemble feature selection, where multiple feature selections are combined in order to increase the robustness of the final set of selected features. We focus on selection methods that are embedded in the estimation of support vector machines (SVMs). SVMs are powerful classification models that have shown state-of-the-art performance on several diagnosis and prognosis tasks on biological data. Their feature selection extensions also offered good results for gene selection tasks. We show that the robustness of SVMs for biomarker discovery can be substantially increased by using ensemble feature selection techniques, while at the same time improving upon classification performances. The proposed methodology is evaluated on four microarray datasets showing increases of up to almost 30{\%} in robustness of the selected biomarkers, along with an improvement of approximately 15{\%} in classification performance. The stability improvement with ensemble methods is particularly noticeable for small signature sizes (a few tens of genes), which is most relevant for the design of a diagnosis or prognosis model from a gene signature.$\backslash$n$\backslash$nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Abeel, Thomas and Helleputte, Thibault and van de Peer, Yves and Dupont, Pierre and Saeys, Yvan},
doi = {10.1093/bioinformatics/btp630},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abeel et al. - 2010 - Robust biomarker identification for cancer diagnosis with ensemble feature selection methods.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {1367-4811},
journal = {Bioinformatics},
keywords = {Algorithms,Biological,Biological: chemistry,Computational Biology,Computational Biology: methods,Neoplasm Proteins,Neoplasm Proteins: analysis,Neoplasms,Neoplasms: diagnosis,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Tumor Markers},
number = {3},
pages = {392--398},
pmid = {19942583},
title = {{Robust biomarker identification for cancer diagnosis with ensemble feature selection methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19942583},
volume = {26},
year = {2010}
}
@article{Trifiletti2015,
author = {Trifiletti, Daniel M. and Showalter, Timothy N.},
doi = {10.3389/fonc.2015.00274},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trifiletti, Showalter - 2015 - Big Data and Comparative Effectiveness Research in Radiation Oncology Synergy and Accelerated Discovery.pdf:pdf},
issn = {2234-943X},
journal = {Frontiers in Oncology},
pmid = {26697409},
title = {{Big Data and Comparative Effectiveness Research in Radiation Oncology: Synergy and Accelerated Discovery}},
url = {http://journal.frontiersin.org/article/10.3389/fonc.2015.00274},
volume = {5},
year = {2015}
}
@article{Parmar2015b,
author = {Parmar, Chintan and Leijenaar, Ralph T. H. and Grossmann, Patrick and Rios-Velazquez, Emmanuel and Bussink, Johan and Rietveld, Derek and Rietbergen, Michelle M. and Haibe-Kains, Benjamin and Lambin, Philippe and Aerts, Hugo J. W. L.},
doi = {10.1038/srep11044},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Parmar et al. - 2015 - Radiomic feature clusters and Prognostic Signatures specific for Lung and Head {\&}amp Neck cancer(2).pdf:pdf},
issn = {2045-2322},
journal = {Scientific Reports},
keywords = {cancer,computational science,corresponding authors,quantitative imaging,radiology,radiomics},
pages = {11044},
publisher = {Nature Publishing Group},
title = {{Radiomic feature clusters and Prognostic Signatures specific for Lung and Head {\&} Neck cancer}},
url = {http://www.nature.com/doifinder/10.1038/srep11044},
volume = {5},
year = {2015}
}
@article{Parmar2015c,
author = {Parmar, Chintan and Grossmann, Patrick and Rietveld, Derek and Rietbergen, Michelle M. and Lambin, Philippe and Aerts, Hugo J. W. L.},
doi = {10.3389/fonc.2015.00272},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Parmar et al. - 2015 - Radiomic Machine Learning Classifiers for Prognostic Biomarkers of Head {\&} Neck Cancer P r o v i s i o n a l.pdf:pdf},
issn = {2234-943X},
title = {{Radiomic Machine Learning Classifiers for Prognostic Biomarkers of Head {\&} Neck Cancer P r o v i s i o n a l}},
year = {2015}
}
@article{Parmar2015,
author = {Parmar, Chintan and Grossmann, Patrick and Bussink, Johan and Lambin, Philippe and Aerts, Hugo J. W. L.},
doi = {10.1038/srep13087},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Parmar et al. - 2015 - Machine Learning methods for Quantitative Radiomic Biomarkers.pdf:pdf},
issn = {2045-2322},
journal = {Scientific Reports},
pages = {13087},
publisher = {Nature Publishing Group},
title = {{Machine Learning methods for Quantitative Radiomic Biomarkers}},
url = {http://www.nature.com/doifinder/10.1038/srep13087},
volume = {5},
year = {2015}
}
@article{Nason1995,
abstract = {Wavelets are of wide potential use in statistical contexts. The basics$\backslash$nof the discrete wavelet transform are reviewed using a filter notation$\backslash$nthat is useful subsequently in the paper. A `stationary wavelet$\backslash$ntransform', where the coefficient sequences are not decimated at$\backslash$neach stage, is described. Two different approaches to the construction$\backslash$nof an inverse of the stationary wavelet transform are set out. The$\backslash$napplication of the stationary wavelet transform as an exploratory$\backslash$nstatistical method is discussed, together with its potential use$\backslash$nin nonparametric regression. A method of local spectral density$\backslash$nestimation is developed. This involves extensions to the wavelet$\backslash$ncontext of standard time series ideas such as the periodogram and$\backslash$nspectrum. The technique is illustrated by its application to data$\backslash$nsets from astronomy and veterinary anatomy.},
author = {Nason, G. P. and Silverman, B. W.},
doi = {10.1007/978-1-4612-2544-7_17},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nason, Silverman - 1995 - The stationary wavelet transform and some statistical applications.pdf:pdf},
isbn = {978-0-387-94564-4},
journal = {Wavelets and statistics},
pages = {281--299},
title = {{The stationary wavelet transform and some statistical applications}},
url = {http://link.springer.com/chapter/10.1007/978-1-4612-2544-7{\_}17},
year = {1995}
}
@article{Lo2015,
author = {Lo, Adeline and Chernoff, Herman and Zheng, Tian and Lo, Shaw-Hwa},
doi = {10.1073/pnas.1518285112},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lo et al. - 2015 - Why significant variables aren't automatically good predictors.pdf:pdf},
isbn = {1518285112},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {45},
pages = {201518285},
pmid = {26504198},
title = {{Why significant variables aren't automatically good predictors}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1518285112},
volume = {112},
year = {2015}
}
@article{Leijenaar2015,
abstract = {BACKGROUND Oropharyngeal squamous cell carcinoma (OPSCC) is one of the fastest growing disease sites of head and neck cancers. A recently described radiomic signature, based exclusively on pre-treatment computed tomography (CT) imaging of the primary tumor volume, was found to be prognostic in independent cohorts of lung and head and neck cancer patients treated in the Netherlands. Here, we further validate this signature in a large and independent North American cohort of OPSCC patients, also considering CT artifacts. METHODS A total of 542 OPSCC patients were included for which we determined the prognostic index (PI) of the radiomic signature. We tested the signature model fit in a Cox regression and assessed model discrimination with Harrell's c-index. Kaplan-Meier survival curves between high and low signature predictions were compared with a log-rank test. Validation was performed in the complete cohort (PMH1) and in the subset of patients without (PMH2) and with (PMH3) visible CT artifacts within the delineated tumor region. RESULTS We identified 267 (49{\%}) patients without and 275 (51{\%}) with visible CT artifacts. The calibration slope ($\beta$) on the PI in a Cox proportional hazards model was 1.27 (H0: $\beta$ = 1, p = 0.152) in the PMH1 (n = 542), 0.855 (H0: $\beta$ = 1, p = 0.524) in the PMH2 (n = 267) and 1.99 (H0: $\beta$ = 1, p = 0.002) in the PMH3 (n = 275) cohort. Harrell's c-index was 0.628 (p = 2.72e-9), 0.634 (p = 2.7e-6) and 0.647 (p = 5.35e-6) for the PMH1, PMH2 and PMH3 cohort, respectively. Kaplan-Meier survival curves were significantly different (p {\textless} 0.05) between high and low radiomic signature model predictions for all cohorts. CONCLUSION Overall, the signature validated well using all CT images as-is, demonstrating a good model fit and preservation of discrimination. Even though CT artifacts were shown to be of influence, the signature had significant prognostic power regardless if patients with CT artifacts were included.},
author = {Leijenaar, Ralph T. H. and Carvalho, Sara and Hoebers, Frank J. P. and Aerts, Hugo J. W. L. and van Elmpt, Wouter J. C. and Huang, Shao Hui and Chan, Biu and Waldron, John N. and O'Sullivan, Brian and Lambin, Philippe},
doi = {10.3109/0284186X.2015.1061214},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Leijenaar et al. - 2015 - External validation of a prognostic CT-based radiomic signature in oropharyngeal squamous cell carcinoma.pdf:pdf},
issn = {1651-226X},
journal = {Acta oncologica},
month = {oct},
number = {9},
pages = {1423--9},
pmid = {26264429},
title = {{External validation of a prognostic CT-based radiomic signature in oropharyngeal squamous cell carcinoma.}},
url = {http://www.tandfonline.com/doi/full/10.3109/0284186X.2015.1061214 http://www.ncbi.nlm.nih.gov/pubmed/26264429},
volume = {54},
year = {2015}
}
@article{McNutt2015,
author = {McNutt, Todd R. and Moore, Kevin and Quon, Harry},
doi = {10.1016/j.ijrobp.2015.11.032},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McNutt, Moore, Quon - 2015 - Needs and Challenges for Big Data in Radiation Oncology.pdf:pdf},
issn = {03603016},
journal = {International Journal of Radiation Oncology*Biology*Physics},
publisher = {Elsevier Ltd},
title = {{Needs and Challenges for Big Data in Radiation Oncology}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0360301615267703},
year = {2015}
}
@article{Domingos2012,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the “folk knowledge” that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
file = {:C$\backslash$:/Users/zwanenbal/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Domingos - 2012 - A few useful things to know about machine learning.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
month = {oct},
number = {10},
pages = {78},
title = {{A few useful things to know about machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
volume = {55},
year = {2012}
}
@article{Caicedo2017,
abstract = {Image-based cell profiling is a high-throughput strategy for the quantification of phenotypic differences among a variety of cell populations. It paves the way to studying biological systems on a large scale by using chemical and genetic perturbations. The general workflow for this technology involves image acquisition with high- throughput microscopy systems and subsequent image processing and analysis. Here, we introduce the steps required to create high-quality image-based (i.e., morphological) profiles from a collection of microscopy images. We recommend techniques that have proven useful in each stage of the data analysis process, on the basis of the experience of 20 laboratories worldwide that are refining their image-based cell-profiling methodologies in pursuit of biological discovery. The recommended techniques cover alternatives that may suit various biological goals, experimental designs, and laboratories' preferences.},
author = {Caicedo, Juan C. and Cooper, Sam and Heigwer, Florian and Warchal, Scott and Qiu, Peng and Molnar, Csaba and Vasilevich, Aliaksei S. and Barry, Joseph D. and Bansal, Harmanjit Singh and Kraus, Oren and Wawer, Mathias and Paavolainen, Lassi and Herrmann, Markus D. and Rohban, Mohammad and Hung, Jane and Hennig, Holger and Concannon, John and Smith, Ian and Clemons, Paul A. and Singh, Shantanu and Rees, Paul and Horvath, Peter and Linington, Roger G. and Carpenter, Anne E.},
doi = {10.1038/nmeth.4397},
file = {:/g40fs1/home{\$}/ZwanenbAl/Eigene Dateien/Papers/Caicedo2107 - Data analysis strategies for image-based cell profiling.pdf:pdf},
issn = {15487105},
journal = {Nature Methods},
mendeley-groups = {Radiomics},
number = {9},
pages = {849--863},
pmid = {28858338},
title = {{Data-analysis strategies for image-based cell profiling}},
volume = {14},
year = {2017}
}
@article{Smith2015,
abstract = {Uneven illumination affects every image acquired by a microscope. It is often overlooked, but it can introduce considerable bias to image measurements. The most reliable correction methods require special reference images, and retrospective alternatives do not fully model the correction process. Our approach overcomes these issues for most optical microscopy applications without the need for reference images.},
author = {Smith, Kevin and Li, Yunpeng and Piccinini, Filippo and Csucs, Gabor and Balazs, Csaba and Bevilacqua, Alessandro and Horvath, Peter},
doi = {10.1038/nmeth.3323},
file = {:/g40fs1/Shares/home/ZwanenbAl/Eigene Dateien/Papers/Smith2015 - CIDRE an illumination-correction method for optical microscopy.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {15487105},
journal = {Nature Methods},
number = {5},
pages = {404--406},
pmid = {25775044},
title = {{CIDRE: An illumination-correction method for optical microscopy}},
volume = {12},
year = {2015}
}

@article{Atkinson2001,
title = "Biomarkers and surrogate endpoints: Preferred definitions and conceptual framework",
author = "{Atkinson A.J.}, Jr and Colburn, {W. A.} and DeGruttola, {V. G.} and DeMets, {D. L.} and Downing, {G. J.} and Hoth, {D. F.} and Oates, {J. A.} and Peck, {C. C.} and Schooley, {R. T.} and Spilker, {B. A.} and J. Woodcock and Zeger, {S. L.}",
year = "2001",
doi = "10.1067/mcp.2001.113989",
volume = "69",
pages = "89--95",
journal = "Clinical Pharmacology and Therapeutics",
issn = "0009-9236",
publisher = "Nature Publishing Group",
number = "3",
}

@misc{vallieres2017responsible,
  title={Responsible Radiomics Research for Faster Clinical Translation},
  author={Vallieres, Martin and Zwanenburg, Alex and Badic, Bogdan and Cheze-Le Rest, Catherine and Visvikis, Dimitris and Hatt, Mathieu},
  year={2017},
  publisher={Soc Nuclear Med}
}

@incollection{thevenaz2000image,
  title={Image interpolation and resampling},
  author={Th{\'e}venaz, Philippe and Blu, Thierry and Unser, Michael},
  booktitle={Handbook of medical imaging},
  pages={393--420},
  year={2000},
  publisher={Academic Press, Inc.}
}

@article{soh1999texture,
  title={Texture analysis of SAR sea ice imagery using gray level co-occurrence matrices},
  author={Soh, L-K and Tsatsoulis, C},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  volume={37},
  number={2},
  pages={780--795},
  year={1999},
  publisher={IEEE}
}

@article{Bailly2016,
author = {Bailly, Cl{\'{e}}ment and Bodet-Milin, Caroline and Couespel, Sol{\`{e}}ne and Necib, Hatem and Kraeber-Bod{\'{e}}r{\'{e}}, Fran{\c{c}}oise and Ansquer, Catherine and Carlier, Thomas},
doi = {10.1371/journal.pone.0159984},
issn = {1932-6203},
journal = {PloS one},
number = {7},
pages = {e0159984},
pmid = {27467882},
title = {{Revisiting the Robustness of PET-Based Textural Features in the Context of Multi-Centric Trials.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27467882 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4965162},
volume = {11},
year = {2016}
}

@article{Altazi2017,
author = {Altazi, Baderaldeen A and Zhang, Geoffrey G and Fernandez, Daniel C and Montejo, Michael E and Hunt, Dylan and Werner, Joan and Biagioli, Matthew C and Moros, Eduardo G},
doi = {10.1002/acm2.12170},
issn = {1526-9914},
journal = {Journal of applied clinical medical physics},
keywords = {FDG,MTV,PET,cervical cancer,gray-level discretization,radiomics},
month = {nov},
number = {6},
pages = {32--48},
pmid = {28891217},
title = {{Reproducibility of F18-FDG PET radiomic features for different cervical tumor segmentation methods, gray-level discretization, and reconstruction algorithms.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28891217 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5689938},
volume = {18},
year = {2017}
}

@article{Mackin2017,
author = {Mackin, Dennis and Fave, Xenia and Zhang, Lifei and Yang, Jinzhong and Jones, A. Kyle and Ng, Chaan S. and Court, Laurence},
doi = {10.1371/journal.pone.0178524},
editor = {Tian, Jie},
issn = {1932-6203},
journal = {PLOS ONE},
month = {sep},
number = {9},
pages = {e0178524},
title = {{Harmonizing the pixel size in retrospective computed tomography radiomics studies}},
url = {http://dx.plos.org/10.1371/journal.pone.0178524},
volume = {12},
year = {2017}
}

@article{Zwanenburg2018,
archivePrefix = {arXiv},
arxivId = {1806.06719},
author = {Zwanenburg, Alex and Leger, Stefan and Agolli, Linda and Pilz, Karoline and Troost, Esther G. C. and Richter, Christian and L{\"{o}}ck, Steffen},
eprint = {1806.06719},
journal = {eprint arXiv:1806.06719 [cs.CV]},
month = {jun},
title = {{Assessing robustness of radiomic features by image perturbation}},
url = {http://arxiv.org/abs/1806.06719},
year = {2018}
}
